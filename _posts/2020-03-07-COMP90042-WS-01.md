---
layout:     post   				    # 使用的布局（不需要改）
title:      自然语言处理 Workshop 01：利用 NLTK 进行预处理  	# 标题 
subtitle:   墨尔本大学 COMP90042 Workshop #副标题
date:       2020-03-06 				# 时间
author:     Andy 						# 作者
header-img: img/post-bg-unimelb-blue.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 自然语言处理
    - COMP90042
    - Workshop


---

<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>


# Workshop 01 利用 NLTK 进行预处理

首先，如果你从来没有使用过 iPython notebooks，为了能够在此工作册上运行代码，你可以选中一个 code cell 之后，点击 Cell 菜单里的运行命令，或者按一下键盘上的 `shift` + `enter`。通常，为了使代码能够正常工作，你需要按照程序的顺序运行 cells。一个给定的 cell 的输出（包括图表在内的任何结果）会在代码运行完毕之后显示在代码下方。为了确保一切顺利，请尝试运行下面的代码：

```python
print("hello world")
```

    hello world

好的，现在让我们对一段来自班级课程网站的 html 片段进行一些简单的预处理：

```python
text = '''
<body>
    <!-- JavaScript plugins (requires jQuery) -->
    <script src="http://code.jquery.com/jquery.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>

    <div class="container">
      <div class="page-header">
  <h3>COMP90042 Natural Language Processing</h3>
</div>

The aims for this subject is for students to develop an understanding of the main algorithms used in natural 
language processing, for use in a diverse range of applications including text classification, machine 
translation, and question answering. Topics to be covered include part-of-speech tagging, n-gram language 
modelling, syntactic parsing and deep learning. The programming language used is Python, see 
<a href="python.html">the detailed configuration instructions</a> for more information on its use in the 
workshops, assignments and installation at home.
</body>
'''
```

首先，让我们利用正则表达式移除 html 标记：

```python
import re

text = re.sub("<[^>]+>", "", text).strip()
print(text)
```

    COMP90042 Natural Language Processing
    
    
    The aims for this subject is for students to develop an understanding of the main algorithms used in natural 
    language processing, for use in a diverse range of applications including text classification, machine 
    translation, and question answering. Topics to be covered include part-of-speech tagging, n-gram language 
    modelling, syntactic parsing and deep learning. The programming language used is Python, see 
    the detailed configuration instructions for more information on its use in the 
    workshops, assignments and installation at home.


We can see more clearly now that there are three newline characters between the title and the main text, and also some newlines within the text. Our sentence tokenizer won't be able to handle the title properly, so let's remove it, and change the other newlines to spaces.


```python
text = text.split("\n\n\n")[1].replace("\n", " ")
print(text)
```

    The aims for this subject is for students to develop an understanding of the main algorithms used in natural  language processing, for use in a diverse range of applications including text classification, machine  translation, and question answering. Topics to be covered include part-of-speech tagging, n-gram language  modelling, syntactic parsing and deep learning. The programming language used is Python, see  the detailed configuration instructions for more information on its use in the  workshops, assignments and installation at home.


Next let's segment the text into sentences. Though a simple method like splitting on periods would work well enough in this case, let's try a sentence segmenter from NLTK, which would be able to handle harder cases if they appeared in our text.

接下来，让我们将文本分割成句子。虽然在这个例子中，像 “按照句号进行 split” 这种简单的方法已经完全够用了，但是，让我们尝试一下 `NLTK` 中的英文分句模块 `pickle`，它可以处理文本中可能出现的更加复杂的情况：

```python
import nltk
nltk.download('punkt')
sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')

sentences = sent_segmenter.tokenize(text)
print(sentences)
```

    ['The aims for this subject is for students to develop an understanding of the main algorithms used in natural  language processing, for use in a diverse range of applications including text classification, machine  translation, and question answering.', 'Topics to be covered include part-of-speech tagging, n-gram language  modelling, syntactic parsing and deep learning.', 'The programming language used is Python, see  the detailed configuration instructions for more information on its use in the  workshops, assignments and installation at home.']


    [nltk_data] Downloading package punkt to /Users/laujh/nltk_data...
    [nltk_data]   Package punkt is already up-to-date!


NLTK also has a word tokenizer. For the first sentence, let's compare a naive split using spaces and the NTLK regex tokenizer


```python
word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()

tokenized_sentence = word_tokenizer.tokenize(sentences[1])
print(tokenized_sentence)
print(sentences[1].split(" "))
```

    ['Topics', 'to', 'be', 'covered', 'include', 'part', '-', 'of', '-', 'speech', 'tagging', ',', 'n', '-', 'gram', 'language', 'modelling', ',', 'syntactic', 'parsing', 'and', 'deep', 'learning', '.']
    ['Topics', 'to', 'be', 'covered', 'include', 'part-of-speech', 'tagging,', 'n-gram', 'language', '', 'modelling,', 'syntactic', 'parsing', 'and', 'deep', 'learning.']


The NLTK tokenizer correctly splits off commas and periods from the ends of words. It also splits up the hyphenated word "part-of-speech", which might be the right behavior for some applications, but not for others.

Let's try out lemmatization. NLTK has a lemmatizer, though using it requires that we know the part of speech of the word. In this case, we'll just try verb lemmatization, and if doesn't change the word, we'll try noun.


```python
nltk.download('wordnet')
lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()

def lemmatize(word):
    lemma = lemmatizer.lemmatize(word,'v')
    if lemma == word:
        lemma = lemmatizer.lemmatize(word,'n')
    return lemma

print([lemmatize(token) for token in tokenized_sentence])
```

    [nltk_data] Downloading package wordnet to /Users/laujh/nltk_data...
    [nltk_data]   Package wordnet is already up-to-date!


    ['Topics', 'to', 'be', 'cover', 'include', 'part', '-', 'of', '-', 'speech', 'tag', ',', 'n', '-', 'gram', 'language', 'model', ',', 'syntactic', 'parse', 'and', 'deep', 'learn', '.']


Compare this to the result of stemming using the Porter Stemmer:


```python
stemmer = nltk.stem.porter.PorterStemmer()
print([stemmer.stem(token) for token in tokenized_sentence])
```

    ['topic', 'to', 'be', 'cover', 'includ', 'vector', 'space', 'model', ',', 'part', '-', 'of', '-', 'speech', 'tag', ',', 'n', '-', 'gram', 'languag', 'model', ',', 'syntact', 'pars', 'and', 'neural', 'sequenc', 'model', '.']



```python

```




