---
layout:     post   				    # 使用的布局（不需要改）
title:      统计机器学习 11：核方法   	# 标题 
subtitle:   墨尔本大学 COMP90051 课程笔记 #副标题
date:       2019-11-16 				# 时间
author:     Andy 						# 作者
header-img: img/post-bg-sml.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 统计机器学习
    - 核函数
    - SVM
---

<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

# Lecture 11 核方法
## 主要内容
* **核化**
  * SVM 对偶公式的基扩展
  * “核技巧”；特征空间点积的快速计算
* **模块化学习**
  * 从特征变换中分离出“学习模块”
  * 表示定理
* **构造核**
  * 常见核及其特性概述
  * Mercer 定理
  * 学习非常规数据类型

## 1. SVM 核化
**通过基扩展进行特征变换；通过对核进行直接评估来实现加速 —— “核技巧”**
### 1.1 SVM 处理非线性数据
* 方法 1：软间隔 SVM（参考 Lecture 10）
* 方法 2：<span style="color:red;">特征空间</span> 变换（参考 Lecture 4）
  * 将数据映射到一个新的特征空间
  * 在新的特征空间中运行硬间隔或者软间隔 SVM
  * 决策边界在原始特征空间中是非线性的  
  <img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-14-WX20200214-133247%402x.png" width="80%">

### 1.2 特征变换（基扩展）
* 考虑一个二分类问题
* 每个样本点具有特征 $[x_1, x_2]$
* 非线性可分  
<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-14-WX20200214-133923%402x.png" width="40%">

* 现在“增加”一个特征 $x_3=x_1^2+x_2^2$
* 每个样本点现在为 $[x_1, x_2, x_1^2+x_2^2]$
* 现在数据变成线性可分了  
<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-14-WX20200214-134026%402x.png" width="40%">

### 1.3 朴素工作流
* 选择 / 设计一个线性模型
* 选择 / 设计一个高维变换 $\varphi (\boldsymbol x)$
  * 希望在添加了许多各种特征之后，其中一些特征将使数据变得线性可分
* 对于 <span style="color:red;">每个</span> 训练样本，以及 <span style="color:red;">每个</span> 新的实例，计算 $\varphi (\boldsymbol x)$
* 训练分类器 / 进行预测
* **问题：** 对于高维 / 无限维的 $\varphi (\boldsymbol x)$，<span style="color:red;">计算 $\varphi (\boldsymbol x)$ 是不现实 / 不可能的</span>。

### 1.4 硬间隔 SVM 的对偶公式
* **训练：** 寻找 $\boldsymbol \lambda$ 使得  

$$\begin{array}{cc}\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol \lambda}\sum_{i=1}^{n}\lambda_i-\dfrac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i \lambda_j y_i y_j \color{red}{\underbrace{\fbox{$\color{black}{\boldsymbol x_i' \boldsymbol x_j}$}}_{\text{点积}}}\\\\
\text{s.t.}\quad \lambda_i\ge 0 \;\text{and}\;\sum_{i=1}^{n}\lambda_i y_i=0 \end{array}$$

* **预测：** 根据 $s$ 的符号对实例 $\boldsymbol x$ 进行分类  

$$s=b^*+\sum_{i=1}^{n}\lambda_i^* y_i \color{red}{\underbrace{\fbox{$\color{black}{\boldsymbol x_i' \boldsymbol x}$}}_{\text{点积}}}$$

注意：对于任意支持向量 $j$，通过求解 $$y_j(b^{*}+\sum_{i=1}^{n}\lambda_i^* y_i \color{red}{\fbox{$\color{black}{\boldsymbol x_i' \boldsymbol x_j}$}})=1$$ 来找到 $b^*$

### 1.5 特征空间中的硬间隔 SVM
* **训练：** 寻找 $\boldsymbol \lambda$ 使得  

$$\begin{array}{cc}\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol \lambda}\sum_{i=1}^{n}\lambda_i-\dfrac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i \lambda_j y_i y_j \color{red}{\fbox{$\color{black}{\varphi (\boldsymbol x_i)' \varphi (\boldsymbol x_j)}$}}\\\\
\text{s.t.}\quad \lambda_i\ge 0 \;\text{and}\;\sum_{i=1}^{n}\lambda_i y_i=0 \end{array}$$

* **预测：** 根据 $s$ 的符号对实例 $\boldsymbol x$ 进行分类  

$$s=b^*+\sum_{i=1}^{n}\lambda_i^* y_i \color{red}{\fbox{$\color{black}{\varphi (\boldsymbol x_i)' \varphi (\boldsymbol x)}$}}$$

注意：对于任意支持向量 $j$，通过求解 $$y_j(b^{*}+\sum_{i=1}^{n}\lambda_i^* y_i \color{red}{\fbox{$\color{black}{\varphi (\boldsymbol x_i)' \varphi (\boldsymbol x_j)}$}})=1$$ 来找到 $b^*$

### 1.6 观察：核表示
* 参数估计和计算预测都仅依赖于 <span style="color:red;">点积</span> 形式的数据
  * 在原始特征空间：$\boldsymbol u' \boldsymbol v=\sum_{i=1}^{m}u_i v_i$
  * 在经过特征变换后的空间：$\varphi(\boldsymbol u)' \varphi(\boldsymbol v)=\sum_{i=1}^{l}\varphi(\boldsymbol u)_i \varphi(\boldsymbol v)_i$
* <span style="color:red;">核函数</span> 是可以在某些特征空间中表示为点积的函数  
  $K(\boldsymbol u, \boldsymbol v)=\varphi(\boldsymbol u)' \varphi(\boldsymbol v)$

### 1.7 核函数是一种捷径：例子
* 对于某些 $\varphi(\boldsymbol x)$，**直接对核函数进行计算** 要比先映射到特征空间然后再计算点积 **更快**。
* 例如，考虑两个向量 $\boldsymbol u =[ u_1 ]$ 和 $\boldsymbol v =[ v_1 ]$，以及变换 $\varphi(\boldsymbol x)=[ x_1^2, \sqrt{2c} x_1, c ]$，其中 $c$ 是某个常数
  * 所以，$\varphi(\boldsymbol u)=[ u_1^2, \sqrt{2c} u_1, c ]'$（<span style="color:green;">2 步操作</span>）和 $\varphi(\boldsymbol v)=[ v_1^2, \sqrt{2c} v_1, c ]'$（<span style="color:green;">+2 步操作</span>）
  * 然后，$\varphi(\boldsymbol u)'\varphi(\boldsymbol v)=(u_1^2 v_1^2+2cu_1 v_1+c^2)$ （<span style="color:green;">+5 步操作 = 9 步操作</span>）
* 这可以通过 **直接计算核函数** 来代替  

  $$\varphi(\boldsymbol u)' \varphi(\boldsymbol v)=(u_1 v_1 +c)^2$$  

  * 现在只需 <span style="color:purple;">3 步操作</span>
  * 这里，$K(\boldsymbol u, \boldsymbol v)=(u_1 v_1 +c)^2$ 是相应的核函数

### 1.8 更通用的：“核技巧”
* 考虑两个训练数据点 $\boldsymbol x_i$ 和 $\boldsymbol x_j$，以及它们在经过变换后的特征空间中的点积。
* $k_{ij}\equiv \varphi(\boldsymbol x_i)'\varphi(\boldsymbol x_j)$ <span style="color:red;">核矩阵</span> 可以按如下步骤计算：
  1. 计算 $\varphi(\boldsymbol x_i)'$
  2. 计算 $\varphi(\boldsymbol x_j)$
  3. 计算 $k_{ij}=\varphi(\boldsymbol x_i)'\varphi(\boldsymbol x_j)$

* 然而，对于某些变换 $\varphi$，存在一种“捷径”函数可以得到与 $K(\boldsymbol x_i,\boldsymbol x_j)=k_{ij}$ 完全相同的答案：
  * 不包含上面的 1-3 步，而且没有计算 $\varphi(\boldsymbol x_i)$ 和 $\varphi(\boldsymbol x_j)$
  * 通常，计算 $k_{ij}$ 的时间复杂度为 $O(m)$，但是计算 $\varphi(\boldsymbol x)$ 的时间复杂度为 $O(l)$，其中 $l \gg m$（<span style="color:red;">计算上不现实</span>）甚至 $l=\infty$（<span style="color:red;">计算上不可行</span>）

### 1.9 核函数硬间隔 SVM
* **训练：** 寻找 $\boldsymbol \lambda$ 使得  

  $$\begin{array}{cc}\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol \lambda}\sum_{i=1}^{n}\lambda_i-\dfrac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i \lambda_j y_i y_j \color{red}{\underbrace{\fbox{$\color{black}{K(\boldsymbol x_i, \boldsymbol x_j)}$}}_{\text{核函数}}}\\\\
  \text{s.t.}\quad \lambda_i\ge 0 \;\text{and}\;\sum_{i=1}^{n}\lambda_i y_i=0 \end{array}$$  

  <span style="color:red;">特征映射通过核函数实现</span>

* **预测：** 根据 $s$ 的符号对实例 $\boldsymbol x$ 进行分类  

  $$s=b^*+\sum_{i=1}^{n}\lambda_i^* y_i \color{red}{\underbrace{\fbox{$\color{black}{K(\boldsymbol x_i, \boldsymbol x)}$}}_{\text{核函数}}}$$

  <span style="color:red;">特征映射通过核函数实现</span>

* 这里，我们注意到，对于任意支持向量 $j$，都有 $$y_j(b^{*}+\sum_{i=1}^{n}\lambda_i^* y_i \color{red}{\fbox{$\color{black}{K(\boldsymbol x_i, \boldsymbol x_j)}$}})=1$$，可以以此来找到 $b^*$

### 1.10 非线性的处理方法
* **<span style="color:red;">ANN</span>**
  * $\boldsymbol u=\varphi(\boldsymbol x)$ 中的元素是输入 $\boldsymbol x$ 经过变换得到的
  * 该 $\varphi$ 具有从数据中学习得到的权重  

  <img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-14-WX20200214-205052%402x.png" width="30%">

* **<span style="color:red;">SVM</span>**
  * 对核函数 $K$ 的选择决定了特征空间 $\varphi$
  * 不学习 $\varphi$ 的权重
  * 但是，甚至不需要计算 $\varphi$
  * 同样支持任意数据类型

---

<span style="color:red;font-style:italic;font-size:15pt;">text</span>



---

## 总结
* 软间隔 SVM
  * 直觉和问题的数学表述
* 构造对偶问题
  * 拉格朗日乘子法、KKT 条件
  * 弱对偶和强对偶
* 补充
  * 互补松弛性
  * 训练注意事项

下节内容：核方法


