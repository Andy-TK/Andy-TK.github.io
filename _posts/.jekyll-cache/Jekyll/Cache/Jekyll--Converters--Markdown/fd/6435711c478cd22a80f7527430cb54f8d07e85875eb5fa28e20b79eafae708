I"M<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-13-多臂老虎机问题">Lecture 13 多臂老虎机问题</h1>
<h2 id="主要内容">主要内容</h2>
<ul>
  <li><strong>随机多臂老虎机</strong>
    <ul>
      <li>“我们学会采取行动的地方；我们仅仅接受奖励形式的间接监督；并且我们只观测行动带来的奖励。” —— 这是一个关于 <strong>探索-利用（Exploration-Exploitation）</strong> 权衡最简单的设定</li>
      <li>不确定性下的顺序决策</li>
      <li>$(\varepsilon)$-Greedy</li>
      <li>UCB 算法</li>
    </ul>
  </li>
</ul>

<h2 id="1-多臂老虎机">1. 多臂老虎机</h2>
<h3 id="11-多臂老虎机问题mab">1.1 多臂老虎机问题（MAB）</h3>
<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-17-WX20200217-191218%402x.png" width="60%" /></p>

<p><strong><span style="color:steelblue">问题描述：</span></strong></p>
<blockquote>
  <p>赌场内，有一名赌徒想要去摇 <strong>老虎机（bandit）</strong>，他面前有一排机器，每台机器都拥有一个 <strong>臂（arm）</strong>，而且每台机器看上去都一样。每次投一枚硬币就能获得一次 <strong>摇臂（play）</strong> 的机会，而且每个臂摇下都有可能获得 <strong>奖励（rewards）</strong> —— 但是，每台老虎机吐出奖励的概率分布是未知的。作为赌徒，自然希望自己的 <strong>累积收益的期望（expected cumulative reward）最大化</strong>（假如一共有 $n$ 次摇臂的机会），那么，请问这名赌徒应该采取怎样的行动？<br /></p>

  <p>—— 这就是 <strong>多臂老虎机问题（Multi-armed bandit, MAB)</strong></p>
</blockquote>

<h3 id="12-探索explorationvs-利用exploitation">1.2 探索（Exploration）vs. 利用（Exploitation）</h3>
<ul>
  <li>多臂老虎机问题（MAB）
    <ul>
      <li>关于平衡探索（Exploration）和利用（Exploitation）的最简单的设定</li>
      <li>与强化学习同属一类机器学习任务家族</li>
    </ul>
  </li>
  <li>一些应用场景
    <ul>
      <li>在线广告</li>
      <li>投资组合选择</li>
      <li>数据库中的缓存</li>
      <li>游戏中的随机搜索（例如，AlphaGo）</li>
      <li>自适应 A / B 测试</li>
      <li>…</li>
    </ul>
  </li>
</ul>

<h3 id="13-随机-mab-设定">1.3 随机 MAB 设定</h3>
<ul>
  <li>可能采取的行动 <script type="math/tex">\{1,...,k\}</script> 称为 “<span style="color:red">臂（arms）</span>”
    <ul>
      <li>臂（arm）$i$ 具有在有界 <span style="color:red">奖励（rewards）</span> 上的分布 $P_i$，其期望为 $u_i$</li>
    </ul>
  </li>
  <li>在 $t=1…T$ 轮：
    <ul>
      <li>摇臂行为（play）<script type="math/tex">i_t\in \{1,...,k\}</script>（可能是随机的）</li>
      <li>得到奖励 $X_{i_t}(t)\sim P_{i_t}$</li>
    </ul>
  </li>
  <li>目标：最小化累积 <span style="color:red">遗憾（regret）</span>
    <ul>
      <li><script type="math/tex">u^*T-\sum_{t=1}^{T}E\left[X_{i_t}(t) \right]</script> $\qquad$其中，<script type="math/tex">u^*=\max \limits_i u_i</script><br />
第一部分为 <span style="color:red">在预先知道每台机器奖励概率分布的情况（上帝视角）下的累积奖励的期望</span><br />
第二部分为 <span style="color:red">实际摇完老虎机后的累积奖励的期望</span></li>
      <li>直觉上，我们应该制定一个简单但是包含未来知识的规则</li>
    </ul>
  </li>
</ul>

<h2 id="2-greedy-和-varepsilon-greedy-算法">2. Greedy 和 $\varepsilon$-Greedy 算法</h2>
<h3 id="21-greedy-算法">2.1 Greedy 算法</h3>
<ul>
  <li>在第 $t$ 轮：
    <ul>
      <li>
        <p>将每个臂（arm）$i$ 观测到的平均奖励作为 <span style="color:red">估计值</span></p>

        <script type="math/tex; mode=display">% <![CDATA[
Q_{t-1}(i)=\begin{cases}\dfrac{\sum_{s=1}^{t-1}X_i(s)1[i_s=i]}{\sum_{s=1}^{t-1}1[i_s=i]}, & \text{if }\sum_{s=1}^{t-1}1[i_s=i]>0 \\ Q_0 , & \text{otherwise}\end{cases} %]]></script>

        <p>其中，在某个臂（arm）$i$ 被第一次摇到之前，其估计值都设为初始常数 $Q_0(i)=Q_0$</p>

        <p>也就是说，将经过之前的 $t-1$ 轮后，如果臂（arm）$i$ 被摇动过了，那么将之前 $t-1$ 轮的平均奖励（reward）作为这台机器的在第 $t$ 轮奖励的估计值；否则，将初始值 $Q_0(i)=Q_0$ 作为估计值。</p>
      </li>
      <li></li>
    </ul>
  </li>
</ul>

<h2 id="总结">总结</h2>

<p>下节内容：多臂老虎机问题</p>
:ET