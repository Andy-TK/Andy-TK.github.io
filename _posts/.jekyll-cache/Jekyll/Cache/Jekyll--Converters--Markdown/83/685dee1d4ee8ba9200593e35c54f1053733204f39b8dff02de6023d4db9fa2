I"S<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-18-高斯混合模型和-em-算法">Lecture 18 高斯混合模型和 EM 算法</h1>
<h2 id="主要内容">主要内容</h2>
<ul>
  <li><strong>无监督学习</strong>
    <ul>
      <li>问题的多样性</li>
    </ul>
  </li>
  <li><strong>高斯混合模型（GMM）</strong>
    <ul>
      <li>一种用于聚类的概率方法</li>
      <li>GMM</li>
      <li>优化问题的 GMM 聚类</li>
    </ul>
  </li>
  <li><strong>期望最大化（EM）算法</strong>
    <h2 id="1-无监督学习">1. 无监督学习</h2>
    <p><strong>机器学习中的一个大的分支，关注在标签缺失的数据上学习其结构</strong></p>
    <h3 id="11-在此之前监督学习">1.1 在此之前：监督学习</h3>
  </li>
  <li>监督学习：总体目标是根据数据做出预测</li>
  <li>为此，我们学习了诸如：随机森林、ANN 和 SVM 等算法</li>
  <li>我们有实例 $\boldsymbol x_i\in \boldsymbol R^m\;(i=1,…,n)$，以及对应的标签 $y_i$ 作为输入，目标是预测新的实例的标签。</li>
  <li>可以看作是一个函数逼近问题，但请注意：泛化能力很关键</li>
  <li>老虎机：部分监督的设定</li>
</ul>

<h3 id="12-现在无监督学习">1.2 现在：无监督学习</h3>
<ul>
  <li>接下来我们将介绍无监督学习方法</li>
  <li>在无监督学习中，没有所谓叫做 “标签” 的专门变量</li>
  <li>相应地，我们只有一个数据点的集合 $\boldsymbol x_i\in \boldsymbol R^m\;(i=1,…,n)$</li>
  <li>无监督学习的目的是 <span style="color:red">探索数据的结构（模式、规律等）</span></li>
  <li>“探索数据结构” 的目的是模糊的</li>
</ul>

<h3 id="13-无监督学习任务">1.3 无监督学习任务</h3>
<ul>
  <li>无监督学习包括很多不同类别的任务：
    <ul>
      <li>聚类</li>
      <li>降维</li>
      <li>概率模型的参数学习</li>
    </ul>
  </li>
  <li>相关应用：
    <ul>
      <li>市场篮子分析。例如，使用超市的交易日志查找经常被一起购买的商品</li>
      <li>离群值检测。例如，发现潜在的信用卡交易欺诈</li>
      <li>经常用于（有监督）机器学习 pipeline 中的无监督任务</li>
    </ul>
  </li>
</ul>

<h3 id="14-回顾k-means-聚类">1.4 回顾：k-means 聚类</h3>
<p><strong><span style="color:steelblue">k-means 算法描述：</span></strong><br />
1.$\,$<strong>初始化：</strong> 随机选择 $k$ 个集群 <span style="color:red">中心</span><br />
2.$\,$<strong>更新：</strong><br />
$\qquad$a. <span style="color:red">分配数据点</span> 到最近的集群中心<br />
$\qquad$b. 在当前的分配下，<span style="color:red">重新计算集群中心</span><br />
3.$\,$<strong>终止：</strong> 如果集群中心没有发生变化，那么算法 <span style="color:purple">停止</span><br />
4.$\,$返回 <span style="color:purple">步骤 2</span></p>

<p>选择典型的 $L_2$ 范数来表示距离。<br />
K-means 仍然是最受欢迎的数据挖掘算法之一。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-25-WX20200225-164732%402x.png" /></p>

<p><strong><center><span style="font-size:10pt">使用重新缩放的 Old Faithful 据集的 K-means 算法的图示</span></center></strong><center><span style="font-size:10pt">Source: <span style="font-style:italic">Pattern Recognition and Machine Learning (p.426)</span> by Bishop</span></center></p>

<ul>
  <li>需要预先指定集群数 $k$</li>
  <li>使用欧几里得距离衡量 “差异性”</li>
  <li>寻找 “球形” 集群</li>
  <li>迭代优化过程</li>
</ul>

<h2 id="2-高斯混合模型gmm">2. 高斯混合模型（GMM）</h2>
<p><strong>从概率的视角看聚类算法</strong></p>
<h3 id="21-对数据聚类中的不确定性建模">2.1 对数据聚类中的不确定性建模</h3>
<ul>
  <li>k-means 聚类将每个数据点都精确地分配到某一个集群</li>
  <li>类似于 k-means，一个概率的混合模型同样需要预先指定集群数 $k$</li>
  <li>不同于 k-means，概率模型让我们得以表达有关每个数据点 <span style="color:red">来源的不确定性</span>
    <ul>
      <li>每个数据点来自集群 $c$ 的概率为 $w_c$，其中 $c=1,…,k$</li>
    </ul>
  </li>
  <li>也就是说，每个数据点仍然只来自某个特定的集群（又称 “组分”），但是我们不确定来自哪一个</li>
  <li>接下来
    <ul>
      <li>将各个组分建模为高斯分布</li>
      <li>拟合过程展示了一般的期望最大化（EM）算法</li>
    </ul>
  </li>
</ul>

<h3 id="22-聚类概率模型">2.2 聚类：概率模型</h3>
<p>数据点 $\boldsymbol x_i$ 是来自 $K$ 个分布（或者说 “组分”）的混合的独立同分布（i.i.d.）样本</p>

<center><span style="font-size:10pt">混合中的每个组分就是我们所说的一个集群</span></center>
<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-25-WX20200225-182850%402x.png" /></p>

<p>原则上，我们可以将 <span style="color:red">组分</span> 假设为任何分布，但是，正态分布是一种常见的建模选择 $\to$ 高斯混合模型（Gaussian Mixture Model, GMM）</p>

<h3 id="23-正态又称-高斯分布">2.3 正态（又称 “高斯”）分布</h3>
<ul>
  <li>
    <p>回忆 $1$-维 高斯分布：</p>

    <script type="math/tex; mode=display">N(x|\mu,\sigma)\equiv \dfrac{1}{\sqrt{2\pi \sigma^2}}\exp \left(-\dfrac{(x-\mu)^2}{2\sigma^2}\right)</script>
  </li>
  <li>
    <p>然后，一个 $d$-维 高斯分布为：</p>

    <script type="math/tex; mode=display">N(\boldsymbol x|\boldsymbol{\mu},\mathbf{\Sigma})\equiv (2\pi)^{-\frac{d}{2}}|\mathbf{\Sigma}|^{-\frac{1}{2}}\exp \left(-\dfrac{1}{2}(\boldsymbol x-\boldsymbol \mu)^T \mathbf{\Sigma}^{-1}(\boldsymbol x-\boldsymbol \mu)\right)</script>

    <ul>
      <li>$\mathbf{\Sigma}$ 是 <span style="color:red">协方差矩阵</span>，是一个 PSD（半正定）对称 $d\times d$ 矩阵</li>
      <li>$|\mathbf{\Sigma}|$ 表示行列式</li>
      <li>不需要记住完整的公式</li>
    </ul>
  </li>
</ul>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-25-WX20200225-185102%402x.png" width="80%" /></p>

<h3 id="24-高斯混合模型gmm">2.4 高斯混合模型（GMM）</h3>
<ul>
  <li>
    <p>高斯混合分布（对于单个数据点）：</p>

    <script type="math/tex; mode=display">P(\boldsymbol x)\equiv \sum_{j=1}^{k}w_j N(\boldsymbol x|\boldsymbol{\mu}_j,\mathbf{\Sigma}_j)\equiv \sum_{j=1}^{k}P(C_j)P(\boldsymbol x_j|\boldsymbol C_j)</script>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$$P(\boldsymbol x_j</td>
          <td>\boldsymbol C_j)\equiv N(\boldsymbol x</td>
          <td>\boldsymbol{\mu}_j,\mathbf{\Sigma}_j)$$ 是类别 $j$ 的类别 / 组分条件密度（建模为高斯分布）</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>这里，$P(C_j)\ge 0$，并且 $\sum_{j=1}^{k}P(C_j)=1$</li>
</ul>

<h2 id="总结">总结</h2>

<p>下节内容：高斯混合模型（GMM）和期望最大化（EM）</p>
:ET