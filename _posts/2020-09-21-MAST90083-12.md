---
layout:     post   				    # 使用的布局（不需要改）
title:      计算统计学与数据科学 12：Q & A  	# 标题 
subtitle:   墨尔本大学 MAST90083 课程笔记 #副标题
date:       2020-09-21				# 时间
author:     Andy 						# 作者
header-img: img/post-bg-unimelb-blue.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 计算统计学与数据科学
    - MAST90083
    - 课程笔记

---

# Lecture 12 Q&A 

**参考教材**：

* *Gareth, J., Daniela, W., Trevor, H., & Robert, T. (2013). An intruduction to statistical learning: with applications in R. Spinger.*
* *Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Spinger Science & Business Media.*

## CH2 统计学习

### Q1：高灵活度模型 vs. 低灵活度模型
#### (a) 样本数 $n$ 很大，预测变量数 $p$ 很小

高灵活度模型，因为样本数量足够大，而预测变量本身并不多，不会导致太高的模型复杂度，过拟合风险较低。

#### (b) 预测变量数 $p$ 很大，样本数 $n$ 很小

低灵活度模型，过多的预测变量会导致过于复杂的模型，加上观测数量不足，选择灵活模型很容易发生过拟合。

#### (c) 预测变量与响应之间的关系是高度非线性的

高灵活度模型，适用于高度非线性的情况，如果采用低灵活度模型，则容易发生欠拟合。

#### (d) 误差项的方差即 $\sigma^2 = \mathrm{Var}(\epsilon)$ 非常大

低灵活度模型，不可减小误差 $\mathrm{Var}(\epsilon)$ 很大说明测试错误率的 U 型曲线较早到达底部，因此，为了使得测试错误率尽可能低，应选择低灵活度模型。

### Q3：绘制偏差平方、方差、测试误差、训练误差、不可减小误差的曲线并解释

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-10-WX20201110-180620%402x.png" width="60%">

$$E\left [(y_0 - \hat f(x_0))^2 \right] = \mathrm{Var}(\hat f(x_0)) + [\mathrm{Bias}(\hat f(x_0))]^2 + \mathrm{Var}(\epsilon)$$

**偏差**：模型假设带来的误差。灵活度越高，模型假设越少，偏差越低。
**方差**：用不同训练集拟合模型时，拟合值的变化。灵活度越高，模型越容易过拟合，方差越高。
**不可减小误差**：由未测量变量或者不可测量变化带来的系统噪声。是一个大于零的常数，与模型灵活度无关。无论选择怎样的模型去拟合真实函数，始终存在的一个误差下界。
**训练误差**：随模型灵活度增加而减小。过于灵活的模型可能会得到非常小的训练误差，存在过拟合风险。
**测试误差**：随模型灵活度增加，先减小后增大，最低点左边欠拟合，右边过拟合。

### Q4：分类、回归、聚类例子各三个

#### (a) 分类的例子

* 申请学校 (University Application)
  * 响应变量：1 (success) 或者 0 (fail)
  * 预测变量：GPA, IELTS score, working experience
  * 目标：prediction

* 糖尿病诊断 (Diabetes Diagnosis)
  * 响应变量：1 (Unhealth) 或者 0 (health)
  * 预测变量：age, BMI, family medical history (yes or no)
  * 目标：prediction

* 信用卡违约 (Credit Default)
  * 响应变量：1 (Default) 或者 0 (not default)
  * 预测变量：income, balance, student (yes or no)
  * 目标：inference

#### (b) 回归的例子

* 个人收入 (Personal Salary)
  * 响应变量：income
  * 预测变量：age, gender, education
  * 目标：inference

* 房价评估 (House Price)
  * 响应变量：house price
  * 预测变量：area, city, year, number of rooms
  * 目标：prediction

* 广告销量 (Advertise Sales)
  * 响应变量：sales
  * 预测变量：advertisement budget on audio, neswpaper and TV
  * 目标：prediction

#### (c) 聚类的例子

* 用户分组 (Group users)
  * 特征变量：age, gender, education, income
  * 目标：make different marketing strategies for different group users

* 产品分组 (Group products)
  * 特征变量：size, weight, sweetness, grown area
  * 目标：giving different price for apples in different groups

* 推荐系统 (Recommendation systems)
  * 特征变量：age, gender, hobbies, browsing history
  * 目标：recommend different types of movies to different users

### Q5：高灵活度模型的优缺点，如何选择

* 高灵活度模型
  * 优点：低偏差，假设少，捕获非线性，捕获复杂变量交互
  * 缺点：高方差，过拟合风险，难以解释，需要调整，训练时间长
* 适用场景
  * 高灵活度模型：存在非线性和交互效应、大量变量、大量数据、算力充足、主要关注预测
  * 低灵活度模型：低灵活度模型假设基本满足 (例如线性、正态性等)、主要关注推断和可解释性

### Q6：比较参数方法与非参数方法以及各自的优缺点

* 参数方法：基于模型估计的两阶段方法，首先对函数 $f$ 的形式给出假设，然后估计模型参数。
  * 优点：将估计真实函数 $f$ 的问题简化为估计一组参数。
  * 缺点：选定的模型与真实 $f$ 在形式上并非是一致，若差别过大会导致效果很差。如果选择灵活模型，则存在过拟合风险。

* 非参数方法：不需要对 $f$ 的形式进行假设。
  * 优点：不限定 $f$ 的形式，可以在更大范围内选择更适合 $f$ 的形状。
  * 缺点：由于没有简化为参数估计问题，需要大量观测数据 (远多于非参数方法) 以得到一个 $f$ 的准确估计。

### Q7：用 KNN 方法在给定数据集上进行预测

贝叶斯决策边界非线性情况下，我们应当选择更小的 $K$。$K$ 越小，模型灵活度越高，得到的决策边界更接近真实情况。如果选择很大的 $K$，最终得到的决策边界会接近线性。

## CH3 线性回归

















下节内容：非线性模型
