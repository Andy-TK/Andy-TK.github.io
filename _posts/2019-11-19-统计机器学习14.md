---
layout:     post   				    # 使用的布局（不需要改）
title:      统计机器学习 14：贝叶斯回归   	# 标题 
subtitle:   墨尔本大学 COMP90051 课程笔记 #副标题
date:       2019-11-19 				# 时间
author:     Andy 						# 作者
header-img: img/post-bg-sml.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 统计机器学习
    - 贝叶斯回归
---

<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

# Lecture 14 贝叶斯回归
## 主要内容
* **点估计未捕获到的不确定性**
* **贝叶斯方法保留不确定性**
* **顺序贝叶斯更新**
* **共轭先验（Normal-Normal）**
* **利用后验在测试集上进行贝叶斯预测**

## 1. 回顾贝叶斯
###  1.1 训练 = 优化？
**学习与推断阶段：**
* **<span style="color:steelblue">对于分类问题：</span>**
  * 建模（以 Logistic 回归为例）

    $$p(y|\boldsymbol x)=\text{sigmoid}(\boldsymbol x'\boldsymbol w)$$

  * 参数拟合数据

    $$\hat{\boldsymbol w}=\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol w}p(\boldsymbol y|\boldsymbol X,\boldsymbol w)p(\boldsymbol w)$$

  * 进行预测

    $$p(y_*|\boldsymbol x_*)=\text{sigmoid}(\boldsymbol x_*'\hat{\boldsymbol w})$$

* **<span style="color:steelblue">对于回归问题：</span>**
  * 建模

    $$p(y|\boldsymbol x)=\text{Normal}(\boldsymbol x'\boldsymbol w;\sigma^2)$$

  * 参数拟合数据

    $$\hat{\boldsymbol w}=\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol w}p(\boldsymbol y|\boldsymbol X,\boldsymbol w)p(\boldsymbol w)$$

  * 进行预测

    $$E[y_* ]=\boldsymbol x_*'\hat{\boldsymbol w}$$

<span style="color:red">$$\hat{\boldsymbol w}$$ 对应于 “点估计”</span>

### 1.2 贝叶斯替代
**$$\hat{\boldsymbol w}$$ 并没有什么特别之处……如果我们不只是使用参数的一个估计值呢？**
* **<span style="color:steelblue">对于分类问题：</span>**
  * 建模

    $$p(y|\boldsymbol x)=\text{sigmoid}(\boldsymbol x'\boldsymbol w)$$

  * 考虑那些可以比较好地拟合数据的 <span style="color:red">可能的参数空间</span>

    $$p(\boldsymbol w|\boldsymbol X,\boldsymbol y)$$

  * 进行 <span style="color:red">“期望的”</span> 预测

    $$p(y_*|\boldsymbol x_*)=E_{p(\boldsymbol w|\boldsymbol X,\boldsymbol y)}\left[\text{sigmoid}(\boldsymbol x_*'\boldsymbol w)\right]$$

* **<span style="color:steelblue">对于回归问题：</span>**
  * 建模

    $$p(y|\boldsymbol x)=\text{Normal}(\boldsymbol x'\boldsymbol w;\sigma^2)$$

  * 考虑那些可以比较好地拟合数据的 <span style="color:red">可能的参数空间</span>

    $$p(\boldsymbol w|\boldsymbol X,\boldsymbol y)$$

  * 进行 <span style="color:red">“期望的”</span> 预测

    $$p(y_*|\boldsymbol x_*)=E_{p(\boldsymbol w|\boldsymbol X,\boldsymbol y)}\left[\text{Normal}(\boldsymbol x_*'\boldsymbol w;\sigma^2)\right]$$

## 2. 不确定性
**如果用于训练的数据集很小，我们很少会完全信任任何从中学习到的模型。我们能否量化这种不确定性，并将其用于预测呢？**
### 2.1 重新审视回归问题

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-19-WX20200219-145612%402x.png" width="60%">  

<span style="color:red">线性回归：</span> $y=w_0+w_1x$  
这里，$y=$ humidity（湿度），$x=$ temperature（温度）

* 从数据中学习模型
  * 通过选择权重来最小化误差残差

    $$\hat{\boldsymbol w}=(\boldsymbol X'\boldsymbol X)^{-1}\boldsymbol X'\boldsymbol y$$

* 但是我们对于得到的 $\hat{\boldsymbol w}$ 和预测值有多大的信心？

### 2.2 我们应该相信点估计 $\hat{\boldsymbol w}$ 吗？
* 我们的学习算法有多稳定？

  <img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-19-WX20200219-152254%402x.png" width="80%">

  **<center><span style="font-size:10pt">两个具有不同噪声水平的数据集以及它们各自对应的似然函数</span></center>**  
  <center><span style="font-size:10pt">Source: <span style="font-style:italic">A First Course in Machine Learning (p.81)</span> by Rogers & Girolami</span></center>  
  <br>  

  * $\hat{\boldsymbol w}$ 对于噪声高度敏感
  * 参数估计的不确定性有多少？
  * 如果目标参数的 **负对数似然（Negative Log Likelihood, NLL）** 的在峰值处越高且窄，说明我们掌握的信息量越大
* 形式化为 **费雪信息矩阵（Fisher Information Matrix）**
  * $E[ 2^{nd} \text{ deriv of NLL}]$  
    $\cal I$ $=\dfrac{1}{\sigma^2}\boldsymbol X'\boldsymbol X$
* 衡量关于 $\hat{\boldsymbol w}$ 的目标函数的曲率

## 3. 贝叶斯视角
**保留所有的未知因素（例如：参数的不确定性）并对它们进行建模，并且在进行统计推断时利用这些信息。**
### 3.1 一个贝叶斯人的视角
* 我们有理由认为 **所有的** 参数对于数据而言都是常数吗？
  * 对于训练数据拟合更好的权重的概率应该大于其他权重的概率
  * 利用所有可能的权重进行预测，乘以各自的概率作为缩放系数
* 这就是 <span style="color:red">贝叶斯推断</span> 背后的思想

### 3.2 参数的不确定性  
<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-19-WX20200219-191611%402x.png" width="40%" align="right">

* 目标函数有很多合理的解
  * 为什么只选择其中的某一个呢？
* 考虑 <span style="color:red">所有</span> 可能的参数值背后的原因
  * 乘以它们的 <span style="color:red">后验概率</span> 作为加权项
* 更具鲁棒性的预测
  * 可以更好地避免过拟合，尤其是对于小的训练集而言
  * 可以得到表达能力更强的模型类别（例如：贝叶斯 Logistic 回归是非线性模型）
  
### 3.3 频率学家 vs. 贝叶斯人的 “分歧”
* **<span style="color:red">频率学家：</span>** 使用 **点估计**、**正则化**、**p值** ... 进行学习
  * 简单的假设背后是复杂的理论支撑
  * 大部分算法都比较简单，非常偏实用的机器学习研究

* **<span style="color:red">贝叶斯人：</span>** 保留 **不确定性**，在进行统计推断时对未知因素进行 **边缘化（求和）**
  * 一些理论
  * 算法通常更加复杂，但并非总是如此
  * 通常（并非绝对）在计算上开销更高

## 4. 贝叶斯回归
**将贝叶斯推断应用于线性回归，对于 $\boldsymbol w$ 使用正态先验**
### 4.1 再谈线性回归
* 回忆线性回归的概率公式
  
  $$y\sim \text{Normal}(\boldsymbol x'\boldsymbol w, \sigma^2)$$
  
  $$\boldsymbol w\sim \text{Normal}(\boldsymbol 0,\gamma^2 \boldsymbol I_D)$$

  其中，$\boldsymbol I_D$ 是 $D\times D$ 的单位矩阵
* 贝叶斯规则

  $$p(\boldsymbol w|\boldsymbol X,\boldsymbol y)=\dfrac{p(\boldsymbol y|\boldsymbol X,\boldsymbol w)p(\boldsymbol w)}{p(\boldsymbol y|\boldsymbol X)}$$ <br>  

  这里，我们假设 $\boldsymbol w$ 与 $\boldsymbol X$ 之间互相独立：  
  $$p(\boldsymbol w|\boldsymbol X,\boldsymbol y) = \dfrac{p(\boldsymbol w,\boldsymbol X,\boldsymbol y)}{p(\boldsymbol X,\boldsymbol y)}
  =\dfrac{p(\boldsymbol y|\boldsymbol X,\boldsymbol w)p(\boldsymbol X,\boldsymbol w)}{p(\boldsymbol y|\boldsymbol X)p(\boldsymbol X)}
  =\dfrac{p(\boldsymbol y|\boldsymbol X,\boldsymbol w)p(\boldsymbol X)p(\boldsymbol w)}{p(\boldsymbol y|\boldsymbol X)p(\boldsymbol X)}
  =\dfrac{p(\boldsymbol y|\boldsymbol X,\boldsymbol w)p(\boldsymbol w)}{p(\boldsymbol y|\boldsymbol X)}$$

  $$\max \limits_{\boldsymbol w}p(\boldsymbol w|\boldsymbol X,\boldsymbol y)=\max \limits_{\boldsymbol w}p(\boldsymbol y|\boldsymbol X,\boldsymbol w)p(\boldsymbol w)$$

  这里，我们采用点估计避免计算边缘似然项。
* 导致目标函数惩罚化（岭回归）

### 4.2 贝叶斯线性回归
* 回退一步，考虑完全后验

  $$\begin{align}
  p(\boldsymbol w|\boldsymbol X,\boldsymbol y,\sigma^2)
  &= \dfrac{p(\boldsymbol y|\boldsymbol X,\boldsymbol w,\sigma^2)p(\boldsymbol w)}{p(\boldsymbol y|\boldsymbol X,\sigma^2)} \\
  &= \dfrac{p(\boldsymbol y|\boldsymbol X,\boldsymbol w,\sigma^2)p(\boldsymbol w)}{\int \color{red}{\underbrace{\color{black}{p(\boldsymbol y|\boldsymbol X,\boldsymbol w,\sigma^2)p(\boldsymbol w)}}_{p(\boldsymbol y,\boldsymbol w|\boldsymbol X,\sigma^2)}} \text{ d} \boldsymbol w}
  \end{align}$$
  
  这里，我们假设噪声的方差已知。
* 我们可以计算分母（<span style="color:red">边缘似然</span> 或者 <span style="color:red">证据</span>）吗？
  * 如果是这样，我们可以使用完全后验，而非仅仅是它的众数
* 我们有两个正态分布
  * 正态似然 $\times$ 正态先验
* 它们的乘积也是一个正态分布
  * **<span style="color:red">共轭先验：</span>** 当似然函数和先验的乘积结果的分布与先验分布相同时（即后验分布与先验分布属于同类），则先验分布与后验分布被称为 **共轭分布**，而先验分布被称为似然函数的 **共轭先验**。  
    例如，高斯分布家族在高斯似然函数下与其自身共轭(自共轭)。
  * 利用正态分布的归一化常数可以很容易地计算出 **证据（边缘似然）**
*  <span style="color:red">后验的闭合解（Closed Form Solution）</span>
  
  $$\begin{align}
  p(\boldsymbol w|\boldsymbol X,\boldsymbol y,\sigma^2) &\propto \text{Normal}(\boldsymbol w|\boldsymbol 0,\gamma^2\boldsymbol I_D)\text{Normal}(\boldsymbol y|\boldsymbol {Xw},\sigma^2\boldsymbol I_N) \\
  &\propto \text{Normal}(\boldsymbol w|\boldsymbol w_N,\boldsymbol V_N)
  \end{align}$$

  其中，$\boldsymbol w_N=\dfrac{1}{\sigma^2}\boldsymbol V_N\boldsymbol X'\boldsymbol y \;,\quad \boldsymbol V_N=\sigma^2(\boldsymbol X'\boldsymbol X+\dfrac{\sigma^2}{\gamma^2}\boldsymbol I_D)^{-1}$

  **注意：** 之前的均值（和众数）都是 MAP 的解。

  我们可以通过两个正态分布的乘积来验证：将指数部分合并在一起，并对常系数部分 “完成平方” 来表示为常系数的平方乘以一个指数部分（即正态分布）。  

  回顾之前 [Lecture 02 的 3.2 节](https://andy-tk.top/2019/11/06/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A002/) 中提到的例子：
* 我们对 $X\mid\theta$ 建模为 $\text{N}(\theta,1)$，先验为 $N(0,1)$
* 假设我们观测到 $X=1$，然后更新先验<br>

  $$\begin{align}
  P(\theta|X=1) &= \dfrac{P(X=1| \theta)P(\theta)}{P(X=1)} \quad\quad\color{purple}{\text{目标是将后验转换为已知分布形式。指数的二次方一定为正态}}\\
  &\propto P(X=1| \theta)P(\theta) \\
  &=\left[\color{purple}{\dfrac{1}{\sqrt{2\pi}}}\exp\left(-\dfrac{(1-\theta)^2}{2}\right)\right]\left[\color{purple}{\dfrac{1}{\sqrt{2\pi}}}\exp\left(-\frac{\theta^2}{2}\right)\right] \quad\quad\color{purple}{\text{丢弃关于 }\theta\text{ 的常数项}}\\
  &\propto \exp\left(-\dfrac{(1-\theta)^2+\theta^2}{2}\right) \quad\quad\color{purple}{\text{合并指数项}}\\
  &= \exp\left(-\dfrac{2\theta^2-2\theta+1}{2}\right) \\
  &= \exp\left(-\dfrac{\theta^2-\theta+\frac{1}{2}}{2\times \color{purple}{\frac{1}{2}}}\right) \quad\quad\color{purple}{\text{将分子项中 }\theta^2\text{ 的系数移到分母上}}\\
  &= \exp\left(-\dfrac{\theta^2-\theta+\color{purple}{\frac{1}{4}}}{2\times \frac{1}{2}}\right) \cdot \color{purple}{\exp\left(-\dfrac{\frac{1}{4}}{2\times \frac{1}{2}}\right)}  \quad\quad\color{purple}{\text{将分子项凑成平方形式：移除多余的常数项}}\\
  &\propto \exp\left(-\dfrac{\theta^2-\theta+\frac{1}{4}}{2\times \frac{1}{2}}\right)\\
  &= \exp\left(-\dfrac{(\theta-\frac{1}{2})^2}{2\times \frac{1}{2}}\right)  \quad\quad\color{purple}{\text{因式分解}}\\
  &\propto N(0.5,0.5) \quad\quad\quad\color{purple}{\text{发现为（非标准）正态分布}}
  \end{align}$$

  注意：允许将常量提到前面，并通过归一化 “忽略”

### 4.3 贝叶斯线性回归例子

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-19-WX20200220-003722%402x.png">

<span style="font-size:10pt">第 1 步：选择先验，这里是中心在原点 (0,0) 附近的球形</span> $\qquad \qquad \qquad \;$ <span style="font-size:10pt">第 2 步：观测训练数据</span>

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-19-WX20200220-003757%402x.png">

$\;\;$ <span style="font-size:10pt">第 3 步：根据先验和似然函数，写出后验的形式</span> $\qquad \qquad \qquad \quad$ <span style="font-size:10pt">第 4 步：从后验中采样</span>

### 4.4 顺序贝叶斯更新
* 可以为给定的数据集构建 
$$p(\boldsymbol w|\boldsymbol X,\boldsymbol y,\sigma^2)$$

* 如果我们观察到越来越多的数据会发生什么？
  1. 从先验 $p(\boldsymbol w)$ 开始
  2. 观测新的带标签的数据点
  3. 计算后验 
  $$p(\boldsymbol w|\boldsymbol X,\boldsymbol y,\sigma^2)$$
  4. <span style="color:red">将得到的后验视为当前的先验</span>，然后再从第 2 步开始重复这个过程

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-19-WX20200220-010534%402x.png" width="80%">

**<center><span aligned="enter" style="font-size:10pt">一个简单线性模型使用顺序贝叶斯学习的例子</span></center>**  
<center><span style="font-size:10pt">Source: <span style="font-style:italic">Pattern Recognition and Machine Learning (p.155)</span> by Bishop</span></center>

## 总结


下节内容：
