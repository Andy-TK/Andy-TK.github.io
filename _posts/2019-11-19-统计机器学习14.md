---
layout:     post   				    # 使用的布局（不需要改）
title:      统计机器学习 14：贝叶斯回归   	# 标题 
subtitle:   墨尔本大学 COMP90051 课程笔记 #副标题
date:       2019-11-19 				# 时间
author:     Andy 						# 作者
header-img: img/post-bg-sml.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 统计机器学习
    - 贝叶斯回归
---

<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

# Lecture 14 贝叶斯回归
## 主要内容
* **点估计未捕获到的不确定性**
* **贝叶斯方法保留不确定性**
* **顺序贝叶斯更新**
* **共轭先验（Normal-Normal）**
* **利用后验在测试集上进行贝叶斯预测**

## 1. 回顾贝叶斯
###  1.1 训练 = 优化？
**学习与推断阶段：**
* **<span style="color:steelblue">对于分类问题：</span>**
  * 建模（以 Logistic 回归为例）

    $$p(y|\boldsymbol x)=\text{sigmoid}(\boldsymbol x'\boldsymbol w)$$

  * 参数拟合数据

    $$\hat{\boldsymbol w}=\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol w}p(\boldsymbol y|\boldsymbol X,\boldsymbol w)p(\boldsymbol w)$$

  * 进行预测

    $$p(y_*|\boldsymbol x_*)=\text{sigmoid}(\boldsymbol x_*'\hat{\boldsymbol w})$$

* **<span style="color:steelblue">对于回归问题：</span>**
  * 建模

    $$p(y|\boldsymbol x)=\text{Normal}(\boldsymbol x'\boldsymbol w;\sigma^2)$$

  * 参数拟合数据

    $$\hat{\boldsymbol w}=\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol w}p(\boldsymbol y|\boldsymbol X,\boldsymbol w)p(\boldsymbol w)$$

  * 进行预测

    $$E[y_* ]=\boldsymbol x_*'\hat{\boldsymbol w}$$

<span style="color:red">$$\hat{\boldsymbol w}$$ 对应于 “点估计”</span>

### 1.2 贝叶斯替代
**$$\hat{\boldsymbol w}$$ 并没有什么特别之处……如果我们不只是使用参数的一个估计值呢？**
* **<span style="color:steelblue">对于分类问题：</span>**
  * 建模

    $$p(y|\boldsymbol x)=\text{sigmoid}(\boldsymbol x'\boldsymbol w)$$

  * 考虑那些可以比较好地拟合数据的 <span style="color:red">可能的参数空间</span>

    $$p(\boldsymbol w|\boldsymbol X,\boldsymbol y)$$

  * 进行 <span style="color:red">“期望的”</span> 预测

    $$p(y_*|\boldsymbol x_*)=E_{p(\boldsymbol w|\boldsymbol X,\boldsymbol y)}\left[\text{sigmoid}(\boldsymbol x_*'\boldsymbol w)\right]$$

* **<span style="color:steelblue">对于回归问题：</span>**
  * 建模

    $$p(y|\boldsymbol x)=\text{Normal}(\boldsymbol x'\boldsymbol w;\sigma^2)$$

  * 考虑那些可以比较好地拟合数据的 <span style="color:red">可能的参数空间</span>

    $$p(\boldsymbol w|\boldsymbol X,\boldsymbol y)$$

  * 进行 <span style="color:red">“期望的”</span> 预测

    $$p(y_*|\boldsymbol x_*)=E_{p(\boldsymbol w|\boldsymbol X,\boldsymbol y)}\left[\text{Normal}(\boldsymbol x_*'\boldsymbol w;\sigma^2)\right]$$

## 2. 不确定性
**如果用于训练的数据集很小，我们很少会完全信任任何从中学习到的模型。我们能否量化这种不确定性，并将其用于预测呢？**
### 2.1 重新审视回归问题

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-19-WX20200219-145612%402x.png" width="60%">  

<span style="color:red">线性回归：</span> $y=w_0+w_1x$  
这里，$y=$ humidity（湿度），$x=$ temperature（温度）

* 从数据中学习模型
  * 通过选择权重来最小化误差残差

    $$\hat{\boldsymbol w}=(\boldsymbol X'\boldsymbol X)^{-1}\boldsymbol X'\boldsymbol y$$

* 但是我们对于得到的 $\hat{\boldsymbol w}$ 和预测值有多大的信心？

### 2.2 我们应该相信点估计 $\hat{\boldsymbol w}$ 吗？
* 我们的学习算法有多稳定？
  <img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-19-WX20200219-152254%402x.png" width="80%">  
  **<center><span style="font-size:10pt">两个具有不同噪声水平的数据集以及它们各自对应的似然函数</span></center>**  
  <center><span style="font-size:10pt">（来源：<span style="font-style:italic">A First Course in Machine Learning (p.81)</span> by Rogers & Girolami）</span></center>  

  * $\hat{\boldsymbol w}$ 对于噪声高度敏感
  * 参数估计的不确定性有多少？
  * 如果目标参数的 **负对数似然（Negative Log Likelihood, NLL）** 的在峰值处越高且窄，说明我们掌握的信息量越大
* 形式化为 **费雪信息矩阵（Fisher Information Matrix）**
  * $E[ 2^{nd} \text{ deriv of NLL}]$  
    $\cal I$ $=\dfrac{1}{\sigma^2}\boldsymbol X'\boldsymbol X$
* 衡量关于 $\hat{\boldsymbol w}$ 的目标函数的曲率

## 3. 贝叶斯视角
**保留所有的未知因素（例如：参数的不确定性）并对它们进行建模，并且在进行统计推断时利用这些信息。**
### 3.1 一个贝叶斯人的视角
* 我们有理由认为 **所有的** 参数对于数据而言都是常数吗？
  * 对于训练数据拟合更好的权重的概率应该大于其他权重的概率
  * 利用所有可能的权重进行预测，乘以各自的概率作为缩放系数
* 这就是 <span style="color:red">贝叶斯推断</span> 背后的思想

### 3.2 参数的不确定性  
<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-19-WX20200219-191611%402x.png" width="40%" align="right">

* 目标函数有很多合理的解
  * 为什么只选择其中的某一个呢？
* 考虑 <span style="color:red">所有</span> 可能的参数值背后的原因
  * 乘以它们的 <span style="color:red">后验概率</span> 作为加权项
* 更具鲁棒性的预测
  * 可以更好地避免过拟合，尤其是对于小的训练集而言
  * 可以得到表达能力更强的模型类别（例如：贝叶斯 Logistic 回归是非线性模型）
  
### 3.3 频率学家 vs. 贝叶斯人的 “分歧”
* **<span style="color:red">频率学家：</span>** 使用 **点估计**、**正则化**、**p值** ... 进行学习
  * 简单的假设背后是复杂的理论支撑
  * 大部分算法都比较简单，非常偏实用的机器学习研究

* **<span style="color:red">贝叶斯人：</span>** 保留 **不确定性**，在进行统计推断时对未知因素进行 **边缘化（求和）**
  * 一些理论
  * 算法通常更加复杂，但并非总是如此
  * 通常（并非绝对）在计算上开销更高

## 4. 贝叶斯回归

$x+y=1 \tag {1}$

由公式$(1)$即可得到结论。



## 总结


下节内容：
