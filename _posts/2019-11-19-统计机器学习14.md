---
layout:     post   				    # 使用的布局（不需要改）
title:      统计机器学习 14：贝叶斯回归   	# 标题 
subtitle:   墨尔本大学 COMP90051 课程笔记 #副标题
date:       2019-11-19 				# 时间
author:     Andy 						# 作者
header-img: img/post-bg-sml.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 统计机器学习
    - 贝叶斯回归
---

<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

# Lecture 14 贝叶斯回归
## 主要内容
* **点估计未捕获到的不确定性**
* **贝叶斯方法保留不确定性**
* **顺序贝叶斯更新**
* **共轭先验（Normal-Normal）**
* **利用后验在测试集上进行贝叶斯预测**

## 1. 回顾贝叶斯
###  1.1 训练 = 优化？
**学习与推断阶段：**
* **<span style="color:steelblue">对于分类问题：</span>**
  * 建模（以 Logistic 回归为例）

    $$p(y|\boldsymbol x)=\text{sigmoid}(\boldsymbol x'\boldsymbol w)$$

  * 参数拟合数据

    $$\hat{\boldsymbol w}=\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol w}p(\boldsymbol y|\boldsymbol X,\boldsymbol w)p(\boldsymbol w)$$

  * 进行预测

    $$p(y_*|\boldsymbol x_*)=\text{sigmoid}(\boldsymbol x_*'\hat{\boldsymbol w})$$

* **<span style="color:steelblue">对于回归问题：</span>**
  * 建模（以 Logistic 回归为例）

    $$p(y|\boldsymbol x)=\text{Normal}(\boldsymbol x'\boldsymbol w;\sigma^2)$$

  * 参数拟合数据

    $$\hat{\boldsymbol w}=\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol w}p(\boldsymbol y|\boldsymbol X,\boldsymbol w)p(\boldsymbol w)$$

  * 进行预测

    $$E[y_* ]=\boldsymbol x_*'\hat{\boldsymbol w}$$

<span style="color:red">$$\hat{\boldsymbol w}$$ 对应于 “点估计”</span>

### 1.2 贝叶斯替代
**$$\hat{\boldsymbol w}$$ 并没有什么特别之处……如果我们不只是使用参数的一个估计值呢？**
* **<span style="color:steelblue">对于分类问题：</span>**
  * 建模（以 Logistic 回归为例）

    $$p(y|\boldsymbol x)=\text{sigmoid}(\boldsymbol x'\boldsymbol w)$$

  * 考虑那些可以比较好地拟合数据的 <span style="color:red">可能的参数空间</span>

    $$p(\boldsymbol w|\boldsymbol X,\boldsymbol y)$$

  * 进行 <span style="color:red">“期望的”</span> 预测

    $$p(y_*|\boldsymbol x_*)=E_{p(\boldsymbol w|\boldsymbol X,\boldsymbol y)}\left[\text{sigmoid}(\boldsymbol x_*'\boldsymbol w)\right]$$

* **<span style="color:steelblue">对于回归问题：</span>**
  * 建模（以 Logistic 回归为例）

    $$p(y|\boldsymbol x)=\text{Normal}(\boldsymbol x'\boldsymbol w;\sigma^2)$$

  * 考虑那些可以比较好地拟合数据的 <span style="color:red">可能的参数空间</span>

    $$p(\boldsymbol w|\boldsymbol X,\boldsymbol y)$$

  * 进行 <span style="color:red">“期望的”</span> 预测

    $$p(y_*|\boldsymbol x_*)=E_{p(\boldsymbol w|\boldsymbol X,\boldsymbol y)}\left[\text{Normal}(\boldsymbol x_*'\boldsymbol w;\sigma^2)\right]$$

## 2. 不确定性
**如果用于训练的数据集很小，我们很少会完全信任任何从中学习到的模型。我们能否量化这种不确定性，并将其用于预测呢？**
### 2.1 重新审视回归问题

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-19-WX20200219-145612%402x.png" width="60%">  

<span style="color:red">线性回归：</span> $y=w_0+w_1x$  
这里，$y=$ humidity（湿度），$x=$ temperature（温度）

* 从数据中学习模型
  * 通过选择权重来最小化误差残差

    $$\hat{\boldsymbol w}=(\boldsymbol X'\boldsymbol X)^{-1}\boldsymbol X'\boldsymbol y$$

* 但是我们对于得到的 $\hat{\boldsymbol w}$ 和预测值有多大的信心？

### 2.2 我们应该相信点估计 $\hat{\boldsymbol w}$ 吗？
* 我们的学习算法有多稳定？
  <img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-19-WX20200219-152254%402x.png" width="80%">  
  **<center><span style="font-size:10pt">两个具有不同噪声水平的数据集以及它们各自对应的似然函数</span></center>**  
  <center><span style="font-size:10pt">（来源：<span style="font-style:italic">A First Course in Machine Learning (p.81)</span> by Rogers & Girolami）</span></center>  

  * $\hat{\boldsymbol w}$ 对于噪声高度敏感
  * 参数估计的不确定性有多少？
  * 如果目标参数的 **负对数似然（Negative Log Likelihood, NLL）** 的在峰值处越高且窄，说明我们掌握的信息量越大
* 形式化为 **费雪信息矩阵（Fisher Information Matrix）**
  * $E[ 2^{nd} \text{ deriv of NLL}]$  
    $\cal I =\dfrac{1}{\sigma^2}\boldsymbol X'\boldsymbol X$






## 总结
* 随机多臂老虎机
  * 不确定性下的顺序决策
  * 最简单的 “探索-vs-利用” 的设定
  * $(\varepsilon)$-Greedy，UCB 算法
* 很多应用和变体：
  * 对抗 MAB：奖励不是随机的，而是任何东西
  * 上下文 MAB：行动基于上下文特征向量
  * 强化学习：更加通用的设定

下节内容：贝叶斯回归
