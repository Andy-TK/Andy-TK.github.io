---
layout:     post   				    # 使用的布局（不需要改）
title:      统计机器学习 19：降维   	# 标题 
subtitle:   墨尔本大学 COMP90051 课程笔记 #副标题
date:       2019-11-24 				# 时间
author:     Andy 						# 作者
header-img: img/post-bg-sml.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 统计机器学习
    - 降维

---

<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

# Lecture 19 降维
## 主要内容
* **主成成分分析（PCA）**
  * 线性降维方法
  * 对角化协方差矩阵
* **核化 PCA**

## 1. 关于降维
### 1.1 数据的真实维度？

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-27-WX20200227-151842%402x.png" width="80%">

### 1.2 降维
* 之前我们介绍了无监督学习中的一类常见任务：聚类
* **降维** 是指使用 <span style="color:red">较少数量的变量</span>（维度）来表示数据，同时保留数据中我们 <span style="color:red">“感兴趣的”</span> 结构
* 通过降维，我们可以达到以下目的：
  * <span style="color:red">可视化</span>（例如，将高维数据映射到 2D）
  * 提高 pipeline 中的 <span style="color:red">计算效率</span>
  * 提升 pipeline 中的数据压缩或者 <span style="color:red">统计效率</span>

### 1.3 探索数据结构
* 一般而言，降维会导致信息丢失
* 诀窍是确保大部分我们 “感兴趣的” 信息被保留下来，而丢失的大部分都属于噪声
* 这通常是可能的，因为相比那些记录的变量，真实数据具有的 <span style="color:red">内在维度可能要少得多</span>
* **例子：** GPS 坐标是 3D 的，而平坦道路上的汽车定位实际是在 2D 流形上的

## 2. 主成成分分析（PCA）
**寻找一种数据的旋转方式使得（新）变量之间的协方差最小化**

### 2.1 主成成分分析
* 主成成分分析（PCA）是普遍用于降维和数据分析的一种常用方法
* 给定一个数据集 $\boldsymbol x_1,...,\boldsymbol x_n$，其中 $\boldsymbol x_i\in \boldsymbol R^m$，PCA 的目标是找到一个新的坐标系，使得大部分方差都集中在第一个坐标轴上，然后剩下的大部分方差都集中在第二个坐标轴上，以此类推
* 然后，降维是基于 <span style="color:red">丢弃坐标</span> 实现的，我们仅保留前 $l$ 个坐标，丢弃后面的其余坐标（$l< m$）

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-27-WX20200227-160440%402x.png">

### 2.2 朴素 PCA 算法
**<span style="color:steelblue">朴素 PCA 算法描述：</span>**  
1.$\,$<span style="color:red">选择一个方向</span> 作为新的坐标轴，使得方差沿着该坐标轴是最大化的  
2.$\,$<span style="color:red">选择下一个方向</span> / 坐标轴垂直于当前所有的坐标轴，使得（余下的）方差着该坐标轴是最大化的  
3.$\,$重复步骤 2，直到你得到了和原始数据中同样数量的坐标轴（即，维度）  
4.$\,$将原始数据 <span style="color:red">投影</span> 到这些新的坐标轴上，从而得到新的坐标（“PCA 坐标”）  
5.$\,$对于每个数据点，仅保留 <span style="color:red">前 $l$ 个坐标</span>  
<br>
如果可以直接实现，那么该算法是有效的，但是，我们还有其他更好的解决方案

### 2.3 问题描述
* PCA 的核心是寻找一个新的坐标系，使得大部分方差都被 “早先的” 几个坐标轴捕获到
* 现在，让我们将这一目标的正式形式写出来，看一下如何实现
* 首先，回忆点积的几何定义 $\boldsymbol u\cdot \boldsymbol v=u_{\boldsymbol v}\\|\boldsymbol v\\|$



## 总结
* 无监督学习
  * 问题的多样性
* 高斯混合模型（GMM）
  * 一种聚类的概率方法
  * GMM 模型
  * GMM 聚类作为一个优化问题
* 期望最大化（EM）算法

下节内容：降维
