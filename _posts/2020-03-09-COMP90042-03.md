---
layout:     post   				    # 使用的布局（不需要改）
title:      自然语言处理 03：N-gram 语言模型   	# 标题 
subtitle:   墨尔本大学 COMP90042 课程笔记 #副标题
date:       2020-03-09 				# 时间
author:     Andy 						# 作者
header-img: img/post-bg-unimelb-blue.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 自然语言处理
    - COMP90042
    - 课程笔记
    - N-gram


---

<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

# Lecture 03 N-gram 语言模型

## 1. 引言
本节课我们将学习 **语言模型（Language Models）**，我们之前在第 1 节课中提到过它。

**为什么我们关心语言模型？**

因为 NLP 中有很大一部分研究都是关于如何 *解释语言（explaining language）* 的。
* 为什么有些句子比其他句子更 **流畅（fluent）**，或者说 **更自然（natural）**？  
    我们为什么关心这个问题呢？  
    因为在很多应用中，我们很关心语言的流畅性与自然性。  
    例如，在语音识别中，你听到一句话，并且想将它转换成文本，所以你需要区分这段语音可能对应的不同文本，并从中选择更加流畅和自然的版本。比如下面两个句子的发音很接近：  

    $$\textit{recognise speech > wreck a nice beach}$$

    从流畅和自然的角度考虑，显然，左边的句子更有可能代表了讲话者的本意。而语言模型可以帮你做到这一点，它会告诉你 “recognise speech” 是人们更倾向表达的意思。
* 那么，我们如何衡量这种 “优度（goodness）”（或者说流畅度、自然度）呢？  
    我们用 **概率** 来衡量它，而语言模型提供了一种很自然的方式来估计句子的概率。
* 在此基础上，一旦你构建了一个语言模型，你还可以用它来 **生成（generation）** 语言。

如果你还有印象，我们之前提到过这个名为 [Talk to Transformer](https://talktotransformer.com/) 的网站。现在，我们尝试通过这个小的语言模型来完成一个段落。我们在用户输入提示框中输入一句话：“*Today we have a lecture on Natural Language Processing.*” 然后点击 `GENERATE ANOTHER` 按钮，我们将看到这个在线语言模型自动生成了一些看上去相当不错的相关段落：

<video width="705" controls>
  <source src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-03-11-LanguageModel.mp4" type="video/mp4">
</video>

**语言模型可以用于哪些任务呢？**
* 主要用于：
  * 语音识别（Speech recognition）
  * 拼写纠错（Spelling correction）
  * 查询补全（Query completion）
  * 光学字符识别（Optical character recognition）
* 其他生成任务：
  * 机器翻译（Machine translation）
  * 概括（Summarisation）
  * 对话系统（Dialogue systems）

**本节课程大纲**
* 推导 n-gram 语言模型
* 平滑（Smoothing）处理稀疏性（sparsity）
* 生成语言
* 评估语言模型

## 2. N-gram 语言模型
### 2.1 概率：从联合概率到条件概率
我们的目标是得到一个由 $m$ 个单词组成的任意序列（即一个包含 $m$ 个单词的句子）的概率：

$$P(w_1,w_2,\dots,w_m)$$

第一步是利用链式法则（chain rule）将联合概率转换成条件概率的连乘形式：

$$P(w_1,w_2,\dots,w_m)=P(w_1)P(w_2\mid w_1)P(w_3\mid w_1,w_2)\cdots P(w_m\mid w_1,\dots,w_{m-1})$$

### 2.2 马尔可夫假设（The Markov Assumption）
目前，这仍然是一个比较棘手的问题，因为随着上下文的不断增加，我们构建的模型中将包含越来越多的参数。所以这里，我们采用一种称为 “马尔可夫假设” 的简化假设：某个单词出现的概率不再依赖于全部上下文，而是取决于离它最近的 $n$ 个单词。因此，我们得到：

$$P(w_i\mid w_1,\dots,w_{i-1})\approx P(w_i\mid w_{i-n+1},\dots,w_{i-1})
$$

对于某个很小的 $n$：  
* 当 $n=1$ 时，一个 unigram 模型：

  $$P(w_1,w_2,\dots,w_m)=\prod_{i=1}^{m}P(w_i)$$

  在 unigram 模型中，我们假设一个句子出现的概率等于其中每个单词单独出现的概率的乘积，这意味着每个单词出现的概率之间相互独立，即我们并不关心每个单词的上下文。
  
  <br>

* 当 $n=2$ 时，一个 bigram 模型：

  $$P(w_1,w_2,\dots,w_m)=\prod_{i=1}^{m}P(w_i\mid w_{i-1})$$

  在 bigram 模型中，我们假设句子中每个单词出现的概率都和它前一个单词出现的概率有关。

  <br>

* 当 $n=3$ 时，一个 trigram 模型：

  $$P(w_1,w_2,\dots,w_m)=\prod_{i=1}^{m}P(w_i\mid w_{i-2},w_{i-1})$$

  在 trigram 模型中，我们假设句子中每个单词出现的概率都和它前两个单词出现的概率有关。

### 2.3 最大似然估计
**我们如何计算这些概率？**

非常简单，我们只需要一个大的用于训练的语料库（corpus），然后我们就可以根据语料库中各个单词的计数（counts），利用最大似然估计来估计该单词出现的概率：

* 对于 unigram 模型：

  $$P(w_i)=\dfrac{C(w_i)}{M}$$

  其中，$C$ 是一个计数函数，$C(w_i)$ 表示单词 $w_i$ 在语料库中出现的次数，$M$ 表示语料库中所有单词 tokens 的数量。

  <br>

* 对于 bigram 模型：

  $$P(w_i\mid w_{i-1})=\dfrac{C(w_{i-1},w_i)}{C(w_{i-1})}$$

  其中，$C(w_{i-1},w_i)$ 表示单词 $w_{i-1}$ 和单词 $w_i$ 前后相邻一起出现的次数。

  <br>

* 对于 n-gram 模型：

  $$P(w_i\mid w_{i-n+1},\dots,w_{i-1})=\dfrac{C(w_{i-n+1},\dots,w_i)}{C(w_{i-n+1},\dots,w_{i-1})}$$

  同理，我们计算 n-gram 出现的次数，除以 (n-1)-gram（即上下文）出现的次数。

### 2.4 序列的开头和结尾表示
在我们进入例子之前，我们需要用一些特殊的记号表示一个序列的开始和结束：
* $$\texttt{<s>}$$ 表示句子的开始（对应于课本 E18 中的 $\square$）
* $$\texttt{</s>}$$ 表示句子的结束（对应于课本 E18 中的 $\blacksquare$）

### 2.5 Trigram 例子
现在，让我们来看一个玩具例子，假设我们有一个只包含两个句子的语料库。

* **语料库**：  
  $$\texttt{<s> <s> } \textit{yes no no no no yes}\texttt{ </s>}$$  
  $$\texttt{<s> <s> } \textit{no no no yes yes yes no}\texttt{ </s>}$$

  可以看到，每个句子开头有两个起始标记，因为我们采用的是 trigram 模型。

  <br>

* **我们希望知道下面的句子在一个 trigram 模型下的概率是多少？**

  $$\texttt{<s> <s> } \textit{yes no no yes}\texttt{ </s>}$$

  $$\begin{align}
  P(\text{sent} =\textit{yes no no yes}) &= P(\textit{yes}\mid \texttt{<s>},\texttt{<s>})\times P(\textit{no}\mid \texttt{<s>},\textit{yes})\times P(\textit{no}\mid \textit{yes},\textit{no})\\
  &\quad \times P(\textit{yes}\mid \textit{no},\textit{no}) \times P(\texttt{</s>} \mid \textit{no},\textit{yes})\\
  &= \dfrac{1}{2} \times 1 \times \dfrac{1}{2} \times \dfrac{2}{5} \times \dfrac{1}{2} \\
  &= 0.1
  \end{align}$$

  **说明**：
  * 首先，我们对要计算的句子的概率按照 trigram 模型拆分成条件概率的连乘形式。
  * 然后，对于等式右边的每一个条件概率项，按照 trigram 模型中的条件概率计算公式，分别统计 “当前单词连同它的上下文” 以及 “单独的上下文部分” 在语料库中出现的次数，并将两者相除，得到该项的计算结果。  
    例如，对于上面等式右边第一个条件概率项，我们考虑句子中第一个单词 “$\textit{yes}$” 及其相邻的 bigram 上下文 “$$\texttt{<s>}\, \texttt{<s>}$$”：
    
    $$P(\textit{yes}\mid \texttt{<s>},\texttt{<s>})=\dfrac{C(\texttt{<s>},\texttt{<s>},\textit{yes})}{C(\texttt{<s>},\texttt{<s>})}=\dfrac{1}{2}$$

    可以看到，子序列 “$$\texttt{<s>}\, \texttt{<s>} \,\textit{yes}$$” 在语料库中只出现过 $1$ 次；而子序列 “$$\texttt{<s>} \,\texttt{<s>}$$” 在语料库中一共出现了 $2$ 次，所以第一个条件概率项的结果为 $\frac{1}{2}$。其余各条件概率项的计算方式同理，另外请注意，在计算第四个条件概率项时，bigram 上下文 “$$\textit{no}\,\textit{no}$$” 在语料库中一共出现了 $5$ 次。

### 2.6 N-gram 语言模型的一些问题
* **语言通常具有长距离效应  —— 需要设置较大的 $n$ 值**  
  有些词对应的上下文可能出现在句子中距离该词很远的地方，这意味着如果我们采用固定长度的上下文（例如：trigram 模型） ，我们可能无法捕捉到足够的上下文相关信息。例如：
  
  $$\textit{The }\color{red}{\textit{lecture/s }}\textit{that took place last week } \color{red}{\textit{was/were }}\textit{on processing.}$$

  在上面的句子中，假如我们要决定系动词 $be$ 是应该用第三人称单数形式 $was$ 还是复数形式 $were$，我们需要回头看句子开头的主语是 $lecture$ 还是 $lectures$。可以看到，它们之间的距离比较远，如果我们采用 bigram 或者 trigram 模型，我们无法得到相关的上下文信息来告诉我们当前位置应该用 $was$ 还是 $were$。这是所有有限上下文语言模型（finite context language models）的一个通病。

  <br>

* **计算出的结果的概率通常会非常小**  
  你会发现，一连串条件概率项连乘得到的结果往往会非常小，对于这个问题，我们可以采用取对数计算 log 概率来避免数值下溢（numerical underflow）。

  <br>

* **对于不存在于语料库中的词，无法计算其出现概率**  
  如果我们要计算概率的句子中包含了一些没有在语料库中出现过的单词（例如：人名），我们应该怎么办？  
  一种比较简单的技巧是，我们可以用某种特殊符号（例如：$$\texttt{<UNK>}$$）来表示这些所谓的 OOV 单词（out-of-vocabulary，即不在词汇表中的单词），并且将语料库中一些非常低频的单词都替换为这种表示未知单词的特殊 token。

  <br>

* **出现在新的上下文（context）中的单词**  
  这个问题和前面的 OOV 单词的问题类似，我们的语料库中可能已经包含了这些单词，但是并没有包含该单词在新句子中对应的特定的 n-gram（即上下文信息）。这意味着该单词在新的句子中对应 n-gram 对于语言模型来说是全新的，而且因为 n-gram 的组合存在如此多的可能性，以至于语料库很难将所有的可能性都覆盖到。  
  例如：假设我们构建了一个 five-gram 语言模型，我们的语料库中一共有 20000 个词汇，那么一共存在多少种可能的 five -gram 组合？  
  答案是：$20000^5$。这是一个非常非常大的数字，以至于无论我们的训练语料库有多大，都不可能捕捉到所有的可能性组合。  
  这是一个相当常见的问题，并且很重要。如果我们回顾一下链式法则，可以看到所有的条件概率都连乘在一起，所以只要其中某一项在计算子序列时 n-gram 的计数为 $0$，计算的最终结果就会为 $0$。
  那么，我们应该怎么做呢？  
  默认情况下，









--------------
## 7. 扩展阅读
* _[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)_, by Jurafsky and Martin  
  * Chapter 2: Normalisation  
  * (包括关于正则表达式和 Levenshtien 编辑距离的回顾内容)
* 关于 Porter Stemmer 算法的具体细节请参考下面的链接：  
  <http://snowball.tartarus.org/algorithms/porter/stemmer.html>

下节内容：N-gram 语言模型


