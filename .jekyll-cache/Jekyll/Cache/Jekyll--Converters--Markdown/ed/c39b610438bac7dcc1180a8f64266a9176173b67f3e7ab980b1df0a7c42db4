I"<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-09-无模型强化学习q-learning-和-sarsa">Lecture 09 无模型强化学习：Q-Learning 和 SARSA</h1>

<p><strong>主要内容：</strong></p>
<ol>
  <li>动机</li>
  <li>强化学习</li>
  <li>Q-Learning</li>
  <li>SARSA</li>
  <li>总结</li>
</ol>

<h2 id="1-动机">1. 动机</h2>
<h3 id="11-学习成果">1.1 学习成果</h3>
<ol>
  <li>识别在哪些情况下，无模型强化学习适用于求解 MDP 问题。</li>
  <li>解释无模型规划与基于模型规划之间的差异。</li>
  <li>应用 Q-Learning 和 SARSA 手动解决小规模 MDP 问题，并编写 Q-Learning 和 SARSA 算法代码自动求解中等规模 MDP 问题。</li>
  <li>比较和对比非策略强化学习与策略强化学习。</li>
</ol>

<h3 id="12-规划与学习">1.2 规划与学习</h3>

<p>到目前为止，我们已经学习了盲目/启发式搜索和价值/策略迭代。</p>

<ul>
  <li>
    <p>搜索和价值/策略迭代都属于 <strong>基于模型</strong> 技术。这意味着我们需要知道模型；具体来说，我们知道 $P_a(s’\mid s)$ 和 $r(s,a,s’)$。</p>
  </li>
  <li>
    <p>Q-Learning 和 SARSA 则属于 <strong>无模型</strong> 技术。这意味着我们不知道 $P_a(s’\mid s)$ 和 $r(s,a,s’)$。</p>
  </li>
  <li>
    <p><strong>如果我们不知道转移和回报，我们该如何计算策略呢？</strong>我们通过尝试行动并观察结果，<strong>从经验中学习</strong>，从而使它成为一个机器学习问题。</p>
  </li>
  <li>
    <p>重要的是，在无模型强化学习中，我们不会尝试学习 $P_a(s’\mid s)$ 或 $r(s,a,s’)$ —— 我们将直接学习策略。</p>
  </li>
  <li>
    <p>另外，有些技术则介于基于模型和无模型之间：基于模拟的技术。在这种情况下，我们将模型视为一个 <strong>模拟器</strong>，因此我们可以利用无模型技术来 <strong>模拟</strong> $P_a(s’\mid s)$ 和 $r(s,a,s’)$ 并学习策略。</p>
  </li>
</ul>

<h2 id="2-强化学习">2. 强化学习</h2>
<h3 id="21-例子神秘游戏">2.1 例子：神秘游戏</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-05-23-WX20200523-193132%402x.png" width="40%" /></p>

<p><a href="https://programmingheroes.blogspot.com/2016/02/udacity-reinforcement-learning-mystery-game.html">游戏网站</a>：该游戏的目的是通过实验了解计算机是如何学习的。游戏通过按键盘上的数字 $1$ 到 $6$ 键进行操作。你需要了解行动产生的结果以及如何赢得比赛。</p>

<p>当你做得很好或很差时，会出现一些回报值。当你完成游戏时，会出现一行短语 “ You Win :)” 。祝你好运！</p>

<center><video width="300" controls="">
  <source src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-05-23-output.mp4" type="video/mp4" />
</video></center>

<p>现在，我们来玩这个游戏。我们可以看到游戏界面是一个 $6\times 6$ 的网格组成，其中有一些不同颜色和形状的色块。刚开始，我们对于游戏规则、获胜条件等一无所知，我们只知道一共有 $6$ 种行动，分别对应键盘上的数字 $1$ 到 $6$，当我们按下这些数字键时，会执行一些行动。</p>

<p>这个游戏背后的思想是：我们并不知道应该做什么，我们要做的就是开始探索。所以，一开始我们可能只是随机地按下键盘上的数字 $1$ 到 $6$ 键，然后进行观察，我们发现，通过这些数字键，我们可以控制较小的绿色方块的行为：</p>

<ul>
  <li>$1$：向上移动</li>
  <li>$2$：向左移动</li>
  <li>$3$：向右移动</li>
  <li>$4$：放下圆形色块</li>
  <li>$5$：向下移动</li>
  <li>$6$：拾取圆形色块</li>
</ul>

<p>并且，我们发现，黑色色块和地图边缘起到了墙的作用，
当我们成功拾取圆形色块时会得到 $+1$ 的回报，当我们在非指定区域放下圆形色块时会得到 $-1$ 的回报，而当我们将圆形色块运送到指定区域（四个角落上对应颜色区域，即右上角的绿色区）时，我们将获得胜利（“ You Win :)”）。</p>

<p>此外，当刷新页面重新开始游戏时，我们会发现可移动小方块和圆形色块的颜色发生了变化（例如：红色），作为人类，我们可以很容易推测此时指定区域也发生了变化（即左上角的红色区），但是对于计算机而言</p>

<ul>
  <li>
    <p>你采取了什么程序？</p>
  </li>
  <li>
    <p>你学到了什么？</p>
  </li>
  <li>
    <p>你使用了什么假设？</p>
  </li>
</ul>

<p>$\to$ 想象一下，对于没有任何假设或直觉的计算机来说有多难！</p>

<p>下节内容：蒙特卡洛树搜索：利用和探索的权衡</p>

:ET