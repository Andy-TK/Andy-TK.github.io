I":%<h1 id="lecture-11-上下文表示">Lecture 11 上下文表示</h1>

<p>这节课我们学习 <strong>上下文表示（Contextual Representation）</strong>，即单词在上下文中的含义。</p>

<h2 id="1-上下文表示">1. 上下文表示</h2>
<h3 id="11-词向量嵌入">1.1 词向量/嵌入</h3>
<p>在之前的章节中，我们已经学习过 <strong>词向量/嵌入（Word Vectors/Embeddings）</strong>，我们还学习了如何通过基于计数的方法来得到词向量。</p>

<ul>
  <li>每个单词 type 都有一个表示。
    <ul>
      <li>Word2Vec</li>
    </ul>
  </li>
  <li>无论单词的上下文是什么，我们得到的单词表示都是相同的。<br />
通过这种方式，无论这些单词在句子中是如何被使用的或者出现在句子中的哪个地方，以及它们的相邻单词是什么，模型学习到的每个单词 type 的词向量/嵌入都只有一种表示。我们称之为 <strong>上下文无关词向量/嵌入（Contextual Independent Word Vectors/Embeddings）</strong>。</li>
  <li>这种上下文无关词向量没有捕获到 <strong>单词的多义性（multiple senses of words）</strong>。<br />
例如：对于单词 “$\textit{duck}$”，其既可以表示鸭子这种动物，也可以表示躲避这一动作。而我们之前的词向量没有办法捕获到这两种含义之间的差异，因为对于同一单词我们只有一种向量表示。</li>
  <li><strong>上下文表示（Contextual representation）$=$ 基于上下文的单词表示</strong><br />
如果一个单词在两个句子中的含义不同，那么我们将得到该单词的两种不同的上下文表示。</li>
  <li>但是，更重要的是，我们发现预训练的上下文表示在大部分下游任务中的表现都 <strong>相当出色</strong>。这种基于上下文的单词表示已经在目前的 NLP 系统中充当着基石的角色。</li>
</ul>

<h3 id="12-rnn-语言模型">1.2 RNN 语言模型</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-17-WX20200617-155248%402x.png" width="80%" /></p>

<p>所以，我们应当如何学习到这种上下文表示呢？</p>

<p>这里，我们有一个 RNN 语言模型：“$\textit{a cow eats grass}$”。这里，RNN 模型试图预测下一个单词：给定单词 “$\textit{a}$”，RNN 模型试图预测下一个单词 “$\textit{cow}$”；给定单词 “$\textit{cow}$”，它试图预测下一个单词 “$\textit{eats}$” 等等。</p>

<p>下面是一个简单的 RNN 语言模型：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-17-WX20200617-160232%402x.png" width="90%" /></p>

<p>模型一共 3 层：输入层是单词的 one-hot 向量；隐藏层作为中间层；输出层用于预测下一个单词。其中，隐藏层的计算公式如右边所示：其接受前一个时间步（time-step）的隐藏状态 $s_{i-1}$，并结合当前输入 $x_i$，然后加上一个偏置项 $b$，然后输入一个非线性激活函数 $\tanh$，然后我们得到当前时间步的隐藏状态 $s_i$；之后，我们将当前时间步的隐藏状态 $s_i$ 输入到一个 $\mathrm{softmax}$ 函数中，得到词汇表中的所有单词的在当前时间步的概率分布。</p>

<p>我们知道，词嵌入对应上面的矩阵 $W_x$，我们可以将隐藏状态 $s_i$ 从某种程度上解释为当前单词的上下文表示。为什么可以这样解释呢？假设当前输入单词为 “$\textit{eats}$”，我们计算出该单词的隐藏状态，该 隐藏状态不仅捕获了单词 “$\textit{eats}$” 的信息，而且还包括之前见过的历史单词：“$\textit{a}$” 和 “$\textit{cow}$”。所以，我们可以将 RNN 语言模型中的隐藏状态从某种程度上视为一种上下文单词表示。</p>

<p>那么，问题解决了吗？</p>

<ul>
  <li>
    <p>几乎解决了，但是还没有完全解决。因为该 RNN 语言模型得到的单词的上下文表示仅仅捕获了该单词左边的上下文。<br />
例如：对于单词 “$\textit{cow}$”，其隐藏状态仅仅捕获了其前面出现过的单词 “$\textit{a}$” 的信息，而没有捕获到其后面出现的单词 “$\textit{eats}$” 的信息。</p>
  </li>
  <li>
    <p>解决方案：使用 <strong>双向 RNN（bidirectional RNN）</strong>模型替代。</p>
  </li>
</ul>

<h3 id="13-双向-rnn">1.3 双向 RNN</h3>

<p>现在，我们来看一下如何利用双向 RNN 模型来捕获当前单词左右两侧的上下文信息，从而得到当前单词的上下文表示。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-17-WX20200617-162852%402x.png" width="90%" /></p>

<p>我们有一个简单的 RNN1，和之前一样，我们有句子 “$\textit{a cow eats grass}$”。其中，$s_0, s_1, s_2, s_3$ 表示每个单词 $x_i$ 的前一个时间步的隐藏状态，即 $s_{i-1}$；输出的当前隐藏状态 $s_i$ 捕获了基于之前单词的上下文表示。然后，我们可以简单地添加一个反向 RNN2，从右向左进行，以捕获当前单词右边的上下文单词，同样，我们会得到一个输出的当前隐藏状态 $u_i$。然后，我们可以连接前向和后向两个 RNN 的隐藏状态 $s_i$ 和 $u_i$，从而得到一个同时捕获了当前单词两侧上下文单词信息的词表示。</p>

<p>还是以单词 “$\textit{cow}$” 为例，我们的预测单词为 “$\textit{eats}$”。这里，当前单词 “$\textit{cow}$” 的上下文表示由两部分构成：其中指向预测单词 “$\textit{eats}$” 的蓝色箭头表示隐藏状态 $s_2$ 捕获的左边的上下文单词 “$\textit{a}$” 的信息，而指向预测单词 “$\textit{eats}$” 的红色箭头表示隐藏状态 $u_1$ 捕获的右边的上下文单词 “$\textit{eats}$” 和 “$\textit{grass}$” 的信息。</p>

<p>所以，通过双向 RNN 模型，我们可以得到同时包含当前单词两侧信息的上下文表示，并且，无需另外设计新的模型或者架构。</p>

<h2 id="2-elmo">2. ELMo</h2>

<p>双向 RNN 这种思路也启发了 <strong>ELMo</strong> 模型：它是一种非常流畅自然的单词上下文表示模型，并且在大部分的 NLP 任务中都取得了非常好的效果。</p>

<h3 id="21-elmo基于语言模型的嵌入">2.1 ELMo：基于语言模型的嵌入</h3>

<p><strong>ELMo</strong> 表示 <strong>基于语言模型的嵌入（Embeddings from Language Models）</strong>。</p>
<ul>
  <li>Peters et al. (2018): <a href="https://arxiv.org/abs/1802.05365v2">https://arxiv.org/abs/1802.05365v2</a></li>
  <li>ELMo 在一个包含 1B（10 亿）单词的语料库上训练了一个双向多层 LSTM 语言模型。</li>
  <li>它结合了来自 LSTM 的 <strong>多层（multiple layers）</strong>的隐藏状态，并用于下游任务中。
    <ul>
      <li>这是 ELMo 的创新点之一：因为之前关于预训练模型的上下文表示研究只使用了顶层的信息，因此并没有在性能上获得太大提升。而对于 ELMo，假如我们使用了一个 2 层的 LSTM，那么我们将同时使用第一层和第二层的 LSTM 的输出。</li>
    </ul>
  </li>
  <li>最重要的是，研究发现，仅仅通过增加一些预训练的上下文词嵌入，就能在大部分的 NLP 任务中取得较大提升。</li>
</ul>

<h3 id="22-elmo-架构">2.2 ELMo 架构</h3>

<ul>
  <li>LSTM 层数 $= 2$</li>
  <li>LSTM 隐藏层维度 $= 4096$</li>
  <li>使用 <strong>字符级的卷积神经网络（Character CNN）</strong>来创建词嵌入。
    <ul>
      <li>没有未知单词</li>
    </ul>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-17-WX20200617-174212%402x.png" width="60%" /></p>

    <p>例如：对于单词 “$\textit{Playing}$”，相比直接创建一个该单词的词嵌入，ELMo 选择将其 token 化为一个个英文字母：“$\textit{P}$”、“$\textit{l}$”、“$\textit{a}$”、“$\textit{y}$”、“$\textit{i}$”、“$\textit{n}$”、“$\textit{g}$”。然后我们学习得到单词中每个字母的字符嵌入，并且在其前后添加 paddings 以保证最终得到的单词嵌入的长度一致。然后将其喂给一个带最大池化层的 CNN 模型，来创建一个基于字符嵌入的单词 “$\textit{Playing}$” 的表示。</p>

    <p><br /></p>

    <p>那么，为什么要这样做呢？因为这样可以基本解决未知单词的问题。例如：假设我们在语料库中没有见过单词 “$\textit{Playing}$”，那么当遇到这个单词时，我们需要另外用一个未知单词嵌入来表示它。而如果我们将其分解为字符嵌入，那么只要我们的语料库中包含了这些字符，我们就不会遇到未知单词的问题。通常，这种方法很简单，因为组成单词的字符通常都是有限的（例如：不考虑大小写的话，英文单词都是由 26 个字母组成）。所以，这是一种可以避免未知单词问题的方法。</p>
  </li>
</ul>

<h3 id="23-提取上下文表示">2.3 提取上下文表示</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-17-WX20200617-181407%402x.png" /></p>

<p>那么，我们如何提取这种上下文表示呢？</p>

<p>首先，我们在 10 亿单词语料库上对双向预训练</p>

<h2 id="4-扩展阅读">4. 扩展阅读</h2>
<ul>
  <li>JM3, Ch 6</li>
</ul>

<p>下节内容：上下文表示</p>

:ET