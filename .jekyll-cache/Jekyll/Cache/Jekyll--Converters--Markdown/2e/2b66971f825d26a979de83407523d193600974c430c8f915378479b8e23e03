I",%<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-06-序列标注隐马尔可夫模型">Lecture 06 序列标注：隐马尔可夫模型</h1>

<p>本节课我们将学习一种自动词性标注模型：<strong>隐马尔可夫模型（Hidden Markov Models, HMM）</strong>。尽管在本节内容中我们将学习如何将 HMM 应用于词性标注，但实际上它是一种通用的机器学习模型，可以应用于任何序列标签问题。</p>

<h2 id="1-从局部分类器到隐马尔可夫模型">1. 从局部分类器到隐马尔可夫模型</h2>
<h3 id="11-词性标注pos-tagging重新回顾">1.1 词性标注（POS Tagging）重新回顾</h3>
<ul>
  <li>$\textit{Janet will back the bill}$</li>
  <li>$\textit{Janet}$<span style="color:red">/NNP</span> $\textit{will}$<span style="color:red">/MB</span> $\textit{back}$<span style="color:red">/VP</span> $\textit{the}$<span style="color:red">/DT</span> $\textit{bill}$<span style="color:red">/NN</span></li>
  <li><strong>局部分类器</strong>：我们可以将每个单词的词性预测作为一个分类问题。
    <ul>
      <li>例如：现在，我们希望预测 “$\textit{bill}$” 这个单词的词性，我们可以将其当成一个分类问题。输入为 “$\textit{bill}$”，以及一些其他特征，例如前面的单词 “$\textit{the, back}$” 以及它们已经预测出的词性 “DT, VP”。然后进行预测，从所有的候选词性标签中对单词 “$\textit{bill}$” 进行词性分类。</li>
      <li>容易发生 <strong>错误传播（error propagation）</strong>。<br />
因为在预测过程中，我们将前面单词词性的预测结果作为当前预测单词的输入特征之一，所以，如果前面单词的词性预测错误，会影响后续单词词性的预测。</li>
    </ul>
  </li>
  <li>如果将 <strong>整个序列当成一个 “类别（class）”</strong> 会怎么样?
    <ul>
      <li>输出：“NNP_MB_VP_DT_NN”</li>
      <li>在之前的局部（单词）层面处理词性分类会碰到错误传播的问题，我们可以尝试在句子层面进行词性标注以避免这个问题。这里，我们将整个单词序列的词性预测作为一个分类问题。例如，输入是一个单词序列 “$\textit{Janet will back the bill}$”，输出是整个单词序列的标签类别 “NNP_MB_VP_DT_NN”。</li>
    </ul>
  </li>
  <li>但是，这种方法实现起来非常困难，因为存在以下问题：
    <ul>
      <li>可能的组合数量会呈几何级数增长：<br />
候选词性数量为 $|Tags|$，序列长度为 $M$，可能的标签序列类别数量为 $|Tags|^M$。<br />
例如：有 $80$ 个可选词性，序列中包含 $20$ 个单词，可能的类别数量为 $80^{20}$。<br />
所以，无论我们的训练集有多大，都不可能覆盖所有可能的类别。</li>
      <li>另一个问题是如何处理不同长度的句子：<br />
即便我们的训练集可以覆盖 $80^{20}$ 种序列类别，但是，当新的预测序列包含其他数量的单词时（例如：$5$ 个 或者 $40$ 个单词），该方法会再次失效。</li>
    </ul>
  </li>
</ul>

<h3 id="12-一种更好的方法">1.2 一种更好的方法</h3>
<ul>
  <li>标注（Tagging）是一个句子层面的任务，因为我们并不是单独对每个单词进行词性标注，而是需要考虑全局上下文，结合整个句子的语境来判断单词的词性。但是作为人类，我们可以将其 <strong>分解（decompose）</strong> 为若干个小的单词层面的任务：对于一个句子，我们倾向于先大致理解其含义，例如哪些是主语，哪些是主要的动词等，然后再深入每个单词，看一下其最有可能的词性是什么。</li>
  <li>解决方案：
    <ul>
      <li>定义一个模型，可以将整个过程分解为单个单词层面的步骤。</li>
      <li>但是在学习和预测的时候考虑整个序列（没有错误传播），即预测的标签序列对于整个句子的概率是最优的。</li>
    </ul>

    <p>因此，我们仍然是从句子层面考虑这个问题，但是我们的模型能够将其分解为一些更小的单词层面的子任务。</p>
  </li>
  <li>这就是 <strong>序列标注（sequence labelling）</strong> 的思想，或者更一般地说，<strong>结构预测（structured prediction）</strong>。</li>
</ul>

<h3 id="13-一种概率模型">1.3 一种概率模型</h3>
<p>现在，我们介绍一种概率模型，并在此基础上推导出隐马尔可夫模型。</p>
<ul>
  <li>目标：从句子 $\boldsymbol w$ 中获得最佳标签序列 $\boldsymbol t$。
    <ul>
      <li>
        <p>$\hat{\boldsymbol t}=\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol t}P(\boldsymbol t\mid \boldsymbol w)$
<br /></p>
      </li>
      <li>
        <p>$\hat{\boldsymbol t}=\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol t}\dfrac{P(\boldsymbol w\mid \boldsymbol t)P(\boldsymbol t)}{P(\boldsymbol w)}=\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol t}P(\boldsymbol w\mid \boldsymbol t)P(\boldsymbol t)$ $\qquad$ （贝叶斯定理）</p>
      </li>
    </ul>
  </li>
  <li>让我们将其分解：
    <ul>
      <li>
        <p>$P(\boldsymbol w\mid \boldsymbol t)=\prod_{i=1}^{n}P(w_i\mid t_i)$ $\qquad$ （一个单词仅依赖于其词性标签的概率）
<br /></p>
      </li>
      <li>
        <p>$P(\boldsymbol t)=\prod_{i=1}^{n}P(t_i\mid t_{i-1})$ $\qquad$ （一个词性标签仅依赖于其前一个词性标签的概率）</p>
      </li>
    </ul>
  </li>
  <li>这里，我们采用了两个类似于 bi-gram 语言模型中的非常强的 <strong>独立假设</strong>：
    <ul>
      <li>假设单词 $w_i$ 出现的概率仅取决于其词性 $t_i$</li>
      <li>假设词性 $t_i$ 出现的概率仅取决于其前一个词性 $t_{i-1}$</li>
    </ul>
  </li>
  <li>这就是 <strong>隐马尔可夫模型（Hidden Markov Model, HMM）</strong></li>
</ul>

<h2 id="2-隐马尔可夫模型">2. 隐马尔可夫模型</h2>

<script type="math/tex; mode=display">\hat{\boldsymbol t}=\mathop{\operatorname{arg\,max}}\limits_{\boldsymbol t}P(\boldsymbol w\mid \boldsymbol t)P(\boldsymbol t)</script>

<script type="math/tex; mode=display">P(\boldsymbol w\mid \boldsymbol t)=\prod_{i=1}^{n}P(w_i\mid t_i)</script>

<script type="math/tex; mode=display">P(\boldsymbol t)=\prod_{i=1}^{n}P(t_i\mid t_{i-1})</script>

<ul>
  <li>为什么是 “马尔可夫（Markov）”?<br />
因为该模型假设序列服从一个 <strong>马尔可夫链（Markov chain）</strong>：一个事件（词性标签 tag）的概率仅取决于前一个事件（词性标签 tag）。</li>
  <li>为什么是 “隐（Hidden）”?<br />
因为这些事件（词性标签 tags）是不可见的：我们仅仅能看到单词序列，而需要预测的词性标签 tags 是未观测到的，我们的目标是寻找最佳的词性标签序列。</li>
</ul>

<h3 id="21-hmm-的训练过程">2.1 HMM 的训练过程</h3>
<ul>
  <li>参数是单独的概率 $P(w_i\mid t_i)$ 和 $P(t_i\mid t_{i-1})$<br />
分别对应 <strong>发射（emission）</strong>$(O)$ 和 <strong>转移（transition）</strong>$(A)$ 概率</li>
  <li>训练过程利用 <strong>最大似然估计（MLE）</strong><br />
在朴素贝叶斯和 n-gram 语言模型中，这一步可以通过简单地计算不同类别的词频完成。</li>
  <li>
    <p>在 HMM 中，我们采用 <strong>完全相同的做法</strong>，例如：</p>

    <ul>
      <li>
        <p>$P(\textit{like}\mid \text{VB})=\dfrac{count(\text{VB},\textit{like})}{count(\text{VB})}$</p>
      </li>
      <li>
        <p>$P(\text{NN}\mid \text{DT})=\dfrac{count(\text{DT},\text{NN})}{count(\text{DT})}$</p>
      </li>
    </ul>

    <p>对于发射概率 $P(\textit{like}\mid \text{VB})$，我们计算语料库中所有词性被标注为 $\text{VB}$ 的单词 “$\textit{like}$” 的数量，然后除以所有的 $\text{VB}$ 词性的数量。对于转移概率 $P(\text{NN}\mid \text{DT})$，我们计算语料库中词性 $\text{DT, NN}$ 的共现次数，除以 $\text{DT}$ 出现的总次数。这里和之前的语言模型相比，唯一的差别就是语言模型的语料库仅包含文本，而这里的语料库包含文本以及相应的词性标注。</p>
  </li>
  <li>序列中的第一个 tag 应该如何处理呢？
    <ul>
      <li>和之前的 n-gram 语言模型的处理方式一样，我们引入一个起始符号 “<script type="math/tex">% <![CDATA[
\texttt{<s>} %]]></script>” 代表序列的开始，例如：
<script type="math/tex">P(NN\mid)</script></li>
    </ul>
  </li>
</ul>

<h2 id="4-总结">4. 总结</h2>
<ul>
  <li>词性（Part of speech）是语言学和自动文本分析之间的一个基本交汇点。</li>
  <li>NLP 中的一项基本任务，可以为许多其他应用程序提供有用的信息。</li>
  <li>应用于词性标注的方法通常是语言学任务中的典型代表，例如：概率方法、序列机器学习</li>
</ul>

<h2 id="5-扩展阅读">5. 扩展阅读</h2>
<ul>
  <li>JM3 Ch. 8 8.1-8.3, 8.5.1</li>
</ul>

<p>下节内容：序列标注：隐马尔可夫模型</p>

:ET