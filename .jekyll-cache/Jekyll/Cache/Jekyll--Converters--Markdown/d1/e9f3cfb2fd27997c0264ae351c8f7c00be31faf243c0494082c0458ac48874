I"]E<h1 id="lecture-05-主成分分析-二">Lecture 05 主成分分析 (二)</h1>

<p><strong>参考教材</strong>：</p>

<ul>
  <li><em>Hardle, W. and Simar, L (2015). Applied multivariate statistical analysis, 4th edition.</em></li>
  <li><em>Hastie, T. Tibshirani, R. and Friedman, J. (2009). The elements of statistical learning, 2nd edition</em></li>
</ul>

<h2 id="1-主成分的解释">1. 主成分的解释</h2>

<p>请记住，我们的目的是通过尽可能少的 PC 来解释 $X_i$ 尽可能多的变化 (低维投影，以便于数据可视化)。</p>

<p>并且，我们有</p>

<script type="math/tex; mode=display">\mathrm{Var}(Y_{ij}) = \lambda_j, \quad \text{for}\; j=1,\dots,p</script>

<p>以及</p>

<script type="math/tex; mode=display">\sum_{j=1}^{p}\lambda_j = \sum_{j=1}^{p} \mathrm{Var}(Y_{ij}) = \mathrm{tr}(\Sigma) = \sum_{j=1}^{p} \mathrm{Var}(X_{ij})</script>

<p>所以，前 $q$ 个主成分能够在多大程度上解释数据的变化可以通过以下比例来度量：</p>

<script type="math/tex; mode=display">\psi_q = \dfrac{\sum_{j=1}^{q}\lambda_j}{\sum_{j=1}^{p}\lambda_j}</script>

<p>在上节课的瑞士纸币的例子中，我们有：</p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">表 1</span>：瑞士纸币数据：特征值、解释方差比例和累积比例。</span></p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-20-WX20201120-144300%402x.png" width="70%" /></p>

<p>因此，我们有</p>

<script type="math/tex; mode=display">\psi_1 = 0.668, \;\psi_2 = 0.876,\; \psi_3 = 0.930,\; \psi_4 = 0.973,\; \psi_5 = 0.992,\; \psi_6 = 1.000</script>

<p>也就是说，第一个主成分解释了数据中 $66.8\%$ 的变化，前两个主成分一起解释了 $87.6\%$ 的变化，前三个主成分一起解释了 $93\%$ 的变化。</p>

<p>通常，我们会将 $\psi_q$ 绘制在图中，称为 <strong>碎石图 (scree plot)</strong>，它将按照 $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p$ 显示，这使我们能够查看哪些成分贡献最大。</p>

<p>在 R 中，我们可以通过 <code class="language-plaintext highlighter-rouge">prcomp</code> 获得 PCA 结果，并用 <code class="language-plaintext highlighter-rouge">screeplot</code> 来绘制碎石图：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-20-WX20201120-145604%402x.png" width="80%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 1</span>：瑞士纸币数据的碎石图，横轴表示 PC，纵轴表示该 PC 解释的方差 (即特征值)。</span></p>

<p>通过观察碎石图，我们可以决定需要考虑的 PC 数量：</p>

<ul>
  <li>考虑全部 PC 是没有意义的，因为这样无法实现降维 (总共有 $p$ 个 PC)。</li>
  <li>另一方面，我们应保留足够数量的 PC，以获取有关数据的大量信息。</li>
  <li>我们需要保留足够数量的 PC，使得它们所解释的累计方差百分比足够大。</li>
  <li>通常，我们会找到碎石图中的某个 <strong>拐点 (elbow)</strong> 并停在那里：以瑞士纸币为例，我们可以停在第 3 个 PC 处。</li>
</ul>

<p>在瑞士纸币的例子中，前两个 PC 一起解释了 $87.6\%$ 的数据方差，理论上看已经足够；但是前 3 个 PC 一起解释了 $93\%$ 的数据方差，要比之前更好一些。</p>

<p>然而，第三个 PC 也只是增加了少量额外信息 (在某些情况下，少量信息实际上可能包含了之前的 PC 无法捕获到的关于数据的一些有趣的方面，因此我们需要对其进行检查)。</p>

<p>通常，最初的几个 PC 共同解释了数据可变性的很大一部分，并且可以很容易从碎石图中看到。如果最初的 3 个或 4 个 PC 无法解释数据的大部分可变性，则意味着我们无法通过 PC 方法有效实现降维 (这种情况下，如果显著减小维数，则会丢失有关数据的太多信息)。</p>

<p>我们可以计算每个主成分变量 $Y_{ik}$ 和原始数据向量 $X_i$ 之间的协方差。</p>

<p>由于我们可以构造多达 $p$ 个主成分，我们可以将 $X_i$ 的 $p$ 个 PC 表示为：</p>

<script type="math/tex; mode=display">Y_i = (Y_{i1},\dots, Y_{ip})^{\mathrm T}</script>

<p>其中，$Y_{ik} = \gamma_k^{\mathrm T} (X_i - \mu)$。</p>

<p>我们可以通过两个向量之间的协方差矩阵来计算 $X_i$ 和 $Y_i$ 中的所有成对元素之间的协方差：</p>

<ul>
  <li>
    <p>令 $\Gamma = (\gamma_1,\dots,\gamma_p)$，它包含了 $\Sigma$ 的 $p$ 个特征向量。</p>
  </li>
  <li>
    <p>令 $\Lambda = \mathrm{diag}(\lambda_1,\dots,\lambda_p)$，它包含了 $p$ 个对应的降序排列的特征值，并且令 $\gamma_{jk}$ 表示 $\gamma_j$ 的第 $k$ 个分量。并且，回忆一下，我们用 $\sigma_{jk}$ 表示 $\Sigma$ 中的第 $(j,k)$ 个元素。</p>
  </li>
  <li>
    <p>我们有</p>

    <script type="math/tex; mode=display">\mathrm{Cov}(X_i,Y_i) = \Gamma \Lambda</script>

    <p>所以</p>

    <script type="math/tex; mode=display">\mathrm{Cov}(X_{ij}, Y_{ik}) = \gamma_{kj}\lambda_k</script>

    <p>因此，</p>

    <script type="math/tex; mode=display">\rho_{X_{ij}, Y_{ik}} = \dfrac{\gamma_{kj}\lambda_k}{(\sigma_{jj}\lambda_k)^{1/2}} = \gamma_{kj} \dfrac{\lambda_k^{1/2}}{(\sigma_{jj})^{1/2}}</script>

    <p>注意：在实践中，这些量均由其经验版本代替。</p>
  </li>
  <li>
    <p>然后，我们有</p>

    <script type="math/tex; mode=display">\sum_{k=1}^{p} \rho_{X_{ij}, Y_{ik}}^2 = \dfrac{\sum_{k=1}^{p}\gamma_{kj}^2 \lambda_k}{\sigma_{jj}} = \dfrac{\sigma_{jj}}{\sigma_{jj}} = 1</script>

    <p><strong>推导过程</strong>：</p>

    <p>我们用 $Y$ 表示 PC 向量，$X$ 表示原始数据向量：</p>

    <script type="math/tex; mode=display">Y = \begin{pmatrix}Y_1 \\ \vdots \\ Y_p \end{pmatrix}\quad \text{and} \quad X = \begin{pmatrix}X_1 \\ \vdots \\ X_p \end{pmatrix}</script>

    <p>$\Gamma$ 表示特征向量矩阵：</p>

    <script type="math/tex; mode=display">\Gamma = (\gamma_1,\dots,\gamma_p) \quad \text{where} \; \gamma_j = \begin{pmatrix}\gamma_{j1} \\ \vdots \\ \gamma_{jp} \end{pmatrix}</script>

    <p>由于 $\Gamma$ 是一个正交矩阵，$\Gamma \Gamma^{\mathrm T} = \mathcal I_p$，我们有</p>

    <script type="math/tex; mode=display">Y = \Gamma^{\mathrm T} X \quad \Longrightarrow \quad X = \Gamma Y \quad \Longrightarrow \quad X_j = \sum_{k=1}^{p} \gamma_{kj} Y_k</script>

    <p>所以，</p>

    <script type="math/tex; mode=display">\sigma_{jj} = \mathrm{Var}(X_j) = \sum_{k=1}^{p}\gamma_{kj}^2 \mathrm{Var}(Y_k) = \sum_{k=1}^{p} \gamma_{kj}^2 \lambda_k</script>

    <p>因此，$X_j$ 的方差中可以由 $Y_k$ 解释的部分所占比例为：</p>

    <script type="math/tex; mode=display">\dfrac{\gamma_{kj}^2 \lambda_k}{\sum_{k=1}^{p} \gamma_{kj}^2 \lambda_k} = \dfrac{\gamma_{kj}^2 \lambda_k}{\sigma_{jj}}</script>
  </li>
  <li>
    <p>所以，相关系数的平方</p>

    <script type="math/tex; mode=display">\rho_{X_{ij},Y_{ik}}^2 = \dfrac{\gamma_{kj}^2 \lambda_k}{\sigma_{jj}}</script>

    <p>可以被解释为 $X_j$ 的方差中由 $Y_k$ 解释的部分所占的比例。</p>
  </li>
</ul>

<p>由于 $\sum_{k=1}^{p} \rho_{X_{ij}, Y_{ik}}^2 = 1$，我们有</p>

<script type="math/tex; mode=display">\rho_{X_{ij}, Y_{i1}}^2 + \rho_{X_{ij}, Y_{i2}}^2 \le 1</script>

<p>所以，相关系数对 $(\rho_{X_{ij}, Y_{i1}},\rho_{X_{ij}, Y_{i2}})$ 落在一个半径为 $1$ 的圆内。如果 $X_{ij}$ 与 $(Y_{i1},Y_{i2})$ 强相关，那么 $(\rho_{X_{ij}, Y_{i1}},\rho_{X_{ij}, Y_{i2}})$ 将落在非常接近圆周的地方。</p>

<p>通过将所有 $j=1,\dots,p$ 的相关系数对 $(\rho_{X_{ij}, Y_{i1}},\rho_{X_{ij}, Y_{i2}})$ 绘制在同一张图中，我们可以看到 $X_i$ 的哪些分量与 $Y_{i1}$ 和 $Y_{i2}$ 相关性最强。</p>

<p><strong>注意</strong>：对于一个给定的 $j$，只有一个 $(\rho_{X_{ij}, Y_{i1}},\rho_{X_{ij}, Y_{i2}})$，对于所有的 $i$ 都是一样的。回忆一下，$\rho_{X_{ij}, Y_{ik}}$ 与 $i$ 无关：</p>

<script type="math/tex; mode=display">\rho_{X_{ij}, Y_{ik}} =\gamma_{kj} \dfrac{\lambda_k^{1/2}}{(\sigma_{jj})^{1/2}}</script>

<p>在实践中，我们通过使用经验协方差矩阵 $S$ 代替理论上的 $\Sigma$ 得到的经验估计量来代替  $\gamma_{kj}$、$\lambda_k$ 和 $\sigma_{jj}$，从而计算相关系数。</p>

<p>回忆一下，在之前瑞士纸币的例子中，我们有</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
Y_{i1} &= 0.044X_{i1}− 0.112X_{i2}− 0.139X_{i3}− 0.768X_{i4}− 0.202X_{i5}+ 0.579X_{i6} \\[2ex]
Y_{i2} &= −0.011X_{i1} − 0.071X_{i2} − 0.066X_{i3} + 0.563X_{i4} − 0.659X_{i5} + 0.489X_{i6}
\end{align} %]]></script>

<p>其中，$X_{i1}$ 到 $X_{i6}$ 分别是 <code class="language-plaintext highlighter-rouge">Length</code>、<code class="language-plaintext highlighter-rouge">Left</code>、<code class="language-plaintext highlighter-rouge">Right</code>、<code class="language-plaintext highlighter-rouge">Bottom</code>、<code class="language-plaintext highlighter-rouge">Top</code>、<code class="language-plaintext highlighter-rouge">Diagonal</code>：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-20-WX20201120-220422%402x.png" width="80%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 2</span>：瑞士纸币数据：$X_j$ 与 PC1 和 PC2 之间的相关系数图。</span></p>

<p>我们看到 <code class="language-plaintext highlighter-rouge">Diagonal</code>、<code class="language-plaintext highlighter-rouge">Top</code> 和 <code class="language-plaintext highlighter-rouge">Bottom</code> 是与前两个 PC 最相关的三个原始变量。也就是说，前两个 PC 很好地解释了这些变量。</p>

<p>对于靠近圆周的变量：</p>

<ul>
  <li>指向相同方向的变量呈正相关。</li>
  <li>指向相反方向的变量呈负相关。</li>
  <li>与坐标轴成小角度的变量与相应的 PC 紧密相关。</li>
</ul>

<p>我们知道 PC1 在 $X_4$ (<code class="language-plaintext highlighter-rouge">Bottom</code>) 和 $X_6$ (<code class="language-plaintext highlighter-rouge">Diagonal</code>) 上的权重最大，它们的系数符号不同。从图 2 可以看到，两者都靠近相关系数圆的圆周，并且 <code class="language-plaintext highlighter-rouge">Bottom</code> 与 PC1 呈负相关，而 <code class="language-plaintext highlighter-rouge">Diagonal</code> 与 PC1 呈正相关：它们对于 PC1 具有相反的效应。</p>

<p>对于 PC2，其在 $X_4$ (<code class="language-plaintext highlighter-rouge">Bottom</code>)、$X_6$ (<code class="language-plaintext highlighter-rouge">Diagonal</code>) 和 $X_5$ (<code class="language-plaintext highlighter-rouge">Top</code>) 上的权重最大，其中前两个系数为正，第三个系数为负。我们看到 $X_4$ 和 $X_6$ 与 PC2 正相关，而 $X_5$ 与 PC2 负相关。</p>

<p>表 2 证实了我们刚才所说的内容。另外，我们可以看到 $X_1$、$X_2$ 和 $X_3$ 与前两个 PC 的相关性不强，而前两个 PC 共同解释的方差百分比很小。</p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">表 2</span>：瑞士纸币数据：原始变量与 PC 之间的相关系数。</span></p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-20-WX20201120-225430%402x.png" width="60%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"><strong>注意</strong>：表 2 来自教材 (Hardle and Simar)，按照我们之前的计算，这里的符号应该全部取反。教材中，在构造前两个 PC 时采用了特征向量的负版本。</span></p>

<p>大致来说，对于靠近圆周的变量，如果前两个 PC 占了数据可变性的很大百分比，则：</p>

<ul>
  <li>我们可以看到哪些变量倾向于影响个体的坐标。</li>
  <li>示例：如果某个个体的 PC1 较大 (与其他个体相比)，则意味着它与 PC1 相关的变量倾向于具有较大值。</li>
  <li>PC2 同理。</li>
  <li>但是我们在解释图形时需要谨慎，因为它们只是粗略的近似，而且，我们只能使用那些可以由前两个 PC 很好地表示的变量。</li>
</ul>

<p>我们可以结合之前的投影数据的散点图一起观察：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-20-WX20201120-231103%402x.png" width="80%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 3</span>：瑞士纸币数据。<strong>上图</strong>：将原始数据投影到前两个 PC 上的散点图。<strong>下图</strong>：$X_j$ 与 PC1 和 PC2 之间的相关系数图。</span></p>

<p>可以看到：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Diagonal</code>、<code class="language-plaintext highlighter-rouge">Bottom</code> 和 <code class="language-plaintext highlighter-rouge">Top</code> 似乎在区分假钞和真钞方面起着重要作用。</li>
  <li>PC1 与 <code class="language-plaintext highlighter-rouge">Diagonal</code> 高度正相关 $\quad \Longrightarrow \quad$ 较大的 PC1 往往与较大的 <code class="language-plaintext highlighter-rouge">Diagonal</code> 一起出现。</li>
  <li>PC1 与 <code class="language-plaintext highlighter-rouge">Bottom</code> 高度负相关 $\quad \Longrightarrow \quad$ 较小的 PC1 往往与较大的 <code class="language-plaintext highlighter-rouge">Bottom</code> 一起出现</li>
  <li>PC2 与 <code class="language-plaintext highlighter-rouge">Top</code> 高度负相关 $\quad \Longrightarrow \quad$ PC2 的值越大，<code class="language-plaintext highlighter-rouge">Top</code> 值往往越小。</li>
</ul>

<p>我们发现，真钞倾向于具有较大的 <code class="language-plaintext highlighter-rouge">Diagonal</code> 和较小的 <code class="language-plaintext highlighter-rouge">Bottom</code>，而伪钞倾向于具有较大的 <code class="language-plaintext highlighter-rouge">Top</code> 和 <code class="language-plaintext highlighter-rouge">Bottom</code>。</p>

<p>我们可以通过观察 <code class="language-plaintext highlighter-rouge">Diagonal</code>、<code class="language-plaintext highlighter-rouge">Bottom</code> 和 <code class="language-plaintext highlighter-rouge">Top</code> 这三个原始变量的散点图验证这一点：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-20-WX20201120-233903%402x.png" width="80%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 4</span>：瑞士纸币数据。<strong>上图</strong>：原始数据散点图。<strong>下图</strong>：原始数据中心化后的散点图。红色代表伪钞，蓝色代表真钞。</span></p>

<h2 id="2-归一化-pca">2. 归一化 PCA</h2>

<p>在某些情况下，原始变量的标度可能差异非常大。由于 PCA 试图捕获数据的大部分可变性，因此 PC1 将仅关注因其规模而变化最大的变量。</p>

<p>回到瑞士纸币的例子，根据我们之前的分析，真钞和伪钞的区分主要与 $X_4$ (<code class="language-plaintext highlighter-rouge">Bottom</code>)、$X_5$ (<code class="language-plaintext highlighter-rouge">Top</code>)、$X_6$ (<code class="language-plaintext highlighter-rouge">Diagonal</code>) 这三个变量有关。假如我们将 $X_1$ (<code class="language-plaintext highlighter-rouge">Length</code>) 乘以 $100$，那么 PC1 将只关注 $X_1$。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-21-WX20201121-131344%402x.png" width="80%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 5</span>：瑞士纸币数据，将 $X_1$ 乘以 $100$。<strong>上图</strong>：将数据投影到前两个 PC 上的散点图。<strong>下图</strong>：$X_j$ 与 PC1 和 PC2 之间的相关系数图。</span></p>

<p>可以看到，当我们将 $X_1$ 乘以 $100$ 后，PC1 与 <code class="language-plaintext highlighter-rouge">Length</code> 呈现强正相关，此时仅靠 PC1 将无法很好地区分真钞和伪钞 (对比之前的图 3)。</p>

<p>如果我们将 $X_1$ (<code class="language-plaintext highlighter-rouge">Length</code>) 和 $X_2$ (<code class="language-plaintext highlighter-rouge">Left</code>) 都乘以 $100$，那么 PC1 和 PC2 将只关注 $X_1$ 和 $X_2$，我们将无法从数据中学习到任何能够区分真钞和伪钞的信息 (即我们丢弃了两个 PC 的区分能力，对比之前的图 3)。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-21-WX20201121-132840%402x.png" width="80%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 6</span>：瑞士纸币数据，将 $X_1$ 和 $X_2$ 都乘以 $100$。<strong>上图</strong>：将数据投影到前两个 PC 上的散点图。<strong>下图</strong>：$X_j$ 与 PC1 和 PC2 之间的相关系数图。</span></p>

<p>当各变量的标度非常不同时，为了避免前几个 PC 将时间浪费在仅捕获少数几个变量的标度，更好的做法是在执行 PC 分析之前重新缩放所有分量以使其方差为 $1$：即将 $X_i =(X_{i1},\dots,X_{ip})^{\mathrm T}$ t</p>

<p>下节内容：主成分分析 (二)</p>
:ET