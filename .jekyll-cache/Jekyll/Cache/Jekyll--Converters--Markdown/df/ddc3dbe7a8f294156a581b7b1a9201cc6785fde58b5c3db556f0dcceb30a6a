I"<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-07-马尔可夫决策过程mdps">Lecture 07 马尔可夫决策过程（MDPs）</h1>

<p>不同于之前基于搜索的规划，从这节课开始，我们将学习基于强化学习的规划。我们将从基础的 <strong>马尔可夫决策过程（Markov Decision Processes，MDPs）</strong> 开始学习，它是所有强化学习的基础模型，我们可以利用它解决一些强化学习问题。马尔可夫过程松弛了很多之前的经典规划中的假设，之后，我们将学习一些强化学习方法，它们松弛的假设要更多。</p>

<p>本节主要内容如下：</p>

<ol>
  <li>动机</li>
  <li>马尔可夫决策过程（MDPs）：定义</li>
  <li>计算：求解 MDPs</li>
  <li>部分观测 MDPs</li>
</ol>

<h2 id="1-动机">1. 动机</h2>
<h3 id="11-学习成果">1.1 学习成果</h3>

<ol>
  <li>确定在哪些情况下，马尔可夫决策过程（MDP）模型适用于我们的问题。</li>
  <li>定义 “马尔可夫决策过程”</li>
  <li>比较 MDPs 与经典规划模型</li>
  <li>说明 Bellman 方程（和动态规划）是如何求解 MDP 问题的。</li>
  <li>应用价值迭代来手动解决小规模 MDP 问题，并编写价值迭代算法的代码来自动解决中等规模的 MDP 问题。</li>
</ol>

<p>从值函数构造策略</p>

<p>比较和对比值迭代与策略迭代</p>

<p>讨论价值迭代和策略迭代算法的优缺点</p>

<p>下节内容：生成启发函数</p>

:ET