I"Z_<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-03-n-gram-语言模型">Lecture 03 N-gram 语言模型</h1>

<h2 id="1-引言">1. 引言</h2>
<p>本节课我们将学习 <strong>语言模型（Language Models）</strong>，我们之前在第 1 节课中提到过它。</p>

<p><strong>为什么我们关心语言模型？</strong></p>

<p>因为 NLP 中有很大一部分研究都是关于如何 <em>解释语言（explaining language）</em> 的。</p>
<ul>
  <li>
    <p>为什么有些句子比其他句子更 <strong>流畅（fluent）</strong>，或者说 <strong>更自然（natural）</strong>？<br />
  我们为什么关心这个问题呢？<br />
  因为在很多应用中，我们很关心语言的流畅性与自然性。<br />
  例如，在语音识别中，你听到一句话，并且想将它转换成文本，所以你需要区分这段语音可能对应的不同文本，并从中选择更加流畅和自然的版本。比如下面两个句子的发音很接近：</p>

    <script type="math/tex; mode=display">\textit{recognise speech > wreck a nice beach}</script>

    <p>从流畅和自然的角度考虑，显然，左边的句子更有可能代表了讲话者的本意。而语言模型可以帮你做到这一点，它会告诉你 “recognise speech” 是人们更倾向表达的意思。</p>
  </li>
  <li>那么，我们如何衡量这种 “优度（goodness）”（或者说流畅度、自然度）呢？<br />
  我们用 <strong>概率</strong> 来衡量它，而语言模型提供了一种很自然的方式来估计句子的概率。</li>
  <li>在此基础上，一旦你构建了一个语言模型，你还可以用它来 <strong>生成（generation）</strong> 语言。</li>
</ul>

<p>如果你还有印象，我们之前提到过这个名为 <a href="https://talktotransformer.com/">Talk to Transformer</a> 的网站。现在，我们尝试通过这个小的语言模型来完成一个段落。我们在用户输入提示框中输入一句话：“<em>Today we have a lecture on Natural Language Processing.</em>” 然后点击 <code class="highlighter-rouge">GENERATE ANOTHER</code> 按钮，我们将看到这个在线语言模型自动生成了一些看上去相当不错的相关段落：</p>

<video width="705" controls="">
  <source src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-03-11-LanguageModel.mp4" type="video/mp4" />
</video>

<p><strong>语言模型可以用于哪些任务呢？</strong></p>
<ul>
  <li>主要用于：
    <ul>
      <li>语音识别（Speech recognition）</li>
      <li>拼写纠错（Spelling correction）</li>
      <li>查询补全（Query completion）</li>
      <li>光学字符识别（Optical character recognition）</li>
    </ul>
  </li>
  <li>其他生成任务：
    <ul>
      <li>机器翻译（Machine translation）</li>
      <li>概括（Summarisation）</li>
      <li>对话系统（Dialogue systems）</li>
    </ul>
  </li>
</ul>

<p><strong>本节课程大纲</strong></p>
<ul>
  <li>推导 n-gram 语言模型</li>
  <li>平滑（Smoothing）处理稀疏性（sparsity）</li>
  <li>生成语言</li>
  <li>评估语言模型</li>
</ul>

<h2 id="2-n-gram-语言模型">2. N-gram 语言模型</h2>
<h3 id="21-概率从联合概率到条件概率">2.1 概率：从联合概率到条件概率</h3>
<p>我们的目标是得到一个由 $m$ 个单词组成的任意序列（即一个包含 $m$ 个单词的句子）的概率：</p>

<script type="math/tex; mode=display">P(w_1,w_2,\dots,w_m)</script>

<p>第一步是利用链式法则（chain rule）将联合概率转换成条件概率的连乘形式：</p>

<script type="math/tex; mode=display">P(w_1,w_2,\dots,w_m)=P(w_1)P(w_2\mid w_1)P(w_3\mid w_1,w_2)\cdots P(w_m\mid w_1,\dots,w_{m-1})</script>

<h3 id="22-马尔可夫假设the-markov-assumption">2.2 马尔可夫假设（The Markov Assumption）</h3>
<p>目前，这仍然是一个比较棘手的问题，因为随着上下文的不断增加，我们构建的模型中将包含越来越多的参数。所以这里，我们采用一种称为 “马尔可夫假设” 的简化假设：某个单词出现的概率不再依赖于全部上下文，而是取决于离它最近的 $n$ 个单词。因此，我们得到：</p>

<script type="math/tex; mode=display">P(w_i\mid w_1,\dots,w_{i-1})\approx P(w_i\mid w_{i-n+1},\dots,w_{i-1})</script>

<p>对于某个很小的 $n$：</p>
<ul>
  <li>
    <p>当 $n=1$ 时，一个 unigram 模型：</p>

    <script type="math/tex; mode=display">P(w_1,w_2,\dots,w_m)=\prod_{i=1}^{m}P(w_i)</script>

    <p>在 unigram 模型中，我们假设一个句子出现的概率等于其中每个单词单独出现的概率的乘积，这意味着每个单词出现的概率之间相互独立，即我们并不关心每个单词的上下文。</p>

    <p><br /></p>
  </li>
  <li>
    <p>当 $n=2$ 时，一个 bigram 模型：</p>

    <script type="math/tex; mode=display">P(w_1,w_2,\dots,w_m)=\prod_{i=1}^{m}P(w_i\mid w_{i-1})</script>

    <p>在 bigram 模型中，我们假设句子中每个单词出现的概率都和它前一个单词出现的概率有关。</p>

    <p><br /></p>
  </li>
  <li>
    <p>当 $n=3$ 时，一个 trigram 模型：</p>

    <script type="math/tex; mode=display">P(w_1,w_2,\dots,w_m)=\prod_{i=1}^{m}P(w_i\mid w_{i-2},w_{i-1})</script>

    <p>在 trigram 模型中，我们假设句子中每个单词出现的概率都和它前两个单词出现的概率有关。</p>
  </li>
</ul>

<h3 id="23-最大似然估计">2.3 最大似然估计</h3>
<p><strong>我们如何计算这些概率？</strong></p>

<p>非常简单，我们只需要一个大的用于训练的语料库（corpus），然后我们就可以根据语料库中各个单词的计数（counts），利用最大似然估计来估计该单词出现的概率：</p>

<ul>
  <li>
    <p>对于 unigram 模型：</p>

    <script type="math/tex; mode=display">P(w_i)=\dfrac{C(w_i)}{M}</script>

    <p>其中，$C$ 是一个计数函数，$C(w_i)$ 表示单词 $w_i$ 在语料库中出现的次数，$M$ 表示语料库中所有单词 tokens 的数量。</p>

    <p><br /></p>
  </li>
  <li>
    <p>对于 bigram 模型：</p>

    <script type="math/tex; mode=display">P(w_i\mid w_{i-1})=\dfrac{C(w_{i-1},w_i)}{C(w_{i-1})}</script>

    <p>其中，$C(w_{i-1},w_i)$ 表示单词 $w_{i-1}$ 和单词 $w_i$ 前后相邻一起出现的次数。</p>

    <p><br /></p>
  </li>
  <li>
    <p>对于 n-gram 模型：</p>

    <script type="math/tex; mode=display">P(w_i\mid w_{i-n+1},\dots,w_{i-1})=\dfrac{C(w_{i-n+1},\dots,w_i)}{C(w_{i-n+1},\dots,w_{i-1})}</script>

    <p>同理，我们计算 n-gram 出现的次数，除以 (n-1)-gram（即上下文）出现的次数。</p>
  </li>
</ul>

<h3 id="24-序列的开头和结尾表示">2.4 序列的开头和结尾表示</h3>
<p>在我们进入例子之前，我们需要用一些特殊的记号表示一个序列的开始和结束：</p>
<ul>
  <li><script type="math/tex">% <![CDATA[
\texttt{<s>} %]]></script> 表示句子的开始（对应于课本 E18 中的 $\square$）</li>
  <li><script type="math/tex">% <![CDATA[
\texttt{</s>} %]]></script> 表示句子的结束（对应于课本 E18 中的 $\blacksquare$）</li>
</ul>

<h3 id="25-trigram-例子"><a name="eg1"><span style="color:#404040">2.5 Trigram 例子</span></a></h3>
<p>现在，让我们来看一个玩具例子，假设我们有一个只包含两个句子的语料库。</p>

<ul>
  <li>
    <p><strong>语料库</strong>：<br />
<script type="math/tex">% <![CDATA[
\texttt{<s> <s> } \textit{yes no no no no yes}\texttt{ </s>} %]]></script><br />
<script type="math/tex">% <![CDATA[
\texttt{<s> <s> } \textit{no no no yes yes yes no}\texttt{ </s>} %]]></script></p>

    <p>可以看到，每个句子开头有两个起始标记，因为我们采用的是 trigram 模型。</p>

    <p><br /></p>
  </li>
  <li>
    <p><strong>我们希望知道下面的句子在一个 trigram 模型下的概率是多少？</strong></p>

    <script type="math/tex; mode=display">% <![CDATA[
\texttt{<s> <s> } \textit{yes no no yes}\texttt{ </s>} %]]></script>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
P(\text{sent} =\textit{yes no no yes}) &= P(\textit{yes}\mid \texttt{<s>},\texttt{<s>})\times P(\textit{no}\mid \texttt{<s>},\textit{yes})\times P(\textit{no}\mid \textit{yes},\textit{no})\\
&\quad \times P(\textit{yes}\mid \textit{no},\textit{no}) \times P(\texttt{</s>} \mid \textit{no},\textit{yes})\\
&= \dfrac{1}{2} \times 1 \times \dfrac{1}{2} \times \dfrac{2}{5} \times \dfrac{1}{2} \\
&= 0.1
\end{align} %]]></script>

    <p><strong>说明</strong>：</p>
    <ul>
      <li>首先，我们对要计算的句子的概率按照 trigram 模型拆分成条件概率的连乘形式。</li>
      <li>
        <p>然后，对于等式右边的每一个条件概率项，按照 trigram 模型中的条件概率计算公式，分别统计 “当前单词连同它的上下文” 以及 “单独的上下文部分” 在语料库中出现的次数，并将两者相除，得到该项的计算结果。<br />
例如，对于上面等式右边第一个条件概率项，我们考虑句子中第一个单词 “$\textit{yes}$” 及其相邻的 bigram 上下文 “<script type="math/tex">% <![CDATA[
\texttt{<s>}\, \texttt{<s>} %]]></script>”：</p>

        <script type="math/tex; mode=display">% <![CDATA[
P(\textit{yes}\mid \texttt{<s>},\texttt{<s>})=\dfrac{C(\texttt{<s>},\texttt{<s>},\textit{yes})}{C(\texttt{<s>},\texttt{<s>})}=\dfrac{1}{2} %]]></script>

        <p>可以看到，子序列 “<script type="math/tex">% <![CDATA[
\texttt{<s>}\, \texttt{<s>} \,\textit{yes} %]]></script>” 在语料库中只出现过 $1$ 次；而子序列 “<script type="math/tex">% <![CDATA[
\texttt{<s>} \,\texttt{<s>} %]]></script>” 在语料库中一共出现了 $2$ 次，所以第一个条件概率项的结果为 $\frac{1}{2}$。其余各条件概率项的计算方式同理，另外请注意，在计算第四个条件概率项时，bigram 上下文 “<script type="math/tex">\textit{no}\,\textit{no}</script>” 在语料库中一共出现了 $5$ 次。</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="26-n-gram-语言模型的一些问题">2.6 N-gram 语言模型的一些问题</h3>
<ul>
  <li>
    <p><strong>语言通常具有长距离效应  —— 需要设置较大的 $n$ 值</strong><br />
有些词对应的上下文可能出现在句子中距离该词很远的地方，这意味着如果我们采用固定长度的上下文（例如：trigram 模型） ，我们可能无法捕捉到足够的上下文相关信息。例如：</p>

    <script type="math/tex; mode=display">\textit{The }\color{red}{\textit{lecture/s }}\textit{that took place last week } \color{red}{\textit{was/were }}\textit{on processing.}</script>

    <p>在上面的句子中，假如我们要决定系动词 $be$ 是应该用第三人称单数形式 $was$ 还是复数形式 $were$，我们需要回头看句子开头的主语是 $lecture$ 还是 $lectures$。可以看到，它们之间的距离比较远，如果我们采用 bigram 或者 trigram 模型，我们无法得到相关的上下文信息来告诉我们当前位置应该用 $was$ 还是 $were$。这是所有有限上下文语言模型（finite context language models）的一个通病。</p>

    <p><br /></p>
  </li>
  <li>
    <p><strong>计算出的结果的概率通常会非常小</strong><br />
你会发现，一连串条件概率项连乘得到的结果往往会非常小，对于这个问题，我们可以采用取对数计算 log 概率来避免数值下溢（numerical underflow）。</p>

    <p><br /></p>
  </li>
  <li>
    <p><strong>对于不存在于语料库中的词，无法计算其出现概率</strong><br />
如果我们要计算概率的句子中包含了一些没有在语料库中出现过的单词（例如：人名），我们应该怎么办？<br />
一种比较简单的技巧是，我们可以用某种特殊符号（例如：<script type="math/tex">% <![CDATA[
\texttt{<UNK>} %]]></script>）来表示这些所谓的 OOV 单词（out-of-vocabulary，即不在词汇表中的单词），并且将语料库中一些非常低频的单词都替换为这种表示未知单词的特殊 token。</p>

    <p><br /></p>
  </li>
  <li>
    <p><strong>出现在新的上下文（context）中的单词</strong><br />
默认情况下，任何我们之前没有在语料库中见过的 n-gram 的计数都为 $0$，这将导致计算出的 <strong>整个句子的概率为 $0$</strong>。</p>

    <p>这个问题和前面的 OOV 单词的问题类似，我们的语料库中可能已经包含了这些单词，但是并没有包含该单词在新句子中对应的特定的 n-gram（即上下文信息）。这意味着该单词在新的句子中对应 n-gram 对于语言模型来说是全新的，而且因为 n-gram 的组合存在如此多的可能性，以至于语料库很难将所有的可能性都覆盖到。</p>

    <p>例如：假设我们构建了一个 five-gram 语言模型，我们的语料库中一共有 20000 个词汇，那么一共存在多少种可能的 five -gram 组合？</p>

    <p>答案是：$20000^5$。这是一个非常非常大的数字，以至于无论我们的训练语料库有多大，都不可能捕捉到所有的可能性组合。</p>

    <p>这是一个相当常见的问题，并且很重要。如果我们回顾一下链式法则，可以看到所有的条件概率都连乘在一起，所以只要其中某一项在计算子序列时 n-gram 的计数为 $0$，那么最终 计算出的整个句子的概率就会为 $0$。</p>

    <p>为此，我们需要对语言模型进行 <strong>平滑处理（smoothing）</strong>。</p>
  </li>
</ul>

<h2 id="3-平滑处理">3. 平滑处理</h2>
<ul>
  <li>基本思想：给之前没有见过的事件赋予概率。</li>
  <li>必须附加一个约束条件：$P(\text{everything})=1$</li>
  <li>有很多不同种类的平滑处理方法：
    <ul>
      <li>拉普拉斯平滑（Laplacian smoothing，又称 Add-one smoothing，即 “加一” 平滑）</li>
      <li>“加 k” 平滑（Add-k smoothing）</li>
      <li>Jelinek-Mercer 插值（又称 线性插值平滑）</li>
      <li>卡茨退避法（Katz backoff）</li>
      <li>绝对折扣（Absolute discounting）</li>
      <li>Kneser-Ney</li>
      <li>……</li>
    </ul>
  </li>
</ul>

<h3 id="31-拉普拉斯平滑加一-平滑">3.1 拉普拉斯平滑（“加一” 平滑）</h3>
<p>简单思想：假装我们看到的每一个 n-gram 都比它们实际出现的次数多 $1$ 次。</p>
<ul>
  <li>
    <p>这意味着即使是那些我们在语料库中没有见过的 n-gram，我们也将它们出现的次数记为 $1$ 次。所以对于一个新句子中的任何东西都至少有 $1$ 次计数。</p>

    <p><br /></p>
  </li>
  <li>
    <p>对于 unigram 模型（$V=$ 词汇表）：</p>

    <script type="math/tex; mode=display">P_{add1}(w_i)=\dfrac{C(w_i)+1}{M+|V|}</script>

    <p>在之前的模型中，我们只是简单的计算单词 $w_i$ 在语料库中出现的次数 $C(w_i)$ ，然后除以语料库中所有单词 tokens 的数量 $M$。</p>

    <p>现在，对于分子而言，我们总是在它此前的基础上加上 $1$。然后，为了保证它仍然是一个合法的概率分布，即单词 $w_i$ 遍历词汇表中所有可能情况的的概率之和 $\sum_{i=1}^{|V|}P(w_i)=1$，我们需要在此前分母 $M$ 的基础上加上词汇表的长度 $|V|$（即语料库中所有单词 types 的总数）。</p>

    <p><br /></p>
  </li>
  <li>
    <p>对于 bigram 模型：</p>

    <script type="math/tex; mode=display">P_{add1}(w_i\mid w_{i-1})=\dfrac{C(w_{i-1},w_i)+1}{C(w_{i-1})+|V|}</script>

    <p>同样地，我们在之前分子的基础上加 $1$，在分母的基础上加 $|V|$。</p>
  </li>
</ul>

<p>所以现在，我们解决了之前新句子中的未知单词或者 n-gram 的计数为 $0$ 从而导致最终计算出的概率为 $0$ 的问题。接下来，让我们来看一个 “加一” 的例子。</p>

<h3 id="32-拉普拉斯加一平滑的例子">3.2 拉普拉斯（“加一”）平滑的例子</h3>
<p>假设现在我们的语料库中只有一个简单的句子：</p>

<script type="math/tex; mode=display">% <![CDATA[
\texttt{<s>} \textit{ the rat ate the cheese } \texttt{</s>} %]]></script>

<p>所以，词汇表为</p>

<script type="math/tex; mode=display">% <![CDATA[
V=\{\textit{the, rat, ate, cheese, }\texttt{</s>}\} %]]></script>

<p>也许你会问：为什么词汇表中只包含了特殊结束标记 <script type="math/tex">% <![CDATA[
\texttt{</s>} %]]></script>，而没有包含特殊起始标记 <script type="math/tex">% <![CDATA[
\texttt{<s>} %]]></script>？<br />
这是因为我们永远不会预测起始标记 <script type="math/tex">% <![CDATA[
\texttt{<s>} %]]></script>，起始标记仅仅用于条件概率项中的条件部分。如果你回忆之前的 <a href="#eg1">trigram 的例子</a>，可以发现我们实际上在最后一个条件概率项中预测了（以句子中最后两个单词作为条件时）结束标记 <script type="math/tex">% <![CDATA[
\texttt{</s>} %]]></script> 的概率，但我们从来没有预测过起始标记 <script type="math/tex">% <![CDATA[
\texttt{<s>} %]]></script> 的概率，因此我们不需要为其计数。</p>

<ul>
  <li>
    <p>所以，在 “加一” 平滑下，bigram 条件概率 $P(\textit{ate}\mid \textit{rat})$ 为：</p>

    <script type="math/tex; mode=display">P(\textit{ate}\mid \textit{rat})=\dfrac{C(\textit{rat},\textit{ate})+1}{C(\textit{rat})+|V|}=\color{red}{\dfrac{2}{6}}</script>

    <p>可以看到，对于分子而言，bigram “<script type="math/tex">\textit{rat ate}</script>” 在语料库中出现过 1 次，所以 <script type="math/tex">C(\textit{rat},\textit{ate})=1</script>，然后我们在其基础上再加上 $1$，结果为 $2$。对于分母而言，单词 “<script type="math/tex">\textit{rat}</script>” 在语料库中出现过 1 次，所以 <script type="math/tex">C(\textit{rat})=1</script>，然后我们在其基础上再加上 $|V| = 5$，结果为 $6$。</p>

    <p><br /></p>
  </li>
  <li>
    <p>同理，在 “加一” 平滑下，bigram 条件概率 $P(\textit{ate}\mid \textit{cheese})$ 为：</p>

    <script type="math/tex; mode=display">P(\textit{ate}\mid \textit{cheese})=\dfrac{C(\textit{cheese},\textit{ate})+1}{C(\textit{cheese})+|V|}=\color{red}{\dfrac{1}{6}}</script>

    <p>对于分子而言，“<script type="math/tex">\textit{cheese ate}</script>” 从来没有在语料库中出现过，所以 <script type="math/tex">C(\textit{cheese},\textit{ate})=0</script>，然后我们在其基础上再加上 $1$，结果为 $1$。对于分母而言，单词 “<script type="math/tex">\textit{cheese}</script>” 在语料库中出现过 1 次，所以 <script type="math/tex">C(\textit{cheese})=1</script>，然后我们在其基础上再加上 $|V| = 5$，结果为 $6$。所以，通过简单的 “加一” 平滑，我们永远不会遇到计数为 $0$ 的问题。</p>
  </li>
</ul>

<h3 id="33-加-k-平滑">3.3 “加 k” 平滑</h3>
<ul>
  <li>
    <p><strong>“加一” 往往太多了</strong><br />
但是很多时候，加 $1$ 会显得太多了，我们并不想每次都加 $1$，因为这会导致原本的罕见事件可能变得有点过于频繁了。并且我们丢弃了所观测的 n-gram 的太多有效计数。</p>

    <p><br /></p>
  </li>
  <li>
    <p><strong>用 “加 k” 来代替 “加一”</strong><br />
这种方式也被称为 <strong>Lidstone 平滑（Lidstone Smoothing）</strong>。</p>

    <p><br /></p>
  </li>
  <li>
    <p><strong>对于 trigram 模型，公式如下</strong>：</p>

    <script type="math/tex; mode=display">P_{addk}(w_i\mid w_{i-1},w_{i-2})=\dfrac{C(w_{i-2},w_{i-1},w_{i})+k}{C(w_{i-2},w_{i-1})+k|V|}</script>

    <p>可以看到，和 “加一” 平滑类似，只是我们将分子中的加 $1$ 变成了加 $k$，并且为了满足单词 $w_i$ 遍历词汇表中所有可能情况的概率之和为 $1$，我们在分母中的 $|V|$ 前面乘了一个系数 $k$。</p>

    <p><br /></p>
  </li>
  <li>
    <p><strong>必须选择一个 $k$</strong><br />
事实上，如何选择一个合适的 $k$ 值对于模型影响非常大。$k$ 在这里实际上是一个超参数，我们需要尝试对其进行调整以便找到一个使模型表现比较好的 $k$ 值。</p>
  </li>
</ul>

<h3 id="34-lidstone-平滑加-k的例子">3.4 Lidstone 平滑（“加 k”）的例子</h3>

<p>现在，让我们来看一个简单的例子：</p>

<ul>
  <li>context $= \textit{alleged}$</li>
  <li>$5$ observed bi-grams</li>
  <li>$2$ unobserved bi-grams</li>
</ul>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-03-13-WechatIMG145.jpeg" width="80%" /></p>

<p>假设我们收集了一些 bi-grams，其中上下文（context）单词是 “$\textit{alleged}$”，其后跟随的单词请见表格中第 1 列（例如：“$\textit{impropriety}$”、“$\textit{offense}$” 等，这些词都是出现在 “$\textit{alleged}$” 后面的词）。其中我们有 $5$ 个观测到的 bi-grams（表格中前 5 行，可以看到它们的计数都大于 $0$）和 $2$ 个未观测到的 bi-grams（表格中最后 2 行，也就是说 “$\textit{alleged infirmity}$” 和 “$\textit{alleged cephalopods}$” 这两个 bi-grams 并不在我们的语料库中，所以它们的计数都是 $0$）。</p>

<p>我们希望计算这些 bi-grams 没有经过平滑处理的概率（unsmoothed probability），即上面表格中的第 3 列：我们只需要计算每个 bi-gram 的计数 $\text{counts}$，然后除以上下文 “$\textit{alleged}$” 在语料库中出现的总次数 $20$。例如，对于第一个 bi-gram “$\textit{alleged impropriety}$”，其未平滑概率为：</p>

<script type="math/tex; mode=display">P(\textit{impropriety}\mid \textit{alleged}) = \dfrac{C(\textit{alleged}, \textit{impropriety})}{C(\textit{alleged})}= \dfrac{8}{20}= 0.4</script>

<p>假如我们希望计算这些 bi-grams 的平滑概率（smoothed probability），即表格中的最后一列：我们只需要在前面未平滑概率的基础上，对分子加上一个 Lidstone 平滑系数 $\alpha=0.1$（也就是 “加 k” 平滑中的 $k$），对分母加上一个词汇表长度 $|V|=7$ 和
平滑系数 $\alpha=0.1$ 的乘积。例如，对于上面的 bi-gram “$\textit{alleged impropriety}$”，其平滑概率为：：</p>

<script type="math/tex; mode=display">P(\textit{impropriety}\mid \textit{alleged}) = \dfrac{C(\textit{alleged}, \textit{impropriety})+\alpha}{C(\textit{alleged})+|V|\times \alpha}= \dfrac{\color{red}{8}+0.1}{20+7\times 0.1}= 0.391</script>

<p>注意，这里词汇表为：</p>

<script type="math/tex; mode=display">% <![CDATA[
V=\{\textit{alleged},\textit{impropriety},\textit{offense},\textit{damage},\textit{deficiencies},\textit{outbreak},\texttt{</s>}\} %]]></script>

<p>其长度为 $|V|=7$。</p>

<p>同理，对于最后一行中未观测到的 bi-gram “$\textit{alleged cephalopods}$”，其概率为：</p>

<script type="math/tex; mode=display">P(\textit{cephalopods}\mid \textit{alleged}) = \dfrac{C(\textit{alleged}, \textit{cephalopods})+\alpha}{C(\textit{alleged})+|V|\times \alpha}= \dfrac{\color{red}{0}+0.1}{20+7\times 0.1}= 0.005</script>

<hr />
<h2 id="7-扩展阅读">7. 扩展阅读</h2>
<ul>
  <li><em><a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a></em>, by Jurafsky and Martin
    <ul>
      <li>Chapter 2: Normalisation</li>
      <li>(包括关于正则表达式和 Levenshtien 编辑距离的回顾内容)</li>
    </ul>
  </li>
  <li>关于 Porter Stemmer 算法的具体细节请参考下面的链接：<br />
<a href="http://snowball.tartarus.org/algorithms/porter/stemmer.html">http://snowball.tartarus.org/algorithms/porter/stemmer.html</a></li>
</ul>

<p>下节内容：N-gram 语言模型</p>

:ET