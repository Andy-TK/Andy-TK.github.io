I"<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-07-nlp-深度学习前馈神经网络">Lecture 07 NLP 深度学习：前馈神经网络</h1>

<p><strong>什么是深度学习？</strong></p>

<ul>
  <li>机器学习的一个分支。</li>
  <li>它是神经网络的另一个名字。</li>
  <li><strong>神经网络</strong>：灵感来源于大脑的工作机制，包含被称为 <strong>神经元</strong> 的计算单元。</li>
  <li>为什么是 <strong>深度</strong>？因为在现代深度学习模型中，有很多连接在一起的层。</li>
</ul>

<h2 id="1-前馈神经网络">1. 前馈神经网络</h2>

<h3 id="11-总体结构">1.1 总体结构</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-17-WX20200417-204406%402x.png" width="50%" /></p>

<ul>
  <li>又称 <strong>多层感知器（multilayer perceptrons）</strong></li>
  <li>上图是一个包含两个隐藏层的前馈神经网络，图中的每个圆圈表示一个神经元，它实际上对应着某个函数，该函数通常是一个非线性函数（例如：Sigmoid 函数），其输入来自前一层的输出的线性加权组合，而每个神经元的输出的线性组合将作为下一层的输入。以此类推，直到输出层，输出层通常是根据实际问题定制的，例如：处理一个三分类问题时，通常可以将输出层神经元个数设为 3 个。</li>
  <li>图中的每个箭头都携带一个权重，其大小取决于输入变量的重要程度。</li>
  <li>Sigmoid 函数在这里引入了非线性。</li>
</ul>

<h3 id="12-神经元">1.2 神经元</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-17-WX20200417-205859%402x.png" width="40%" /></p>

<p>每个 <strong>神经元</strong> 都是一个 <strong>函数</strong></p>
<ul>
  <li>
    <p>给定输入（向量） $\mathbf x$，计算实值（标量）$h$</p>

    <script type="math/tex; mode=display">h=\tanh \left(\sum_{j}w_jx_j+b\right)</script>
  </li>
  <li>对输入进行缩放（乘以权重，$\mathbf w$），并且加上偏移量（偏置，$b$）</li>
  <li>采用 <strong>非线性函数</strong>，例如：logistic sigmoid, hyperbolic sigmoid（tanh），或者 ReLU</li>
</ul>

<h3 id="13-矩阵向量表示">1.3 矩阵向量表示</h3>

<ul>
  <li>
    <p>通常会有很多个隐藏神经元，即</p>

    <script type="math/tex; mode=display">h_i=\tanh \left(\sum_{j}w_{ij}x_{j}+b_i\right)</script>
  </li>
  <li>每个神经元都有属于自己的权重向量 $\mathbf w_i$ 和偏置项 $b_i$</li>
  <li>
    <p>某一层的计算结果可以用矩阵和向量操作表示</p>

    <script type="math/tex; mode=display">\mathbf h=\tanh (W\mathbf x + \mathbf b)</script>

    <p>其中，$W$ 是一个由权重向量 $\mathbf w_i$ 构成的矩阵，$\mathbf x$ 是输入向量，$\mathbf b$ 是所有偏置项组成的向量。</p>
  </li>
  <li>此时，非线性函数实际上作用在每个元素上。</li>
</ul>

<h3 id="14-输出层">1.4 输出层</h3>

<ul>
  <li>二分类问题（例如：对一条推文进行正面/负面情感分类）
    <ul>
      <li>使用 Sigmoid 激活函数（又称 Logistic 函数）</li>
    </ul>
  </li>
  <li>多分类问题（例如：对一个文档的主题进行分类）
    <ul>
      <li>
        <p>使用 Softmax 函数，确保概率大于 $0$，并且和为 $1$。</p>

        <script type="math/tex; mode=display">\left[\dfrac{\exp(v_1)}{\sum_i\exp(v_i)},\dfrac{\exp(v_2)}{\sum_i\exp(v_i)},\dots,\dfrac{\exp(v_m)}{\sum_i\exp(v_i)} \right]</script>
      </li>
    </ul>
  </li>
</ul>

<h3 id="15-前馈神经网络">1.5 前馈神经网络</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-17-WX20200417-204406%402x.png" width="50%" /></p>

<p>让我们回到之前的例子，上图是一个包含 1 个输入层、2 个隐藏层和 1 个输出层的前馈神经网络。它可以表示为：</p>

<script type="math/tex; mode=display">\mathbf h_1=\tanh (W_1\mathbf x + \mathbf b_1)</script>

<script type="math/tex; mode=display">\mathbf h_2=\tanh (W_2\mathbf h_1 + \mathbf b_2)</script>

<script type="math/tex; mode=display">\mathbf y=\text{softmax}(W_3\mathbf h_2)</script>

<p>权重矩阵 $W$ 和偏置向量 $\mathbf b$ 就是该模型的所有参数，我们通过定义目标函数，利用梯度下降等方法对其进行训练。</p>

<h3 id="16-从数据中学习">1.6 从数据中学习</h3>

<p><strong>如何从数据中学习参数？</strong></p>

<ul>
  <li>
    <p>本质上，模型试图尽可能地 “拟合” 训练数据，我们可以通过其分配给正确输出的概率来衡量：</p>

    <script type="math/tex; mode=display">L=\prod_{i=0}^{m}P(y_i\mid x_i)</script>

    <ul>
      <li><strong>最大化</strong> 所有训练数据的总概率 $L$</li>
      <li>等价于 <strong>最小化</strong> $\log L$，对于参数而言</li>
    </ul>
  </li>
  <li>
    <p>训练采用梯度下降</p>
    <ul>
      <li>很多工具，例如 tensorflow、pytorch、dynet 等，利用自动微分来自动计算梯度。</li>
    </ul>
  </li>
</ul>

<h2 id="2-应用">2. 应用</h2>
<p>现在，我们来看一些如何为下游应用构建神经网络的例子。</p>

<h3 id="21-主题分类">2.1 主题分类</h3>
<p>给定一个文档，基于一个预定义的主题集合（例如：经济、政治、体育等）对其进行分类。</p>

<p><strong>输入</strong>：词袋（bag-of-words）</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-18-WX20200418-115639%402x.png" width="80%" /></p>

<p><strong>模型</strong>：
考虑之前的两层模型，假设输出类别为 3 类：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-17-WX20200417-204406%402x.png" width="40%" /></p>

<script type="math/tex; mode=display">\mathbf h_1=\tanh (W_1\mathbf x + \mathbf b_1)</script>

<script type="math/tex; mode=display">\mathbf h_2=\tanh (W_2\mathbf h_1 + \mathbf b_2)</script>

<script type="math/tex; mode=display">\mathbf y=\text{softmax}(W_3\mathbf h_2)</script>

<p>对于第一个文档 <strong>doc 1</strong>：<br />
输入向量 $\mathbf x=[0,2,3,0]$<br />
输出向量 $\mathbf y=[0.1,0.6,0.3]$</p>

<h2 id="3-总结">3. 总结</h2>
<ul>
  <li>对于序列标注任务，HMM 是一种简单高效的方法。</li>
  <li>非常有竞争力，并且训练过程很快（利用维特比译码算法），可以作为其他序列标注任务的 natural baseline。</li>
  <li>主要缺点：与 MEMMs 和 CRFs 这类判别式模型相比，HMM 在特征表示方面不是非常灵活。</li>
</ul>

<h2 id="4-扩展阅读">4. 扩展阅读</h2>
<ul>
  <li>JM3 Appendix A A.1-A.2, A.4</li>
  <li>See also E18 Chapter 7.3</li>
  <li>参考资料：
    <ul>
      <li>Rabiner’s HMM tutorial: <a href="http://tinyurl.com/2hqaf8">http://tinyurl.com/2hqaf8</a></li>
      <li>Lafferty et al, Conditional random fields: Probabilistic models for segmenting and labeling sequence data (2001)</li>
    </ul>
  </li>
</ul>

<p>下节内容：NLP 深度学习：前馈网络</p>

:ET