I"t.<h1 id="lecture-09-分类和回归树及相关方法">Lecture 09 分类和回归树及相关方法</h1>

<p><strong>参考教材</strong>：</p>

<ul>
  <li><em>Hardle, W. and Simar, L (2015). Applied multivariate statistical analysis, 4th edition.</em></li>
  <li><em>Hastie, T. Tibshirani, R. and Friedman, J. (2009). The elements of statistical learning, 2nd edition</em></li>
</ul>

<h2 id="1-引言">1. 引言</h2>

<p>前一章中的方法依赖于很强的 <strong>参数假设 (parametric assumptions)</strong> (线性模型、逻辑模型或正态性假设)。</p>

<p>当这些假设基本正确时，这些分类器可以很好地工作；但是当这些假设与真实情况相差很远时，这些分类器的性能可能会非常差。</p>

<p>回忆一下之前的例子：当分隔边界看上去并非线性时，线性分类器将无法很好地工作。</p>

<p>因此，我们需要一些对于强参数假设涉及较少的灵活模型。</p>

<p>这里，我们将介绍由 Leo Breiman 在 1980 年代普及的概念：<strong>回归树 (regression trees)</strong>。</p>

<h2 id="2-回归树">2. 回归树</h2>

<p>假设我们观察到一个 i.i.d. 样本 $(X_1,Y_1),\dots,(X_n,Y_n)$ 来自以下基础均值模型</p>

<script type="math/tex; mode=display">E(Y_i \mid X_i=x) = m(x)</script>

<p>其中，$Y_i\in \mathbb R$ 和 $X_i=(X_i1,\dots,X_ip)^{\mathrm T} \in \mathbb R^p$ 是连续的。</p>

<p>在本节内容中，这种表示法可能会引起混淆。<span style="color:red">因此，我们将样本表示为</span></p>

<script type="math/tex; mode=display">(\mathcal X_1,\mathcal Y_1),\dots,(\mathcal X_n,\mathcal Y_n)</script>

<p>其中，$\mathcal Y_i \in \mathbb R, \; \mathcal X_i=(\mathcal X_{i1},\dots,\mathcal X_{ip})^{\mathrm T}$。</p>

<p>当我们不想强调数据的样本依赖性时，我们使用符号 $(X,Y)$，其中，$(X,Y)$ 与 $(\mathcal X_i,\mathcal Y_i)$ 具有相同的分布。特别地，它也来自模型</p>

<script type="math/tex; mode=display">E(Y\mid X=x) = m(x)</script>

<p>其中，$Y\in \mathbb R$ 和 $X=(X_1,\dots,X_p)^{\mathrm T} \in \mathbb R^p$ 是连续的。</p>

<p>因此，我们保留了符号 $\{X_1,\dots,X_p\}$ 以表示一般向量 $X$ 的 <strong>坐标 (coordinates)</strong>。</p>

<p>通常，均值函数 $m(\cdot)$ 在 <strong>特征空间 (feature space)</strong> (即 $X$ 的域，或者 $X$ 的所有可能值的 $\mathbb R^p$ 的子集) 上看上去可能非常不平滑。</p>

<p>但是，如果 $x_1$ 和 $x_2$ 是特征空间中两个距离很近的点，则很可能有 $m(x_1)\approx m(x_2)$。</p>

<p><strong>回归树的主要思想</strong>：将特征空间 <strong>划分 (partition)</strong> 为不相交的区域 $R_1,R_2,\dots$，在每个区域 $R_i$ 上，我们用一个常数来近似 $m(x)$。</p>

<p>为了解决这个问题：考虑 $p=2$ 的情况。这里我们需要估计回归曲线</p>

<script type="math/tex; mode=display">m(x) = E(Y \mid X = x)</script>

<p>其中，$Y\in \mathbb R$ 和 $X=(X_1,X_2)^{\mathrm T} \in \mathbb R^2$ 是连续的。</p>

<p>例如，我们构造了一系列关于类型的分区：</p>

<ol>
  <li>
    <p>首先，选择变量 $X_1$ 和一个实数 $t_1$。 然后，将特征空间中满足 $X_1 \le t_1$ 的所有点视为一个区域，将满足 $X_1 &gt; t_1$ 的所有点视为另一个区域。这种二元拆分方法将产生两个区域，分别由 $\{X_1 \le t_1\}$ 和 $\{X_1 &gt; t_1\}$ 描述。这里，我们将 $X_1$ 和 $t_1$ 分别称为该步的 <strong>分裂变量 (splitting variable)</strong> 和 <strong>分裂点 (split point)</strong>。</p>
  </li>
  <li>
    <p>接下来，在区域 $\{X_1 \le t_1\}$ 中，我们可以选择分裂变量 $X_2$ 和分裂点 $t_2$，并将 $\{X_1 \le t_1\}$ 进一步划分为两个子区域，分别由 $\{X_2 \le t_2\}$ 和 $\{X_2 &gt; t_2\}$ 描述。</p>
  </li>
  <li>
    <p>对于第 1 步中的另一个区域 $\{X_1 &gt; t_1\}$，我们可以再次选择 $X_1$ 作为分裂变量，以及一个分裂点 $t_3$，并类似地将该区域进一步划分为 $\{X_1 \le t_3\}$ 和 $\{X_1 &gt; t_3\}$ 两个子区域。</p>
  </li>
  <li>
    <p>继续相同的过程……</p>
  </li>
</ol>

<p>(这里，我们没有讨论如何在每一步中选择分裂变量和分裂点……)</p>

<p>重复这种二元分区步骤几次后，我们会将特征空间划分为一些不相交的矩形区域，记为 $R_1,\dots,R_L$。</p>

<p>我们可以利用 $\{X_j \le t\}$ 和 $\{X_j &gt; t\}$ 这种二元分区序列构造一棵树。经过多次划分后，停止分裂过程，并得到区域 $R_1,\dots,R_L$。这些划分和区域对应树中的分支和结点。$R_i$ 被称为终止结点/叶子。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-10-24-WX20201024-231046%402x.png" width="40%" /></p>

<p>特征空间被划分为一系列矩形区域 $R_1,R_2,\dots$：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-10-24-WX20201024-231215%402x.png" width="40%" /></p>

<ul>
  <li>过程结束时得到的区域 $R_1,\dots,R_L$ 被称为树的 <strong>终止结点(terminal nodes)</strong> 或 <strong>叶子(leaves)</strong>。</li>
  <li>树内部的 $\{X_1 \le 1\}$ 之类的划分被称为 <strong>内部结点 (internal nodes)</strong>。</li>
  <li>连接结点的树的各段被称为树的 <strong>分支 (branches)</strong>。</li>
</ul>

<p>一旦我们将空间划分为区域 $R_1,\dots,R_L$，在每个区域上，我们将用一个常数来近似回归曲线 $m$：</p>

<p>对于特征空间中的所有 $x$，</p>

<script type="math/tex; mode=display">% <![CDATA[
m(x)\approx \sum_{\ell=1}^{L}c_{\ell}I\{x \in R_{\ell}\}=\begin{cases}c_{\ell} & \text{if }x\in R_{\ell} \\[2ex] 0 & \text{otherwise}\end{cases} %]]></script>

<p>区域 $R_1,R_2,\dots$ 上的分段常数近似如下：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-10-24-WX20201024-232638%402x.png" width="40%" /></p>

<p>为什么这种方法很灵活？因为只要将特征空间划分为足够小的块，我们总是可以通过每个块上的常数来很好地近似回归曲线。</p>

<p>下面是 $p = 1$ 情况下的一个示例：这里，对特征空间进行分区意味着将 $X$ 的取值范围划分为不同的子区间。</p>

<ul>
  <li>
    <p>在没有划分特征空间的情况下，我们无法用一个常数很好地近似 $m$ (暂时忽略 <code class="language-plaintext highlighter-rouge">*</code> 表示的数据点。想象一下，红线绘制为函数值 $m(x)$ 在整个区间上的平均值。这可以通过一个积分操作来完成）：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-10-25-WX20201025-101851%402x.png" width="80%" /></p>
  </li>
  <li>
    <p>当我们划分为较小的区间时，效果明显更好了。(将每条红线的水平想象为 $m(x)$ 在红线所跨的区间上的平均值)：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-10-25-WX20201025-101043%402x.png" width="80%" /></p>
  </li>
</ul>

<p>对特征空间的划分越细，常数 $c_1,c_2,c_3,\dots$ 在区域 $R_1,R_2,R_3,\dots$ 上对 $m$ 的近似就越好。</p>

<p>然而，这些只是理论上的：在实践中，我们仅观察到一组样本，因此，如果划分过于精细，将无法找到正确的常数。</p>

<p>那么，我们应该怎样从真实数据中选择常数 $c_1,c_2,\dots$ 呢？或者更一般地，当 $X$ 是一个 $p$ 维向量时，<strong>怎样拟合一棵回归树</strong>？</p>

<p>假设对于不同的 $j\in \{1,\dots,p\}$ 和 $t$ 值，我们使用一系列形式为 $\{X_j \ne t\}$ 和 $\{X_j &gt; t\}$ 的二元划分将特征空间划分为不相交的区域 $R_1,\dots,R_L$。</p>

<p>然后，对于 $\ell in \{1,\dots,L\}$ 和 $x\in R_{\ell}$，我们通过下式来近似 $m(x)$</p>

<script type="math/tex; mode=display">\hat m(x)=\hat c_{\ell}= \text{average}(\mathcal Y_i) \quad \text{s.t.} \quad \mathcal X_i \in R_{\ell}</script>

<p>因此，在每个区域 $R_{\ell}$ 上，我们取 $\mathcal X_i\in R_{\ell}$ 的 $\mathcal Y_i$ 的平均值。这与选择使下面的 RSS 最小化的 $c_1,\dots,c_L$ 是相同的</p>

<script type="math/tex; mode=display">\sum_{i=1}^{n}\left[\mathcal Y_i - \sum_{\ell=1}^{L}c_{\ell}I\{\mathcal X_i \in R_{\ell}\}\right]^2 = \sum_{\ell=1}^{L}\sum_{\mathcal X_i \in R_{\ell}}(\mathcal Y_i - c_{\ell})^2</script>

<p><strong>但是，我们如何确定树的骨架？换而言之，我们如何选择连续的划分？</strong></p>

<p>回忆 $X=(X_1,\dots,X_p)^{\mathrm T}$，并且一个划分是基于 $X$ 的一个分量 $X_j$。</p>

<p>理想情况下，我们希望找到能够最小化 RSS 的树，但是这在 <strong>计算上不可行</strong>。</p>

<p>为什么？因为这将涉及比较 <strong>所有可能的划分序列</strong> $\{X_j \ne t\}$ 和 $\{X_j &gt; t\}$，对于所有的 $j=1,\dots,p$ 和 $X_j$ 所能取得的所有 $t$ 值。</p>

<p>实际上，对于 $X_j$ 的拆分，我们只需要考虑 $t$ 在 $\mathcal X_{1j},\dots,\mathcal X_{nj}$ 上的值。为什么？因为一个划分会将观测数据分为两部分。仅当 $t$ 等于观测数据之一时，这些分区才会发生改变。</p>

<p>但是考虑所有分区仍然非常耗时。</p>

<p>因此，我们选择 <strong>逐渐生成一棵树</strong>，并在每一步中构造使 RSS 最小化的划分：</p>

<ol>
  <li>
    <p>从全部数据开始，并考虑第一次划分的所有可能方式，即考虑将数据分为两个区域的所有划分</p>

    <script type="math/tex; mode=display">R_1(j,t)=\{\mathcal X_i \quad \text{s.t.} \quad \mathcal X_{ij}\le t\},\quad R_2(j,t)=\{\mathcal X_i \quad \text{s.t.} \quad \mathcal X_{ij} > t\}</script>

    <p>对于所有的 $j=1,\dots,p$ 和 $\mathcal X_{ij}$ 所能取得的所有 $t$ 值。</p>

    <p>如前所述，我们只需要考虑 $t\in \{\mathcal X_{1j},\dots,\mathcal X_{nj}\}$。</p>
  </li>
  <li>
    <p>在所有 $j$ 和 $t$ 上，选择能够最小化 RSS 的 <strong>分裂变量</strong> $j$ 和 <strong>分裂点</strong> $t$：</p>

    <script type="math/tex; mode=display">\min_{c_1} \sum_{\mathcal X_i\in R_1(j,t)} (\mathcal Y_i-c_1)^2 + \min_{c_2} \sum_{\mathcal X_i\in R_2(j,t)} (\mathcal Y_i-c_2)^2</script>

    <p>如前所述，这等价于在所有 $j$ 和 $t$ 上，最小化</p>

    <script type="math/tex; mode=display">\sum_{\mathcal X_i\in R_1(j,t)} (\mathcal Y_i- \hat c_1)^2 + \sum_{\mathcal X_i\in R_2(j,t)} (\mathcal Y_i- \hat c_2)^2</script>

    <p>其中，</p>

    <script type="math/tex; mode=display">\hat c_{\ell} = \text{average}(\mathcal Y_i) \quad \text{s.t.}\quad \mathcal X_i \in R_{\ell}(j,t)</script>
  </li>
  <li>
    <p>一旦找到了上面定义的最佳 $j$ 和 $t$，我们就确定了第一个划分。它创建了 $R_1(j,t)$ 和 $R_2(j,t)$ 两个数据区域。</p>
  </li>
  <li>
    <p>然后，在这两个区域中的每一个上，我们 <strong>重复相同的过程</strong>：</p>

    <ul>
      <li>考虑由 $j$ 和 $t$ 选择确定的这两个区域中的 $R_1$ 的所有可能分裂，并找到最小化RSS的j和t。</li>
    </ul>
  </li>
</ol>

<p>*然后考虑由j和t的选择确定的R 2在两个区域中的所有可能分裂，并找到最小化RSS的j和t。</p>

<p>*总共确定4个区域（每个区域已分为两部分）。</p>

<p>同样，在这里，当根据X j将R<code class="language-plaintext highlighter-rouge">分成两部分时，我们只需要考虑X ij'中的t使得X i∈R</code>即可。</p>

<p>5.注意：每个R`都有自己的分割变量X j和分割点t。</p>

<p>6.我们继续对上一步中创建的每个新区域重复分割过程，直到我们决定停止我们的树。</p>

<p>下节内容：分类和回归树及相关方法</p>
:ET