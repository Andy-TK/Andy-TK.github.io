I"|<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-07-马尔可夫决策过程mdps">Lecture 07 马尔可夫决策过程（MDPs）</h1>

<p>不同于之前基于搜索的规划，从这节课开始，我们将学习基于强化学习的规划。我们将从基础的 <strong>马尔可夫决策过程（Markov Decision Processes，MDPs）</strong> 开始学习，它是所有强化学习的基础模型，我们可以利用它解决一些强化学习问题。马尔可夫过程松弛了很多之前的经典规划中的假设，之后，我们将学习一些强化学习方法，它们松弛的假设要更多。</p>

<p>本节主要内容如下：</p>

<ol>
  <li>动机</li>
  <li>马尔可夫决策过程（MDPs）：定义</li>
  <li>计算：求解 MDPs</li>
  <li>部分观测 MDPs</li>
</ol>

<h2 id="1-动机">1. 动机</h2>
<h3 id="11-学习成果">1.1 学习成果</h3>

<ol>
  <li>确定在哪些情况下，马尔可夫决策过程（MDP）模型适用于我们的问题。</li>
  <li>定义 “马尔可夫决策过程”。</li>
  <li>比较 MDPs 与经典规划模型。</li>
  <li>说明 Bellman 方程（和动态规划）是如何求解 MDP 问题的。</li>
  <li>应用价值迭代来手动解决小规模 MDP 问题，并编写价值迭代算法的代码来自动解决中等规模的 MDP 问题。</li>
  <li>根据价值函数构造策略。</li>
  <li>比较和对比价值迭代与策略迭代。</li>
  <li>讨论价值迭代和策略迭代算法的优缺点。</li>
</ol>

<h3 id="12-相关阅读">1.2 相关阅读</h3>
<ul>
  <li>任何概率论基础材料。</li>
  <li>Chapter 17 of <em>Artiﬁcial Intelligence — A Modern Approach</em> by Russell and Norvig.</li>
  <li>Chapter 4 of <em>Reinforcement Learning: An Introduction, second edition</em>.</li>
</ul>

<h3 id="13-移除假设">1.3 移除假设</h3>
<p><strong>经典规划</strong>：目前为止，我们已经学习了经典规划。经典规划工具可以在大的搜索空间中快速求解，但是需要如下假设：</p>
<ul>
  <li>确定性事件</li>
  <li>环境改变只是行动导致的结果</li>
  <li>完美的知识（全知）</li>
  <li>单个执行器（全能）</li>
</ul>

<p>在本课程的其余部分，我们将研究如何放松其中的一些假设，我们将从第一个开始：确定性事件。</p>

<h3 id="14-马尔可夫决策过程">1.4 马尔可夫决策过程</h3>
<p><strong>马尔可夫决策过程（Markov Decision Processes，MDP）</strong>移除了确定性事件的假设，而是假设每个行动可能导致多个结果，其中每个结果都有一个相关的概率。</p>

<p>例如：</p>
<ul>
  <li>抛一枚硬币有 2 种结果：正面（$\frac{1}{2}$）和背面（$\frac{1}{2}$）</li>
  <li>同时掷两个骰子有 12 种结果：2（$\frac{1}{36}$），2（$\frac{1}{36}$）</li>
</ul>

<p>下节内容：生成启发函数</p>
:ET