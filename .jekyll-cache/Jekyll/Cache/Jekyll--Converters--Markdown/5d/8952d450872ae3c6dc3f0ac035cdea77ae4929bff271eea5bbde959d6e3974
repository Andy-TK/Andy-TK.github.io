I"
<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-10-更高效的强化学习回报设计和-q-函数-逼近">Lecture 10 更高效的强化学习：回报设计和 $Q$-函数 逼近</h1>

<p><strong>主要内容：</strong></p>
<ol>
  <li>动机</li>
  <li>$Q$-函数 逼近</li>
  <li>回报设计和初始化 $Q$-函数</li>
  <li>总结</li>
</ol>

<h2 id="1-动机">1. 动机</h2>
<h3 id="11-学习成果">1.1 学习成果</h3>
<ol>
  <li>给定一些已知特征，手动应用线性 $Q$-函数 逼近来求解小规模 MDP 问题。</li>
  <li>选择合适的特征，设计并实现 $Q$-函数逼近，以实现无模型强化学习技术，从而自动求解中等规模的 MDP 问题。</li>
  <li>比较各种函数逼近方法的优缺点。</li>
  <li>比较和对比线性函数逼近与基于深度神经网络的函数逼近。</li>
  <li>说明如何使用回报设计来帮助无模型强化学习方法达到收敛。</li>
  <li>将回报设计手动应用于给定的潜在函数，以解决小规模 MDP 问题。</li>
  <li>设计并实现潜在函数，以自动解决中等规模的 MDP 问题。</li>
  <li>对比和比较回报设计与 $Q$-函数 初始化。</li>
</ol>

<h3 id="12-强化学习一些弱项">1.2 强化学习：一些弱项</h3>
<p>在之前的讲座中，我们了解了强化学习中基本的时序差分（TD）方法：$Q$-学习 和 SARSA。如前所述，这两种方法在以下一些基本方面存在一些缺点：</p>

<ul>
  <li>不同于蒙特卡洛方法中在获得回报后将该回报进行反向传播，TD 方法使用 bootstrapping（它们使用 $Q(s,a)$ 来估计未来折扣回报），这意味着对于回报稀疏问题，它可能需要很长时间才能将回报传播到整个 $Q$-函数。</li>
  <li>两种方法都估计了一个 $Q$-函数 $Q(s,a)$，而对此最简单的建模方法是通过 $Q$-表。但是，这需要我们维护一个大小为 $|A|\times |S|$ 的表，而对于任何非平凡的问题来说，这样一张表都显得过于庞大了。</li>
  <li>使用 $Q$-表 要求我们对于每个可到达的状态进行多次访问，并且对于每个行动应用多次以获得一个较好的 $Q(s,a)$ 的估计。因此，如果我们从未访问过某个状态 $s$，我们将无法估计 $Q(s,a)$，即便我们已经访问过一些与状态 $s$ 非常相似的状态。</li>
  <li>回报可能非常稀疏，这意味着只有很少的状态/动作会得到非零回报。这显然是有问题的，因为初始时，强化学习算法的行为完全是随机的，并且很难发现那些较好的回报。还记得上一讲的 Freeway 游戏的演示吗？UCT 在那种情况下失效了。</li>
</ul>

<h3 id="13-强化学习一些改进">1.3 强化学习：一些改进</h3>
<p>为了克服这些限制，我们将看一下 3 种可以改进时序差分方法的简单方法：</p>

<ul>
  <li><strong>$n$-步时序差分学习</strong>：蒙特卡洛技术执行整个行动轨迹，然后反向传播回报；而基本的 TD 方法仅查看下一步的回报，来估计未来回报。相比之下，$n$-步方法在更新回报之前，将查看向前 $n$-步的回报，然后估算剩余部分。</li>
  <li><strong>逼近方法</strong>：相比计算一个精确的 $Q$-函数，我们可以使用简单方法来逼近它，这些方法既不需要大型 $Q$-表（因此方法的可扩展性更好），同时也可以用来提供合理的 $Q(s,a)$，<strong>即使我们之前从未在状态 $s$ 下应用过行动 $a$</strong>。</li>
  <li><strong>回报设计（Reward shaping）和 $Q$-值初始化</strong>：如果回报是稀疏的，对于那些我们认为能够使我们更接近解（目标状态）的行为，我们可以修改/增强回报函数
的回报</li>
</ul>

<p>回报函数以奖励我们认为使我们更接近解决方案的行为，或者我们可以猜测最佳Q函数和初始Q（s，a ）就是这个。这个讲座！</p>

<p>下节内容：更高效的强化学习：回报设计和 $Q$-函数 逼近</p>

:ET