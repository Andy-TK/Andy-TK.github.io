I"<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-08-蒙特卡洛树搜索利用和探索的权衡">Lecture 08 蒙特卡洛树搜索：利用和探索的权衡</h1>

<p>这节课我们将学习 <strong>蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）</strong>。蒙特卡洛树搜索是一种用于解决强化学习问题的新方法。</p>

<p>MCTS 通常用于 <strong>在线决策（online decision）</strong> 或者 <strong>在线学习（online learning）</strong>。</p>

<p>与之相对的被称为 <strong>离线学习（offline learning）</strong>，在离线学习中，我们提前完成所有的规划，当执行行动的时候，我们执行之前规划的所有行动，例如：在经典规划中，我们先规划好了一个行动序列，然后依次执行它们，我们也可能从中提取策略，并按照策略行动。</p>

<p>相反，在在线学习中，行动和选择是交互的，例如：在 MCTS 中，我们在每一次规划都会用来选择下一个行动，然后在执行该行动后，我们将重新进行下一轮的规划。</p>

<p>在实际解决规划问题时，我们可以自行选择采用在线学习或者离线学习的方式，有些技术基于在线学习，有些基于离线学习。例如：价值评估将提供一个完全收敛的策略，它更适用于离线规划；MCTS 则更倾向于在线规划；而经典规划中的启发式搜索既可用于在线学习，也可用于离线学习。在这里，我们将看到很多经典规划应用于我们的问题，我们可以选择忽略转移概率，直接选择行动直到目标状态；或者考虑转移概率，中途可能到达某个状态，然后将该状态作为初始状态重新规划。</p>

<p><strong>本节课主要内容如下：</strong></p>
<ol>
  <li>问题</li>
  <li>蒙特卡洛树搜索 —— 基础</li>
  <li>多臂老虎机</li>
  <li>蒙特卡洛树搜索和多臂老虎机</li>
  <li>总结</li>
</ol>

<p><strong>学习成果：</strong></p>
<ol>
  <li></li>
</ol>

<p>解释MDP的在线计划与在线计划之间的区别。</p>

<p>应用MCTS手动解决小规模MDP问题并编程MCTS算法以自动解决中型MDP问题</p>

<p>根据MCTS算法产生的Q函数构造策略</p>

<p>选择并应用多臂强盗算法</p>

<p>将多臂强盗算法（包括UCB）集成到MCTS算法</p>

<p>将MCTS与价值/政策迭代进行比较和对比</p>

<p>讨论MCTS系列算法的优缺点。</p>

<p>下节内容：蒙特卡洛树搜索：利用和探索的权衡</p>

:ET