I",<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-09-广义线性模型glm">Lecture 09 广义线性模型（GLM）</h1>
<p>我们已经见过了广义线性模型的一些特例，其中，响应变量 $Y$ 服从伯努利分布。现在我们可以看一下更加广义的情况，响应变量 $Y$ 不再是正态的。本章将包含以下三部分：</p>

<ul>
  <li>介绍</li>
  <li>估计</li>
  <li>推断</li>
</ul>

<h2 id="1-介绍">1. 介绍</h2>
<p>令 $Y_1,Y_2,\dots,Y_n$ 为随机变量 $Y$ 的独立观测，其关联的协变量向量为 $\mathbf x_1,\mathbf x_2,\dots,\mathbf x_n$。</p>

<p>如果随机变量 $Y$ 的这些独立观测服从正态分布，那么我们可以采用线性模型来建立随机变量 $Y$ 和其关联的协变量之间的联系。对于 $i=1,\dots,n$，我们有</p>

<script type="math/tex; mode=display">Y_i\overset{\mathrm{d}}{=}N(\mu_i,\sigma^2) \quad \text{彼此独立，并且} \quad \mu_i=\mathbf x_i^{\mathrm{T}}\boldsymbol \beta</script>

<p>其中，响应变量 $Y_i$ 的期望 $\mu_i$ 是关于协变量 $\mathbf x_i$ 的线性函数。</p>

<p>现在，我们试图将该模型从正态分布推广到 <strong>指数族（exponential famil y）</strong> 分布：我们假设</p>

<script type="math/tex; mode=display">Y_i\overset{\mathrm{d}}{=}\mathcal {EF}(\mathrm{mean}=\mu_i)  \quad \text{彼此独立，并且} \quad g(\mu_i)=\eta_i=\mathbf x_i^{\mathrm{T}}\boldsymbol \beta</script>

<p>其中，$\eta_i=\mathbf x_i^{\mathrm{T}}\boldsymbol \beta$ 是 <strong>线性预测变量（linear predictor）</strong>。</p>

<p>函数 $g(.)$ 被称为 <strong>连接函数（link function）</strong>：它提供了线性预测变量和响应 $Y$ 的均值之间的连接。</p>

<p>如果 $g(\mu_i)$ 被设置为等于指数族中的 <strong>典范参数（canonical parameter）</strong>，又称 <strong>自然参数（natural parameter）</strong>，那么，$g(.)$ 被称为 <strong>自然连接（natural link）</strong> 或者 <strong>典范连接（canonical link）</strong>。</p>

<p><br /></p>

<p>如果 $Y$ 的分布是 <strong>典范形式（canonical form）</strong>，那么，它的概率密度函数（pdf）$f(y\mid \theta,\phi)$ 满足</p>

<script type="math/tex; mode=display">\ln f(y\mid \theta,\phi)=\dfrac{y\theta-b(\theta)}{a(\phi)}+c(y,\phi)</script>

<p>其中，$\theta$ 被称为 <strong>典范参数（canonical parameter）</strong>或者 <strong>自然参数（natural parameter）</strong>，代表了 <strong>位置（location）</strong>；而 $\phi$ 被称为 <strong>散布参数（dispersion parameter）</strong>，代表了 <strong>尺度（scale）</strong>。我们可以通过指定函数 $a,b,c$ 来定义不同的指数族分布。通常，$a(\phi)=\phi \big/ w$，其中，$w$ 是已知的 <strong>权重（weight）</strong>。</p>

<p>可以推导出</p>

<script type="math/tex; mode=display">\dfrac{\partial \ln f(y\mid \theta,\phi)}{\partial \theta}=\dfrac{y-b'(\theta)}{a(\phi)}\;, \qquad \dfrac{\partial^2 \ln f(y\mid \theta,\phi)}{\partial \theta^2}=-\dfrac{b''(\theta)}{a(\phi)}</script>

<p><br /></p>

<p>另一方面，</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\mathbb{E}\left[\dfrac{\partial \ln f(y\mid \theta,\phi)}{\partial \theta} \right] &= \int \dfrac{\partial \ln f(y\mid \theta,\phi)}{\partial \theta} \cdot f(y\mid \theta,\phi)dy\\
&=\dfrac{\partial}{\partial \theta}\int f(y\mid \theta,\phi)dy=\dfrac{\partial}{\partial \theta}1=0\\\\

\mathbb{E}\left[\dfrac{\partial^2 \ln f(y\mid \theta,\phi)}{\partial \theta^2} \right] &= \int \dfrac{\partial}{\partial \theta} \left(\dfrac{f_{\theta}'(y\mid \theta,\phi)}{f(y\mid \theta,\phi)} \right)\cdot f(y\mid \theta,\phi)dy\\
&= \int f_{\theta}''(y\mid \theta,\phi)dy-\int \left(\dfrac{\partial \ln f(y\mid \theta,\phi)}{\partial \theta}\right)^2 f(y\mid \theta,\phi)dy\\
&= 0-\mathbb{E}\left[\left(\dfrac{\partial \ln f(y\mid \theta,\phi)}{\partial \theta} \right)^2\right]
\end{align} %]]></script>

<p>注意，这里有个技巧是利用 $(\ln f)’=f’\big/f$ 的性质。</p>

<p><br /></p>

<p>结合上面的推导，可以得到</p>

<script type="math/tex; mode=display">\mu=\mathbb{E}(Y)=b'(\theta)\;, \qquad \sigma^2=\mathrm{var}(Y)=a(\phi)b''(\theta)</script>

<ul>
  <li>
    <p><strong>例子：泊松分布</strong><br />
如果 $Y\overset{\mathrm d}{=}\mathrm{Poi}(\lambda)$，那么 $\ln f(y\mid \lambda)=-\lambda + y\ln \lambda - \ln y!$，所以 $\theta=\ln \lambda$。</p>

    <p>因此，我们有 $\ln f(y\mid \lambda)=-e^{\theta}+y\theta-\ln y!$；所以，$b(\theta)=e^{\theta}$，并且 $a(\phi)=1$。</p>

    <p>由上面的结果可得</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\mu &= \mathbb{E}(Y)=b'(\theta)=e^{\theta}=\lambda \\\\
\sigma^2 &= \mathrm{var}(Y)=a(\phi)b''(\theta)=e^{\theta}=\lambda
\end{align} %]]></script>
  </li>
  <li>
    <p><strong>例子：二项分布</strong><br />
如果 $Y\overset{\mathrm d}{=}\mathrm{Bin}(m,p)$，那么 $\ln f(y\mid m,p)=\text{const}+y\ln \frac{p}{1-p} + m\ln(1-p)$，所以 $\theta=\ln \frac{p}{1-p}$，$p=\frac{e^{\theta}}{1+e^{\theta}}$。</p>

    <p>因此，我们有 $b(\theta)=-m\ln(1-p)=m\ln(1+e^{\theta})$，并且 $a(\phi)=1$。</p>

    <p>由上面的结果可得</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\mu &= \mathbb{E}(Y)=b'(\theta)=\dfrac{me^{\theta}}{1+e^{\theta}}=mp \\\\
\sigma^2 &= \mathrm{var}(Y)=a(\phi)b''(\theta)=\dfrac{me^{\theta}}{(1+e^{\theta})^2}=mp(1-p)
\end{align} %]]></script>
  </li>
</ul>

<h2 id="2-估计">2. 估计</h2>
<h3 id="21-例子具有对数连接的泊松回归模型">2.1 例子：具有对数连接的泊松回归模型</h3>

<p>我们更详细地考虑一个相当标准的案例 —— <strong>具有对数连接的泊松回归模型（Poisson regression model with log link）</strong>，该模型可以视为 GLM 的一个模板，其连接函数等于自然参数：</p>

<p><strong>例子</strong>：具有对数连接的泊松回归模型</p>
<ul>
  <li>$Y_i\overset{\mathrm d}{=}\mathrm{Poi}(\lambda_i)$，其中 $i=1,2,\dots,n$；$Y_i$ 之间彼此相互独立。</li>
  <li>均值参数 $\lambda$ 依赖于协变量 $\mathbf x_1,\mathbf x_2,\dots,\mathbf x_q$。</li>
  <li>自然参数 $\theta_i=\ln \lambda_i$。自然连接 $g(\lambda_i)=\ln \lambda_i$。</li>
  <li>这提供了一个对数线性模型 <script type="math/tex">\eta_i=\ln \lambda_i=\mathbf x_i^{\mathrm{T}}\boldsymbol \beta=\sum_{\ell=1}^{q}x_{i\ell}\beta_{\ell}</script></li>
  <li>其矩阵形式为：$\ln \boldsymbol{\lambda}=\boldsymbol{\eta}=X\boldsymbol{\beta}$，其中 $\boldsymbol{\eta}=(\eta_1,\dots,\eta_n)^{\mathrm{T}},\boldsymbol{\lambda}=(\lambda_1,\dots,\lambda_n)^{\mathrm{T}}$。</li>
  <li>
    <p>联合对数似然函数（Joint log-likelihood function）</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\ell (\boldsymbol \beta)=\ln f &= -\sum_{i=1}^{n}\lambda_i+\sum_{i=1}^{n}y_i\ln \lambda_i-\sum_{i=1}^{n}\ln y! \\
&= -\sum_{i=1}^{n}e^{\sum_{\ell=1}^{q}x_{i\ell}\beta_{\ell}}+\sum_{i=1}^{n}y_i\sum_{\ell=1}^{q}x_{i\ell}\beta_{\ell}+\mathrm{const}
\end{align} %]]></script>
  </li>
  <li>
    <p>记分函数（score function）</p>

    <script type="math/tex; mode=display">\mathbf u(\boldsymbol \beta)=\left(\dfrac{\partial \ln f}{\partial \beta_j}\right)=\left(-\sum_{i=1}^{n}x_{ij}e^{\sum_{\ell=1}^{q}x_{i\ell}\beta_{\ell}}+\sum_{i=1}^{n}x_{ij}y_i\right)=X^{\mathrm{T}}(\mathbf y-\boldsymbol \lambda)</script>
  </li>
  <li>
    <p>海森函数（Hessian function）</p>

    <script type="math/tex; mode=display">H(\boldsymbol \beta)=\left(\dfrac{\partial^2 \ln f}{\partial \beta_j \partial \beta_k}\right)=\left(-\sum_{i=1}^{n}x_{ij}x_{ik}e^{\sum_{\ell=1}^{q}x_{i\ell}\beta_{\ell}}\right)=-X^{\mathrm T}\Lambda X</script>

    <p>其中，$\Lambda=\Lambda(\boldsymbol \beta)=\mathrm{diag}(\lambda_1,\lambda_2,\dots,\lambda_n)$。</p>
  </li>
</ul>

<p>因此，</p>

<ul>
  <li>
    <p>观测信息（Observed information）</p>

    <script type="math/tex; mode=display">J(\boldsymbol \beta)=-H(\boldsymbol \beta)=X^{\mathrm T}\Lambda X</script>
  </li>
  <li>
    <p>费雪信息（Fisher information）</p>

    <script type="math/tex; mode=display">I(\boldsymbol \beta)=-\mathbb{E}[H(\boldsymbol \beta)]=X^{\mathrm T}\Lambda X</script>
  </li>
  <li>
    <p>记分函数的方差（Variance of score function）</p>

    <script type="math/tex; mode=display">\mathrm{var}(\mathbf u(\boldsymbol \beta))=I(\boldsymbol \beta)=X^{\mathrm T}\Lambda X \overset{\mathrm{denoted}}{=}V(\boldsymbol \beta)</script>
  </li>
</ul>

<h3 id="22-记分法method-of-scoring">2.2 记分法（Method of scoring）</h3>

<p>$\boldsymbol \beta$ 的 <strong>极大似然估计 MLE</strong> $\hat {\boldsymbol \beta}$ 可以通过 <strong>牛顿-拉弗森（Newton-Raphson）</strong>或者 <strong>费雪记分法（Fisher scoring）</strong>得到。</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat {\boldsymbol \beta}^{(k+1)} &= \hat {\boldsymbol \beta}^{(k)}+\left[\hat {V}^{(k)} \right]^{-1}\mathbf u(\hat {\boldsymbol \beta}^{(k)}) \\\\
\Longrightarrow \quad \hat {V}^{(k)}\hat {\boldsymbol \beta}^{(k+1)} &= \hat {V}^{(k)}\hat {\boldsymbol \beta}^{(k)}+\mathbf u(\hat {\boldsymbol \beta}^{(k)}) \\\\
\Longrightarrow \quad X^{\mathrm T}\hat{\Lambda}^{(k)}X \hat {\boldsymbol \beta}^{(k+1)} &= X^{\mathrm T}\hat{\Lambda}^{(k)}X \hat {\boldsymbol \beta}^{(k)} + X^{\mathrm T}\hat{\Lambda}^{(k)}\left[(\hat {\Lambda}^{(k)})^{-1} (\mathbf y-\hat {\boldsymbol \lambda}^{(k)}) \right]\\\\
\Longrightarrow \quad X^{\mathrm T}\hat{\Lambda}^{(k)}X \hat {\boldsymbol \beta}^{(k+1)} &= X^{\mathrm T}\hat{\Lambda}^{(k)}\hat{\mathbf z}^{(k)}
\end{align} %]]></script>

<p>其中，</p>

<script type="math/tex; mode=display">\hat{\mathbf z}^{(k)} = X \hat {\boldsymbol \beta}^{(k)}+(\hat {\Lambda}^{(k)})^{-1} (\mathbf y-\hat {\boldsymbol \lambda}^{(k)}) \;,\qquad
\hat {\boldsymbol \lambda}^{(k)} = \boldsymbol \lambda (\hat {\boldsymbol \beta}^{(k)}) \;,\qquad
\hat{\Lambda}^{(k)} = \Lambda (\hat {\boldsymbol \beta}^{(k)})</script>

<p>这本质上是 <strong>加权最小二乘等式（weighted least squares equation）</strong>：</p>

<script type="math/tex; mode=display">X^{\mathrm T}\mathbf W X\hat {\boldsymbol \beta}=X^{\mathrm T}\mathbf {Wy}</script>

<p>因此，记分法中的 <strong>迭代步（iterative step）</strong>相当于在 $\mathbf x_i$ 上拟合一个 $\hat z_i^{(k)}$ 的加权回归，权重为 $\hat \lambda_i^{(k)}$。</p>

<p>注意，在每次迭代中，$\hat \lambda_i^{(k)}$ 和 $\hat z_i^{(k)}$ 的值都需要更新，因为它们都依赖于 $\hat{\boldsymbol \beta}^{(k)}$。</p>

<p>在广义线性模型中，我们总是可以将记分法中的迭代步像这样表示为加权回归：该过程被称为迭代地重新加权最小二乘法（IRWLS）。</p>

<p>下节内容：导论 (4)</p>
:ET