I"8+<h1 id="lecture-15-损失函数-一">Lecture 15 损失函数 (一)</h1>

<p>在前几节课中，我们学习了模型模块中的一些知识，包括如何构建模型以及怎样进行模型初始化。本节课我们将开始学习损失函数模块。</p>

<h2 id="1-损失函数的概念">1. 损失函数的概念</h2>

<p><strong>损失函数 (Loss Function)</strong>：衡量模型输出与真实标签之间的差异。</p>

<p>下面是一个一元线性回归的拟合过程示意图：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-20-WX20201220-200705%402x.png" width="60%" /></p>

<p>图中的绿色方块代表训练样本点 $(x_i, y_i)$，蓝色直线代表训练得到的模型 $\hat y = w_0 + w_1 x$，其中，$w_0$ 代表截距，$w_1 = \Delta y / \Delta x$ 代表斜率。可以看到，模型并没有完美地拟合每一个数据点，所以数据点和模型之间存在一个 <strong>损失 (Loss)</strong>，这里我们采用垂直方向上模型输出与真实数据点之差的绝对值 $|\hat y -y|$ 作为损失函数的度量。</p>

<p>另外，当我们谈到损失函数时，经常会涉及到以下三个概念：</p>

<ul>
  <li>
    <p><strong>损失函数 (Loss Function)</strong>：计算单个样本的差异。</p>

\[\mathrm{Loss} = f(\hat y, y)\]
  </li>
  <li>
    <p><strong>代价函数 (Cost Function)</strong>：计算整个训练集 $\mathrm{Loss}$ 的平均值。</p>

\[\mathrm{Cost} = \dfrac{1}{n}\sum_{i=1}^{n} f(\hat y_i, y_i)\]
  </li>
  <li>
    <p><strong>目标函数 (Objective Function)</strong>：最终需要优化的目标，通常包含代价函数和正则项。</p>

\[\mathrm{Obj} = \mathrm{Cost} + \mathrm{Regularization}\]
  </li>
</ul>

<p>注意，代价函数并不是越小越好，因为存在过拟合的风险。所以我们需要加上一些约束 (即正则项) 来防止模型变得过于复杂而导致过拟合，常用的有 L1 和 L2 正则项。因此，代价函数和正则项最终构成了我们的目标函数。</p>

<p>下面我们来看一下 PyTorch 中的 <code class="language-plaintext highlighter-rouge">_Loss</code> 类：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">_Loss</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">or</span> <span class="nb">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="p">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="nb">reduce</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可以看到，<code class="language-plaintext highlighter-rouge">_Loss</code> 是继承于 <code class="language-plaintext highlighter-rouge">Module</code> 类的，所以从某种程度上我们可以将 <code class="language-plaintext highlighter-rouge">_Loss</code> 也视为一个网络层。它的初始化函数中主要有 3 个参数，其中 <code class="language-plaintext highlighter-rouge">size_average</code> 和 <code class="language-plaintext highlighter-rouge">reduce</code> 这两个参数即将在后续版本中被舍弃，因为 <code class="language-plaintext highlighter-rouge">reduction</code> 参数已经可以实现前两者的功能。</p>

<h2 id="2-交叉熵损失函数">2. 交叉熵损失函数</h2>

<h4 id="nncrossentropyloss"><code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss</code></h4>

<p><strong>功能</strong>：<code class="language-plaintext highlighter-rouge">nn.LogSoftmax()</code> 与 <code class="language-plaintext highlighter-rouge">nn.NLLLoss()</code> 结合，进行交叉熵计算。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span>
    <span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>注意：这里的计算过程和交叉熵公式存在一些差异。主要差别在于这里使用了一个 Softmax 函数对数据进行了归一化处理，使其落在一个正常的概率值范围内。</p>

<p>在分类任务中，我们经常采用这是因为交叉熵函数经常用于分类任务，而在分类任务中我们常常需要计算不同类别的概率值。所以交叉熵可以用来衡量两个概率分布之间的差异，交叉熵值越低说明两个概率分布越接近。</p>

<p>那么为什么交叉熵值越低，两个概率分布越接近呢？这需要从它与信息熵和相对熵之间的关系说起：</p>

<p><span><center>交叉熵 $=$ 信息熵 $+$ 相对熵</center></span></p>

<p>我们先来看最基本的 <strong>熵 (Entropy)</strong> 的概念：熵准确来说应该叫做 <strong>信息熵 (Information Entropy)</strong>，它是由信息论之父香农从热力学中借鉴过来的一个概念，用于描述某个事件的不确定性：某个事件不确定性越高，它的熵就越大。例如：“明天下雨” 这一事件要比 “明天太阳会升起” 这一事件的熵大得多，因为前者的不确定性较高。这里我们需要引入 <strong>自信息</strong> 的概念。</p>

<ul>
  <li>
    <p><strong>自信息 (Self-information)</strong>：用于衡量单个事件的不确定性。</p>

\[I(X) = -\log [P(X)]\]

    <p>其中，$P(X)$ 为事件 $X$ 的概率。</p>
  </li>
  <li>
    <p><strong>熵 (Entropy)</strong>：自信息的期望，用于描述整个概率分布的不确定性。事件的不确定性越高，它的熵就越大。</p>

\[H(P) = \mathrm{E}_{X\sim P}[I(X)] = \sum_{i=1}^{n}P(x_i)\log P(x_i)\]
  </li>
</ul>

<p>为了更好地理解熵与事件不确定性的关系，我们来看一个示意图：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-20-entropy.png" width="60%" /></p>

<p>上面是伯努利分布 (两点分布) 的信息熵，可以看到，当事件概率为 $0.5$ 时，它的信息熵最大，大约在 $0.69$ 附近，即此时该事件的不确定性是最大的。注意，这里的 $0.69$ 是在二分类模型训练过程中经常会碰到的一个 Loss 值：有时在模型训练出问题时，无论我们如何进行迭代，模型的 Loss 值始终恒定在 $0.69$；或者在模型刚初始化完成第一次迭代后，其 Loss 值也很可能是 $0.69$，这表明我们的模型当前是不具备任何判别能力的，因为其对于两个类别中的任何一个都认为概率是 $0.5$。</p>

<p>下面我们来看一下相对熵的概念：</p>

<ul>
  <li>
    <p><strong>相对熵 (Relative Entropy)</strong>：又称 <strong>KL 散度 (Kullback-Leibler Divergence, KLD)</strong>，用于衡量两个概率分布之间的差异 (或者说距离)。注意，虽然 KL 散度可以衡量两个分布之间的距离，但它本身并不是一个距离函数，因为距离函数具有对称性，即 $P$ 到 $Q$ 的距离必须等于 $Q$ 到 $P$ 的距离，而相对熵不具备这种对称性。</p>

\[D_{\mathrm{KL}}(P, Q) = \mathrm{E}_{X \sim P}\left[\log \dfrac{P(X)}{Q(X)}\right]\]

    <p>其中，$P$ 是数据的真实分布，$Q$ 是模型拟合的分布，二者定义在相同的概率空间上。我们需要用拟合分布 $Q$ 去逼近真实分布 $P$，所以相对熵不具备对称性。</p>
  </li>
</ul>

<p>下面我们再来看一下交叉熵的公式：</p>

<ul>
  <li>
    <p><strong>交叉熵 (Cross Entropy)</strong>：用于衡量两个分布之间的相似度。</p>

\[H(P,Q)= -\sum_{i=1}^{n}P(x_i)\log Q(x_i)\]
  </li>
</ul>

<p>下面我们对相对熵的公式进行展开推导变换，来观察一下相对熵与信息熵和交叉熵之间的关系：</p>

\[\begin{aligned}
D_{\mathrm{KL}}(P, Q) &amp;= \mathrm{E}_{X \sim P}\left[\log \dfrac{P(X)}{Q(X)}\right] \\[2ex]
&amp;= \mathrm{E}_{X \sim P} [\log P(X) - \log Q(X) ] \\[2ex]
&amp;= \sum_{i=1}^{n} P(x_i) [\log P(x_i) - \log Q(x_i) ] \\[2ex]
&amp;= \sum_{i=1}^{n} P(x_i) \log P(x_i) - \sum_{i=1}^{n} P(x_i) \log Q(x_i) \\[2ex]
&amp;= H(P, Q) - H(P)
\end{aligned}\]

<p>所以，<strong>交叉熵等于信息熵加上相对熵</strong>：</p>

\[H(P, Q) = H(P) + D_{\mathrm{KL}}(P, Q)\]

<p>这里，$P$ 为训练集中的样本分布，$Q$ 为模型给出的分布。所以在机器学习中，我们最小化交叉熵实际上等价于最小化相对熵，因为训练集是固定的，所以 $H(P)$ 在这里是一个常数。</p>

<p>主要参数：</p>

<p>• weight ：各类别的 loss 设置权值</p>

<p>• ignore_index ：忽略某个类别</p>

<p>• reduction ：计算模式， 可为 none/sum/mean none - 逐个元素计算 sum - 所有元素求和， 返回标量 mean - 加权平均， 返回标量</p>

<p>下节内容：损失函数 (一)</p>
:ET