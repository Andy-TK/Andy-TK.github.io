I"'<h1 id="lecture-09-分类和回归树及相关方法">Lecture 09 分类和回归树及相关方法</h1>

<p><strong>参考教材</strong>：</p>

<ul>
  <li><em>Hardle, W. and Simar, L (2015). Applied multivariate statistical analysis, 4th edition.</em></li>
  <li><em>Hastie, T. Tibshirani, R. and Friedman, J. (2009). The elements of statistical learning, 2nd edition</em></li>
</ul>

<h2 id="1-引言">1. 引言</h2>

<p>前一章中的方法依赖于很强的 <strong>参数假设 (parametric assumptions)</strong> (线性模型、逻辑模型或正态性假设)。</p>

<p>当这些假设基本正确时，这些分类器可以很好地工作；但是当这些假设与真实情况相差很远时，这些分类器的性能可能会非常差。</p>

<p>回忆一下之前的例子：当分隔边界看上去并非线性时，线性分类器将无法很好地工作。</p>

<p>因此，我们需要一些对于强参数假设涉及较少的灵活模型。</p>

<p>这里，我们将介绍由 Leo Breiman 在 1980 年代普及的概念：<strong>回归树 (regression trees)</strong>。</p>

<h2 id="2-回归树">2. 回归树</h2>

<p>假设我们观察到一个 i.i.d. 样本 $(X_1,Y_1),\dots,(X_n,Y_n)$ 来自以下基础均值模型</p>

<script type="math/tex; mode=display">E(Y_i \mid X_i=x) = m(x)</script>

<p>其中，$Y_i\in \mathbb R$ 和 $X_i=(X_i1,\dots,X_ip)^{\mathrm T} \in \mathbb R^p$ 是连续的。</p>

<p>在本节内容中，该表示法将变得混乱。 因此，我们将样本表示为</p>

<p>下节内容：分类和回归树及相关方法</p>
:ET