I"S<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-10-更高效的强化学习回报设计和-q-函数-逼近">Lecture 10 更高效的强化学习：回报设计和 $Q$-函数 逼近</h1>

<p><strong>主要内容：</strong></p>
<ol>
  <li>动机</li>
  <li>$Q$-函数 逼近</li>
  <li>回报设计和初始化 $Q$-函数</li>
  <li>总结</li>
</ol>

<h2 id="1-动机">1. 动机</h2>
<h3 id="11-学习成果">1.1 学习成果</h3>
<ol>
  <li>给定一些已知特征，手动应用线性 $Q$-函数 逼近来求解小规模 MDP 问题。</li>
  <li>选择合适的特征，设计并实现 $Q$-函数逼近，以实现无模型强化学习技术，从而自动求解中等规模的 MDP 问题。</li>
  <li>比较各种函数逼近方法的优缺点。</li>
  <li>比较和对比线性函数逼近与基于深度神经网络的函数逼近。</li>
  <li>说明如何使用回报设计来帮助无模型强化学习方法达到收敛。</li>
  <li>将回报设计手动应用于给定的潜在函数，以解决小规模 MDP 问题。</li>
  <li>设计并实现潜在函数，以自动解决中等规模的 MDP 问题。</li>
  <li>对比和比较回报设计与通过Q函数初始化比较和对比奖励塑造</li>
</ol>

<p>下节内容：更高效的强化学习：回报设计和 $Q$-函数 逼近</p>

:ET