I"}2<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-07-马尔可夫决策过程mdps">Lecture 07 马尔可夫决策过程（MDPs）</h1>

<p>不同于之前基于搜索的规划，从这节课开始，我们将学习基于强化学习的规划。我们将从基础的 <strong>马尔可夫决策过程（Markov Decision Processes，MDPs）</strong> 开始学习，它是所有强化学习的基础模型，我们可以利用它解决一些强化学习问题。马尔可夫过程松弛了很多之前的经典规划中的假设，之后，我们将学习一些强化学习方法，它们松弛的假设要更多。</p>

<p>本节主要内容如下：</p>

<ol>
  <li>动机</li>
  <li>马尔可夫决策过程（MDPs）：定义</li>
  <li>计算：求解 MDPs</li>
  <li>部分观测 MDPs</li>
</ol>

<h2 id="1-动机">1. 动机</h2>
<h3 id="11-学习成果">1.1 学习成果</h3>

<ol>
  <li>确定在哪些情况下，马尔可夫决策过程（MDP）模型适用于我们的问题。</li>
  <li>定义 “马尔可夫决策过程”。</li>
  <li>比较 MDPs 与经典规划模型。</li>
  <li>说明 Bellman 方程（和动态规划）是如何求解 MDP 问题的。</li>
  <li>应用价值迭代来手动解决小规模 MDP 问题，并编写价值迭代算法的代码来自动解决中等规模的 MDP 问题。</li>
  <li>根据价值函数构造策略。</li>
  <li>比较和对比价值迭代与策略迭代。</li>
  <li>讨论价值迭代和策略迭代算法的优缺点。</li>
</ol>

<h3 id="12-相关阅读">1.2 相关阅读</h3>
<ul>
  <li>任何概率论基础材料。</li>
  <li>Chapter 17 of <em>Artiﬁcial Intelligence — A Modern Approach</em> by Russell and Norvig.</li>
  <li>Chapter 4 of <em>Reinforcement Learning: An Introduction, second edition</em>.</li>
</ul>

<h3 id="13-移除假设">1.3 移除假设</h3>
<p><strong>经典规划</strong>：目前为止，我们已经学习了经典规划。经典规划工具可以在大的搜索空间中快速求解，但是需要如下假设：</p>
<ul>
  <li>确定性事件</li>
  <li>环境改变只是行动导致的结果</li>
  <li>完美的知识（全知）</li>
  <li>单个执行器（全能）</li>
</ul>

<p>在本课程的其余部分，我们将研究如何放松其中的一些假设，我们将从第一个开始：确定性事件。</p>

<h3 id="14-马尔可夫决策过程">1.4 马尔可夫决策过程</h3>
<p><strong>马尔可夫决策过程（Markov Decision Processes，MDP）</strong>移除了确定性事件的假设，而是假设每个行动可能导致多个结果，其中每个结果都有一个相关的概率。</p>

<p>例如：</p>
<ul>
  <li>抛一枚硬币有 2 种结果：正面（$\frac{1}{2}$）和背面（$\frac{1}{2}$）</li>
  <li>同时掷两个骰子有 12 种结果：2（$\frac{1}{36}$）, 3（$\frac{2}{36}$）, 4（$\frac{3}{36}$）, $\dots$, 12（$\frac{1}{36}$）</li>
  <li>当试图用一个机械臂拾取物体时，可以有 2 种结果：成功（$\frac{4}{5}$）和失败（$\frac{1}{5}$）</li>
</ul>

<p>MDP 已成功应用于许多领域的规划中：机器人导航、规划在矿山的哪个区域挖掘矿物、为患者提供治疗、对车辆进行维护计划等。</p>

<h3 id="15-典型例子网格世界">1.5 典型例子：网格世界</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-05-20-WX20200520-204625%402x.png" width="40%" /></p>

<p>图中网格的左下角是我们的 agent，灰色方块是墙（无法通行），右上角两个带标记的格子给出了回报（reward）：到达绿色方块得 $1$ 分（agent 应该到达的目标）；到达橙色方块得到一个负的回报 $-1$（agent 应该避开的网格）。</p>

<p>但是，事情的发展可能超出我们的预期 —— 有时行动带来的效应并不是我们希望看到的：</p>

<ul>
  <li>如果 agent 尝试向北移动，我们假设有 $80\%$ 的时间（即 $0.8$ 的概率）它会按照计划移动（假设途中没有墙）</li>
  <li>有 $10\%$ 的时间（即 $0.1$ 的概率），试图向北移动会导致 agent 向西移动（假设途中没有墙）</li>
  <li>有 $10\%$ 的时间（即 $0.1$ 的概率），试图向北移动会导致 agent 向东移动（假设途中没有墙）</li>
  <li>如果碰到墙，则 agent 会停在原地。</li>
</ul>

<p>这和之前的经典规划不同，在经典规划中，当我们执行一个行动时，我们可以确切地知道它将导致的后继状态是什么。但是，在这个例子中，我们并不知道向北移动这一行动将导致的后继状态是什么，我们只知道 3 种可能的结果状态以及它们各自的概率分布，但是我们并不知道最终结果会是其中的哪一个，直到行动已经完成，然后我们可以观察 agent 到底处于哪种后继状态。</p>

<p>上面是马尔可夫决策过程的一个经典例子。MDP 可以应用在很多场景中，例如：我们试图模拟连接网络服务器，$99\%$ 的时间，能够成功连接网络服务器，而 $1\%$ 的时间，会发生连接失败。</p>

<h2 id="2-马尔可夫决策过程mdps定义">2. 马尔可夫决策过程（MDPs）：定义</h2>
<h3 id="21-折扣回报马尔可夫决策过程discounted-reward-mdp">2.1 折扣回报马尔可夫决策过程（Discounted Reward MDP）</h3>

<p>MDP 是 <strong>完全可观测的、概率的</strong> 状态模型。MDP 的最常见表述是 <em>折扣回报</em> 马尔可夫决策过程：</p>
<ul>
  <li>一个状态空间 $S$</li>
  <li>初始状态 $s_0\in S$</li>
  <li>行动 $A(s)\subseteq A$ 可应用于每个状态 $s\in S$</li>
  <li><strong>转移概率</strong> $P_a(s’\mid s)$，对于 $s\in S$ 和 $a\in A(s)$</li>
  <li><strong>回报</strong> $r(s,a,s’)$ 可以为正或负，采取行动 $a$ 从状态 $s$ 转换为状态 $s’$</li>
  <li>一个 <strong>折扣因子</strong> $0\le \gamma &lt;1$</li>
</ul>

<p>和经典规划相比，有哪些不同之处？主要有以下四点：</p>
<ul>
  <li>转移函数不再是确定性的。每个行动有 $P_a(s’\mid s)$ 的概率达到状态 $s’$，如果行动 $a$ 执行之前的状态为 $s$。</li>
  <li>没有目标。每个行动在采取之后都会接受一个回报。回报的值取决于其采用的状态。</li>
  <li>没有行动代价，它们被建模为负的回报。</li>
  <li>我们有一个折扣因子。</li>
</ul>

<p>MDP 可以视为经典规划的一种扩展，它使得我们可以模拟更多的问题，但是也会使得求解过程和经典规划非常不一样。</p>

<h3 id="22-折扣回报">2.2 折扣回报</h3>

<p>折扣因子 $\gamma$ 决定了与当前回报相比，未来回报应该打多少折扣。</p>

<p>例如，你是希望今天就能拿到 100 美元还是一年后拿到 100 美元？我们（人类）通常会对未来回报打折，并为近期回报赋予更高的价值。</p>

<p>假设我们的 agent 按照 $r_1,r_2,r_3,r_4,\dots$ 的顺序接受回报。 如果 $\gamma$ 是折扣因子，那么 <strong>折扣回报（discounted reward）</strong> 为：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
V &= r_1+\gamma r_2 + \gamma^2 r_3 +\gamma^3 r_4 + \dots \\
&= r_1 + \gamma(r_2+\gamma(r_3+\gamma(r_4+\dots)))
\end{align} %]]></script>

<p>如果在时间步 $t$ 接受的回报为 $V_t$，那么 $V_t=r_t + \gamma V_{t+1}$</p>

<p>与折扣回报相对的另外一种方式是 <strong>累加回报（addictive reward）</strong>：</p>

<script type="math/tex; mode=display">V = r_1+r_2 + r_3 + r_4 + \dots</script>

<p>前面 1.5 小节网格世界的例子使用的就是累加回报。注意，在我们用于启发式搜索算法的路径耗散函数中，也隐式地使用了累加性。</p>

<p>折扣因子 $\gamma$ 的取值范围为 $[0,1)$，区间右边没有包含 $1$ 意味着我们总是选择对未来回报打折：当 $\gamma$ 接近 $0$ 时，我们认为未来回报无关紧要；当 $\gamma$ 接近 $1$ 时，未来回报和累加回报完全等价。</p>

<h3 id="23-mdp-建模概率-pddl">2.3 MDP 建模：概率 PDDL</h3>

<p>概率 PDDL 是表示 MDP 的一种方法。它通过其他一些构造扩展了 PDDL。其中最相关的是，结果可以与概率联系起来。以下代码描述了 “炸弹和厕所” 问题，其中两个包裹中的一个装有 “炸弹”。“炸弹” 可以通过浸入的方式来疏通厕所，但有 $0.05$ 的概率会使得 “炸弹” 堵塞厕所。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(define (domain bomb-and-toilet)
    (:requirements :conditional-effects :probabilistic-effects) 
    (:predicates (bomb-in-package ?pkg) (toilet-clogged) (bomb-defused)) 

(:action dunk-package
    :parameters (?pkg)
    :effect (and (when (bomb-in-package ?pkg) (bomb-defused)) 
            (probabilistic 0.05 (toilet-clogged))))
</code></pre></div></div>

<h3 id="24-mdp-问题的解策略">2.4 MDP 问题的解：策略</h3>

<p>折扣 MDP 的规划问题与经典规划不同，因为行动导致的结果是不确定的。所以，相比经典规划中生成一个行动序列，MDP 生成的是一个 <strong>策略 (policy)</strong>。</p>

<p>策略 $\pi$ 是一个函数，它告诉 agent 在每种状态下采取哪种行动是最好的选择。策略可以是 <strong>确定性的 (deterministic)</strong>，也可以是 <strong>随机的 (stochastic)</strong>。</p>

<p><strong>确定性策略（deterministic policy）</strong>$\pi:S\to A$ 是一个从状态到动作的映射。它指定了在每种可能的状态下选择哪种行动。因此，如果我们处于状态 $s$，我们的 agent 应该选择由 $\pi(s)$ 所定义的行动。</p>

<p>回到前面网格世界的例子，我们希望从左下角的初始位置移动到右上角的绿色目标方格：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-05-20-WX20200520-204625%402x.png" width="40%" /></p>

<p>在经典规划中，我们可能得到一个这样的行动序列：</p>

<script type="math/tex; mode=display">[\texttt{move_up},\texttt{move_up},\texttt{move_right}, \texttt{move_right}, \texttt{move_right}]</script>

<p>但是，在 MDP 中，行动导致的后继状态是不确定的，所以，我们选择用策略将状态映射到行动。</p>

<p>一个网格世界的策略的图表示为：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-05-20-WX20200521-001221%402x.png" width="35%" /></p>

<p>所以，在初始状态（左下角网格），agent 遵循该策略应当向上移动。</p>

<p>当然，agent 无法直接使用图策略，MDP 规划器的输出看上去可能是下面这样的：</p>
<ul>
  <li>$at(0,0) \; \Longrightarrow \; \texttt{move_up}$</li>
  <li>$at(0,1) \; \Longrightarrow \; \texttt{move_up}$</li>
  <li>$at(0,2) \; \Longrightarrow \; \texttt{move_right}$</li>
  <li>$at(1,0) \; \Longrightarrow \; \texttt{move_left}$</li>
  <li>$at(1,2) \; \Longrightarrow \; \texttt{move_right}$</li>
  <li>$at(2,0) \; \Longrightarrow \; \texttt{move_up}$</li>
  <li>$at(2,1) \; \Longrightarrow \; \texttt{move_up}$</li>
  <li>$at(2,2) \; \Longrightarrow \; \texttt{move_right}$</li>
  <li>$at(3,0) \; \Longrightarrow \; \texttt{move_left}$</li>
</ul>

<p>然后，agent 可以对其解析并使用：方法是先确定其所处的状态，查看该状态对应的行动，并执行该行动。然后重复该过程，直到达到目标状态。</p>

<p>这种策略被称为 <strong>确定性策略</strong>，因为尽管行动导致的结果状态是不确定的，但是对于同一状态，我们始终采取一样的行动，即 $\pi(s)\to A$。例如，当 agent 位于 $(0,0)$ 处时，我们始终选择 $\texttt{move_up}$。</p>

<p>相应地，另外一种被称为 <strong>随机性策略</strong>，即 $\pi(s,a)\in [0, 1]$，对于同一种状态，我们可能以不同的概率采取不同的行动。</p>

<p><strong>随机性策略（stochastic policy）</strong>$\pi:S\times A \to \mathbb R$ 指定了 agent 应当选择行动的概率分布。直观地，$\pi(s,a)$ 指定了在状态 $s$ 下执行行动 $a$ 的概率。</p>

<p>为了执行随机策略，我们可以直接采取具有 $\pi(s,a)$ 最大化的行动。 但是，在许多领域中，最好根据概率分布选择一个动作。 也就是说，以概率方式选择动作，以使较高概率的动作与它们的相对概率成比例地选择。</p>

<p>在本主题中，我们将仅关注确定性策略，但是随机策略有其位置。</p>

<p>下节内容：生成启发函数</p>
:ET